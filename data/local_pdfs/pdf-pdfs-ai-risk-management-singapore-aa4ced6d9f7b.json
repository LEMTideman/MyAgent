{
  "doc_id": "pdf-pdfs-ai-risk-management-singapore-aa4ced6d9f7b",
  "source_type": "local_pdf",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Risk Management - Singapore.pdf",
  "title": "AI Risk Management - Singapore",
  "text": "ARTIFICIAL INTELLIGENCE \n\n# MODEL RISK \n\n# MAN AGEMENT \n\n# OBSERVATIONS FROM A THEMATIC REVIEW  \n\n> Information Paper\n> December 2024\n\nArtificial Intelligence Model Risk Management | 2\n\n## Contents \n\n1. Overview 3\n\n2. Background 3\n\n3. Objectives and Key Focus Areas 7\n\n4. Governance and Oversight 10 \n\n5. Key Risk Management Systems and Processes 12 \n\n5.1  Identification 12 \n\n5.2 Inventory 13 \n\n5.3 Risk Materiality Assessment 15 \n\n6 Development and Deployment 16 \n\n6.1 Standards and Processes 16 \n\n6.2 Data Management 18 \n\n6.3 Development 21 \n\n6.4 Validation 30 \n\n6.5 Deployment, Monitoring and Change Management 31 \n\n7 Other Key Areas 36 \n\n7.1 Generative AI 36 \n\n7.2 Third -Party AI 43 \n\n8 Conclusion 44 \n\nAnnex A - Definitions 46 \n\nAnnex B - Useful References 49 Artificial Intelligence Model Risk Management | 3\n\n# 1. Overview \n\n1.1  This information paper sets out good practices relating to Artificial Intelligence \n\n(AI) (including Generative AI)  1 model risk management (MRM)  2 that were \n\nobserved during a recent thematic review of selected banks . The information \n\npaper focuses on the following key areas 3 : AI governance and oversight ; AI \n\nidentification, inventorisation and risk materiality assessment; as well as AI \n\ndevelopment, validation, deployment, monitoring and change management. \n\n1.2  While the thematic review focused on selected banks, the good practices \n\nhighlighted in this information paper should generally appl y to other financial \n\ninstitutions (FIs), which should take reference from these when developing and \n\ndeploying AI. \n\n# 2. Background \n\nIndustry use of AI and Generative AI and associated risks \n\n2.1  The launch of ChatGPT in November 2022 and recent advancements in AI, \n\nparticularly Generative AI, has led to an increased interest in leveraging AI and \n\nGenerative AI in the banking and broader financial sector. Prior to these \n\ndevelopments, FIs have used AI in a wide range of areas and use cases. Key areas \n\nwhere we observed significant use of AI by banks during the thematic review \n\ninclude risk management, customer engagement and servicing , as well as to  \n\n> 1\n\nGenerative AI is a subset of AI , and a n AI or Generative AI system can comprise one or more AI or Generative AI models and \n\nother machine -based components . For the purposes of this paper, AI generally refers to both AI and Generative AI models and \n\nsystems . Where a point pertains specifically to an AI model or an AI system, or to Generative AI , we will use the respective \n\nterm s explicitly in the paper. We define the terms AI and Generative AI, as well as AI model, system and use case in greater \n\ndetail in Annex A.  \n\n> 2\n\nIn line with the footnote above and r ecognising that the AI MRM is intrinsically linked to the risk management of systems in \n\nwhich AI models are used, when we refer to AI MRM or AI risk management in this paper, it generally refers to the risk \n\nmanagement of AI models and systems.  \n\n> 3\n\nThe aim of this information paper is not to cover all aspects of model risk management, but to focus on good practices in \n\nareas that are more relevant to AI MRM .Artificial Intelligence Model Risk Management | 4\n\nsupport internal operational processes . For example, we have seen banks use AI, \n\nparticularly decision tree -based machine learning (ML) models such as XGBoost, \n\nLightGBM and CatBoost  4 , in financial risk management to detect abnormal \n\nfinancial market movements, or to estimate loan prepayment rates. They are also \n\ncommonly used in anti -money laundering (AML) systems to detect suspicious \n\ntransactions , and in fraud detection systems . In customer engagement and \n\nservicing, banks use AI to predict customer preferences, personalise financial \n\nproduct recommendations and manage customer feedback. AI is also widely \n\nused to support internal operational processes across a wide range of business \n\nfunctions, for example, to automate checking and verification processes (e.g. , for \n\ncustomer information ), prioritise incident management (e.g. , triaging IT incidents \n\nfor attention ), or forecast demand for services (e.g. , ATM cash withdrawals) .\n\n2.2  While the use of AI in these areas can enhance operational efficiency , facilitate \n\nrisk management and enhance financial services , they can also increase risk \n\nexposure if not developed or deployed responsibly . Po tential risks include: \n\n• Financial risks , e.g.,  poor accuracy of AI used for risk management could  lead \n\nto poor risk assessments and consequent financial losses. \n\n• Operational risks , e.g., unexpected behaviour of  AI used to automate financial \n\noperations could lead to  operational disruptions or errors in critical processes. \n\n• Regulatory risks , e.g., poor performance of AI used to support AML efforts \n\ncould lead to non -compliance with regulations. \n\n• Reputational risk s, e.g., wrong or inappropriate information from AI -based \n\ncustomer -facing systems , such as chatbots , could lead to customer complaints \n\nand negative media attention, and consequent reputational damage.                 \n\n> 4Decision tree -based ML models make predictions based on a tree -like structure learnt from data. Models such as X GBoost,\n> LightGBM and CatBoost utilise a series of decision trees together with a boosting technique . Each decision tree in the series\n> focuses on the errors made by aprior decision tree to improve predictions .Such models are also explainable as the relative\n> importance of different features to model predictions can be extracted.\n\nArtificial Intelligence Model Risk Management | 5\n\n2.3  While natural language processing (NLP) 5 and computer vision (CV) 6 techniques \n\nwere already in use in the financial sector prior to the emergence of Generative \n\nAI 7 for text or image -related tasks, recent Generative AI models such as OpenAI’s \n\nGPT 8 large language models (LLM s) and D ALL -E9 image generation models , or \n\nAnthropic’s Claude LLM s10  offer better performance in tasks such as cluster ing \n\ndocuments . They have also enable d new use cases , e.g. , to generat e text content \n\nand image s for marketing , or to process multimodal data 11  for financial analysis .\n\n2.4  Based on the thematic review, use of Generative AI in banks appear s to still be at \n\nan early stage . The current focus is on the use of Generative AI to assist or \n\naugment humans for productivity enhancements , and not in applying Generative \n\nAI to direct customer facing applications . Use cases being explored by banks \n\ninclude risk management (e.g., detecting emerging risks in text information );\n\ncustomer engagement and service (e.g., summarising customer interactions or \n\ngenerating marketing content ); and research and reporting (e.g., investment \n\nanalyses ). Banks are also exploring the use of Generative AI in copilots 12  to \n\nsupport staff, for example , in coding, or for general text -related tasks such as \n\nsummarisation and answering queries based on information in internal \n\nknowledge repositories. \n\n2.5  With Generative AI, existing risks associated with AI may be amplified 13 . For \n\nexample, Generative AI's potential for hallucinations and unpredictable  \n\n> 5\n\nNatural language processing (NLP) is commonly used to refer to techniques that process , analyse , make predictions or \n\ngenerate outputs relating to human language, both in its written and spoken forms.  \n\n> 6\n\nComputer vision (CV) is commonly used to refer to techniques that enable machines to process and generate outputs based \n\non visual information from the world.  \n\n> 7\n\nFor example, for news sentiment analysis, information extraction, clustering documents based on underlying topics, or \n\ndigitising physical documents . \n\n> 8\n\nGenerative Pre -trained Transformers (GPT) are a family of Generative AI models developed by OpenAI , that include s models \n\nsuch GPT 4 and GPT -4o.  \n\n> 9\n\nDALL -E is a Generative AI model that generates images from text prompts or descriptions.  \n\n> 10\n\nClaude models are a family of Generative AI models developed by Anthropic and include models such as Claude 3.5 Haiku \n\nand Sonnet.  \n\n> 11\n\nMultimodal data refers to datasets that comprise multiple types of data, e.g., text, images, audio or video.  \n\n> 12\n\nIn the context of Generative AI, the term copilot is typically used to refer to Generative AI being used to assist or augment \n\nhumans on specific tasks . \n\n> 13\n\nMore details on risks associated with Generative AI have already been covered extensively in Project Mind Forge ’s white \n\npaper on “Emerging Risks and Opportunities of Generative AI for Banks ” and will not be repeated in this information paper. \n\nThe whitepaper can be accessed at  https://www.mas.gov.sg/schemes -and -initiatives/project -mindforge .Artificial Intelligence Model Risk Management | 6\n\nbehaviours may pose significant risks  if Genera tive AI is used in mission -critical \n\nareas. The complexity of Gen erative AI models and lack of established \n\nexplainability techniques also create s challenges for understanding and \n\nexplaining decisions , while the d iverse and often opaque data sources used in \n\nGen erative AI training, coupled with difficulties in evaluating bias of Generative \n\nAI outputs , could lead to unfair decisions .\n\nMAS’ Efforts on Responsible AI for the Financial Sector \n\n2.6  Alongside the growing use of AI in the financial sector and such associated risks ,\n\nMAS ha d established key principles to guide financial institutions in their \n\nresponsible use of AI. \n\n2.7  In 2018, MAS co -created the principles of Fairness, Ethics, Accountability and \n\nTransparency (FEAT) with the financial industry to promote the deployment of AI \n\nand data analytics in a responsible manner. To provide guidance to FIs in \n\nimplementing FEAT, MAS started working with an industry consortium on the \n\nVeritas Initiative 14  in November 2019. The Veritas Initiative aimed to support FIs \n\nin incorporating the FEAT Principles into their AI and data analytics solutions, and \n\nhas released assessment methodologies, a toolkit , and accompanying case \n\nstudies. \n\n2.8  With the emergence of Generative AI, Project Mind Forge 15 , which is also driven \n\nby the Veritas Initiative, was established to examine the risk s and opportunities \n\nof Generative AI. The first phase of Project Mind Forge was supported by a \n\nconsortium of banks and released a risk framework for Generative AI in \n\nNovember 2023. \n\n2.9  More recently, MAS released an information paper relating to Generative AI risks \n\nin July 2024 16 . The paper provides an overview of key cyber threats arising from \n\nGenerative AI, the risk implications, and mitigation measures that FIs could take         \n\n> 14 See https://www.mas.gov.sg/schemes -and -initiatives/veritas\n> 15 See https://www.mas.gov.sg/schemes -and -initiatives/project -mindforge\n> 16 See https://www.mas.gov.sg/regulation/circulars/cyber -risks -associated -with -generative -artificial -intelligence Artificial Intelligence Model Risk Management |7\n\nto address such risks. The paper covered areas enabled by Generative AI, such as \n\ndeepfakes, phishing and malware, as well as threats to deployed Generative AI, \n\nsuch as data leakage and model manipulation. \n\n# 3. Objectives and Key Focus Area s\n\n3.1  This information paper , which focus es on AI MRM, is part of MAS’ incremental \n\nefforts to ensure responsible use of AI in the financial sector . A key difference \n\nbetween an AI -based system and other system s is the use of one or more AI \n\nmodels within the system, which potentially increases uncertainties in outcomes .\n\nRobust MRM of such AI models is important to support the responsible use of AI .\n\n3.2  As the maturity of AI MRM may vary significantly across different FIs, MAS \n\nconducted a thematic review of selected banks’ AI MRM practices in mid -2024 .\n\nThe objective was to gather good practices for sharing across the industry .\n\n3.3  Based on information gathered during the review, MAS observed good practices \n\nby banks in the se key focus areas 17 :\n\n• Section 4: Oversight and Governance of AI \n\n- Updating of existing policies and procedures of relevant risk management \n\nfunctions to strengthen AI governance ;\n\n- Establish ing cross -functional oversight forums to ensure that evolving AI \n\nrisks are appropriately managed across the bank; \n\n- Articulat ing clear statements and principles to govern areas such as the \n\nfair, ethical, accountable and transparent use of AI ; and           \n\n> 17 For the purposes of the subsequent parts of this information paper, the good practices relating to AI would also apply to\n> Generative AI as practicable. Specific considerations relating to Generative AI will be covered in Section 7.1 .Artificial Intelligence Model Risk Management |8\n\n- Building capabilities in AI across the bank to support both innovation and \n\nrisk managemen t. \n\n• Section 5: Key Risk Management Systems and Processes \n\n- Identif ying AI usage and risks across the bank so that commensurate risk \n\nmanagement can be applied ;\n\n- Utilis ing AI inventories, which provide a central view of AI usage across \n\nthe bank to support oversight ; and \n\n- Assessing the materiality of risks that AI poses using key risk dimensions so \n\nthat relevant controls can be applied proportionately .\n\n• Section 6: Development and Deployment of AI \n\n- Establishing standards and processes for k ey areas that are important for \n\nthe d evelopment of AI , such as data management, robustness and \n\nstability, explainability and fairness , reproducibility and auditability ;\n\n- Conducting independent validation or peer review s 18  of AI before \n\ndeployment based on risk materialities ; and \n\n- Instituting p re -deployment checks and monitoring of deployed AI to \n\nensure that it behave s as intended, and application of appropriate change \n\nmanagement standards and processes where necessary .       \n\n> 18 The terms validation sand reviews are usually used interchangeably by banks to refer to assessments or checks of the AI\n> model development process, whether by an independent party, or another peer developer. More details on such validations\n> and reviews are provided in Section 6. 4.Artificial Intelligence Model Risk Management |9\n> Overview of Key Thematic Focus Areas\n\n3.4  These key focus areas are generally also applicable to Generative AI, as well as AI \n\n(including Generative AI ) from third -party providers. Nonetheless, there may be \n\nadditional considerations for Generative AI , as well as AI from third -party \n\nproviders. Hence, additional observations on good practices relating to \n\nGenerative AI and third -party AI are also outlined in Sections 7 .1 and 7.2 of this \n\ninformation paper respectively .\n\n3.5  The risks posed by AI and Generative AI extend beyond MRM and relate to non -\n\nAI specific areas such as general data governance and management, technology \n\nand cyber risk management, as well as third party risk management . These are \n\nnot covered in this information paper, and existing regulatory requirements and \n\nsupervisory expectations , including but not limited to notice s, guidelines or \n\ninformation papers on data governance, technology and outsourcing risk \n\nmanagement would apply , where relevant 19 .      \n\n> 19 Links to relevant publications are provided in Annex B.Artificial Intelligence Model Risk Management |10\n\n# 4. Governance and Oversight \n\nOve rview \n\nWhile existing control functions continue to play key roles in AI risk management , most \n\nbanks have update d governance structures, roles and responsibilities, as well as policies \n\nand processes to address AI risks and keep pace with AI developments . Good practices \n\ninclude :\n\n• establishing cross -functional oversight forums to avoid gaps in AI risk \n\nmanagement ;\n\n• updating control standards, policies and procedures, and clearly set ting out roles \n\nand responsibilities to address AI risks ;\n\n• developing clear statements and guidelines to govern areas such as fair, ethical, \n\naccountable and transparent use of AI across the bank ; and \n\n• building capabilities in AI across the bank to support both innovation and risk \n\nmanagement .\n\nExisting governance structures and such good practices are important to help support \n\nBoard and Senior Management in exercis ing oversight over the bank’s use of AI , and \n\nensure that the bank’s risk managemen t is robust and commensurate with its state of \n\nuse of AI .\n\n4.1 While existing risk governance framework s and structure s 20  continue to be \n\nrelevant for AI governance and risk management , a number of banks have \n\nestablish ed cross -functional AI oversight forums . Such forums serve as key \n\nplatform s for coordinating governance and oversight of AI usage across various \n\nfunctions . They also play an important role in addressing emerging challenges \n\nand potential gaps in risk management as the AI landscape evolves , and ensuring \n\nthat standards and processes, such as relevant AI development and deployment \n\nstandards , are aligned across the bank.                \n\n> 20 Aside from MRM, risk governance frameworks and structures from other areas that are usually relevant to AI risk\n> management include (but are not limited to )data, technology and cyber ,third -party risk management, as well as legal and\n> compliance .Artificial Intelligence Model Risk Management |11\n\n4.2 The mandates of these forums often include establishing a consistent and \n\ncomprehensive framework for managing AI risks, evaluating use cases that \n\nrequire broader cross -functional inputs, and reviewing AI governance \n\nrequirements to ensure they keep pace with the state of AI usage in the bank. \n\nData and analytics, r isk management, legal and compliance, technology, audit, as \n\nwell as other relevant busines s and corporate functions , are typically represented \n\nat such cross -functional oversight forums .\n\n4. 3 A number of banks have also found value in compiling policies and procedures \n\nthat are relevant to AI into a central guide to ensure that consistent standards \n\nfor AI are applied across the bank.  As more AI use cases are rolled out in banks, \n\nand the state of AI technology evolves, the use of AI may accentuate existing risks \n\nor introduce new risks. Hence, most banks have  reviewed and , where necessary, \n\nupdated existing policies and procedures to keep pace with the increasing use of \n\nAI across the bank , or new AI developments, e.g., updat ing policies and \n\nprocedures relating to performance testing of AI for new use cases , or \n\nestablishing new policies and procedures for AI models that are dynamically \n\nupdated based on new data .\n\n4. 4 Given the broad range of use cases for AI, and the potential for inappropriate \n\nuse, most banks have set out central statements and principles on how they \n\nintend to use AI responsibly, including developing guidelines to govern areas such \n\nas fair, ethical, accountable, and transparent use of AI  21  . Such efforts are \n\nimportant in setting the tone and establishing clear guidance on how AI should \n\nbe used appropriately across the bank, and to prevent potential harms to \n\nconsumers and other stakeholders arising from the use of AI . In addition to \n\ncentral statement s and principles , some banks have also taken steps to \n\noperationalise such central statement s and principles by mapping them to key                                    \n\n> 21 More details on these areas can be found in MAS ’publications relating to the FEAT principles under the Veritas Initiative .\n> Similar principles covering areas relating to the responsible or ethical use of AI in the financial sector have also been published\n> in other jurisdictions ,e.g. ,the Hong Kong Monetary Authority (HKMA) issued guiding principles for the use of big data analytics\n> and AI covering governance and accountability, fairness, transparency and disclosure, and data privacy and protection in 2019;\n> De Nederlandsche Bank (DNB) issued the SAFEST principles on s oundness, accountability, fairness, ethics, skills, and\n> transparency in 2019. Artificial Intelligence Model Risk Management |12\n\ncontrols , which are in turn mapped to the relevant functions responsible for \n\nthese controls .\n\n4. 5 Given the growing interest in AI, b anks also recognised the need to develop AI \n\ncapabilities and have established plans to upskill both their staff and senior \n\nexecutives . Aside from building awareness, banks have developed AI training that \n\nfacilitate staff in leveraging an d using AI in a n effective and responsible manner. \n\nSome banks have also set up AI Centre s of Excellence to drive innovation ,\n\npromote best practices and build AI capabilities across the bank .\n\n# 5. Key Risk Management Systems and \n\n# Processes \n\nOverview \n\nMost banks have recognised the need to establish or updat e key risk management \n\nsystems and processes for AI , particularly in the following areas: \n\n• policies and procedures for identifying AI usage and risks across the bank , so that \n\ncommensurate risk management can be applied ;\n\n• systems and processes to ensure the completeness of AI inventories, which \n\ncapture the approved scope of use and provide a central view of AI usage to \n\nsupport oversight ; and \n\n• assessment of the risk materiality of AI that cover s key risk dimensions, such as \n\nAI’s impact on the bank and stakeholders , the complexity of AI used , and the \n\nbank’s reliance on AI , so that relevant controls can be applied proportionately. \n\n5.1  Identification \n\n5.1.1  Identif ying where AI is used is important so that the relevant governance and risk \n\nmanagement controls can be applied . Even when using widely accepted \n\ndefinition s, such as the Organisation for Economic Co -operation and Artificial Intelligence Model Risk Management | 13 \n\nDevelopment’s  definition of AI 22 , considerable ambiguity remain s around the \n\ndefinition of AI due to its broad and evolving scope .\n\n5.1.2  Most banks leverage d definitions in existing MRM policies and procedures as a\n\nfoundation for identifying AI models 23 , and extended or adapted these definitions \n\nto account for AI -specific characteristics . Some banks shared that the uncertainty \n\nof model outputs is a common source of risk for both AI and conventional \n\nmodels 24 , and that the presence of such uncertainties was a key feature that was \n\nusually considered when identifying AI. MRM control functions also typically play \n\na key role in AI identification , often serving as the key control function \n\nresponsible for AI identification systems and processes , e.g., setting up \n\nattestation processes, or acting as the final arbiter in determin ing whether AI is \n\nbeing used . Some banks have also developed tools or portals to facilitate the \n\nprocess of identifying and classifying AI across the bank in a consistent manner. \n\n5.2 Inventory \n\n5.2.1  Banks mostly maintain a formal AI inventory 25  with a comprehensive record of \n\nwhere AI is used in the bank . A key area that an AI inventory supports, alongside \n\nthe relevant policies, procedures and systems, is to ensure that AI are only used \n\nwithin the scope in which they have been approved for use, e.g. , the purpose ,\n\njurisdiction, use case, application, system, and other conditions for which they \n\nhave been developed, validated and deployed. This is critical because \n\nunapproved usage of AI, particularly in higher -risk use cases, can lead to  \n\n> 22\n\nThe OECD’s definition of AI: An AI system is a machine -based system that, for explicit or implicit objectives, infers, from the \n\ninput it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence \n\nphysic al or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.  \n\n> 23\n\nThis could entail step -by -step guide to facilitate the identification of techniques that meet the bank’s definition of AI.  \n\n> 24\n\nModel s usually refer to quantitative algorithms, methods or techniques that process input data into quantitative estimates \n\nwhich may be used for analysis or decision making. Apart from AI models, which typically refer to machine or deep learning \n\nmodels, banks also routinely utilise conventional models, such as economic, financial, or statistical models. Some quantitative \n\nalgorithms, methods or techniques , such as logistic regressions , are commonly regarded as both AI and statistical models. A\n\nmore detailed definition of models can be found in Annex A.  \n\n> 25\n\nMost banks have established software system s for their AI inventor ies that not only record where AI is used in the bank, but \n\nmay also include additional features outlined above, such as automated tracking of approvals and issues , and identification of \n\ninter -dependences between AI . A small number of banks still rely on spreadsheets for their AI inventories, but this approach is \n\nmore prone to operational issues, e.g., outdated records, and would not allow for the additional features outlined above. Artificial Intelligence Model Risk Management | 14 \n\nunintended consequences. For example,  AI approved for use in one jurisdiction \n\nshould not automatically be treated as approved for use in others as the data, \n\nassumptions and considerations may not be similar , and the AI may not perform \n\nas expected in a different context .\n\n5.2.2  A few banks also utilise d their AI inventory system to track the use of AI through \n\ntheir lifecycle, and to establish checkpoints for different risk management \n\nprocesses at the various stages of the AI lifecycle. A few banks also use d the AI \n\ninventory to support the identification and monitoring of aggregate AI risks and \n\ninterdependencies across different AI models and systems . The AI inventory may \n\nalso serv e as a central repository for AI artifacts needed for model maintenance, \n\nvalidation and incident or issue management .\n\n5.2.3  Most banks have established clear policies on the scope of AI assets to be \n\ninventoried, the roles responsible for maintaining the inventory, and the \n\nprocesses for updating it. AI models are typically included within regular model \n\ninventories but specific tags or fields added to identify AI and capture AI -relevant \n\nattributes. One bank built an AI use case inventory that aggregate d information \n\nfrom the AI model inventory and other inventories or repositories relating to \n\nassets and controls in areas such as data, technology and operational \n\nmanagement . This provided the bank with a comprehensive and clear view of the \n\nlinkages between AI models and other relevant assets and controls. \n\n5.2.4  Across banks, AI inventories generally capture key attributes such as the AI’s \n\npurpose and description , scope of use, jurisdiction, model type, model output 26 ,\n\nupstream and downstream dependencies , model status, risk materiality rating, \n\napprovals obtained for validation and deployment , responsible AI requirements ,\n\nwaiver or dispensation details 27 , use of personally identifiable information (PII) 28 ,\n\npersonnel responsible such as owner s, sponsor s, users, developer s, and \n\nvalidator s. For third -party AI , additional attributes such as the AI provider, model              \n\n> 26 Model output refers to the type of output generated by the AI model. For example, the model output attribute could be the\n> likelihood of customer attrition, or the credit score of a customer.\n> 27 Waiver or d ispensation details refer to information about exceptions/special permissions granted, regarding the\n> development or deployment of AI, that deviate from the bank's standard policies and procedures .\n> 28 For example, full name, national identification number, personal mobile number. Artificial Intelligence Model Risk Management |15\n\nversion, endpoints utilised , as well as other details from the AI developers 29  may \n\nalso be included. \n\n5.3 Risk Materiality Assessment \n\n5.3.1.  Risk materiality assessment s are critical for banks to calibrate their approach to \n\nrisk management of AI across the diverse areas in which AI can be used (e.g., to \n\nmap the risk materiality of AI to the depth and scope of validation and monitoring \n\nrequired ). In assessing risk materiality , most banks considered both quantitative \n\nand qualitative risk dimensions that could generally be grouped into three broad \n\ncategories :\n\na.  Impact on the bank , its customers or other stakeholders , including but not \n\nlimited to financial, operational, regulatory and reputational impact . A few \n\nbanks developed granular, function -specific definitions of impact to provide \n\ngreater clarity .\n\nb.  Complexity due to the nature of the AI model or system, or the novelty of the \n\narea or use case in which AI is being applied .\n\nc.  Reliance on AI, which takes into account the autonomy granted to the AI, or \n\nthe involvement of human s in the loop as risk mitigant s.\n\n5.3. 2 Most banks have also established processes to review that risk materialit ies \n\nassigned to AI remain appropriate over time. Similarly, quantitative and \n\nqualitative measures and methods use d to assign risk materialities were also \n\nreviewed , e.g., measures used to quant ify financial impact would be updated if \n\nthe nature of the business in which AI was used had evolved.             \n\n> 29 These may be provided in AI or AI m odel cards , which are documents or information usually released alongside open -source\n> AI models that facilitate transparency and accountability by providing essential information on key areas such as the AI model’s\n> purpose, performance, limitations, ethical considerations .More information on details that may be included in such cards are\n> available in papers such as https://link.springer.com/chapter/10.1007/978 -3-031 -68024 -3_3 .Artificial Intelligence Model Risk Management |16\n\n# 6 Development and Deployment \n\nOverview \n\nMost banks have established standards and processes for development, validation, and \n\ndeployment of AI to address key risks. \n\n• For development of AI , key a reas that banks paid greater attention to include data \n\nmanagement, model selection, robustness and stability, explainability and \n\nfairness , as well as reproducibility and auditability .\n\n• For validation , banks required independent validation s or review s of AI of higher \n\nrisk materiality prior to deployment , to ensure that development and deployment \n\nstandards have been adhered to. For AI of lower risk materiality, most banks \n\nconducted peer reviews that are calibrated to the risks posed by the use of AI \n\nprior to deployment .\n\n• To ensure that AI would behave as intended when deployed and that any data \n\nand model drifts are detected and addressed, banks performed pre -deployment \n\nchecks, closely monitor ed deployed AI based on appropriate metrics , and appl ied \n\nappropriate change management standards and processes. \n\n6.1 Standards and Processes \n\n6.1.1.  To support robust risk management of AI across its lifecycle, banks have \n\nestablished standards and processes in the key areas of development, validation, \n\ndeployment, monitoring and change management . Most banks built upon \n\nexisting MRM standards and processes for development, validation, deployment, \n\nmonitoring and change management , but updated these standards and \n\nprocesses to address risks posed by AI. \n\n6.1.2.  Key standards and processes relating to conventional model development ,\n\nvalidation , deployment , monitoring and change management that banks Artificial Intelligence Model Risk Management | 17 \n\ngenerally  regard as relevant to AI are listed below 30 . Observations on key areas \n\nof focus for AI, and how banks have adapted or updated these standards and \n\nprocesses in these areas to address AI risks will be outlined in the subsequent \n\nsections. \n\na.  Data management - Determining suitability of data, such as the \n\nrepresentativeness of data for the intended objective, assessment of \n\ncompleteness , reliability , quality, and relevance of data, and approaches for \n\ndetermining train ing and test ing datasets. \n\nb.  Model selection - Defining the intended objective of the model and justifying \n\nhow the selection and design of the model is relevant and appropriate for \n\nachieving the desired objective , including the selection of architectures 31  and \n\ntechniques 32  that are appropriate for the use case and objecti ve .\n\nc.  Performance evaluation - Setting appropriate evaluation approaches and \n\nthresholds, and assessing the model’s ability to perform under a range of \n\nconditions in accordance with its intended usage and objective .\n\nd.  Documentation - Providing sufficient detail to facilitate reproducibility by an \n\nindependent party, including details on data sources , lineage , and processing \n\nsteps ; model architecture and techniques ; evaluation and testing approaches \n\nand results.  \n\n> 30\n\nAs highlighted previously, even prior to the use of AI models, banks already utilise d conventional models, such as economic, \n\nfinancial, or statistical models , and would have instituted model risk management standards and processes for such models .\n\nWhile these standards and processes may have precede d the use of AI model s in the bank , their general principles and \n\nconsiderations may also be applicable to AI models.  \n\n> 31\n\nModel architecture, in the context of AI, relates to the underlying structure and design of the model . It could involve choosing \n\nbetween decision tree -based models such as XGBoost, which were previously described in Section 2, or neural network -based \n\nmodels such as recurrent neural network or transformer models , based on various considerations. For example, d ecision tree -\n\nbased models may be more suitable for structured data, such as tabular data, while recurrent neural network or transformer \n\nmodels may be more suitable for text or time -series data as they are designed for sequential data.  \n\n> 32\n\nTechniques may include methods that are used to train a model from the data . In the context of AI, these may include \n\nsupervised learning techniques that use labelled data during training to learn how to generate predictions , or unsupervised \n\nlearning techniques which learn general patterns from unlabelled data . For more details on supervised and unsupervised \n\nlearning , please refer to Annex A. Artificial Intelligence Model Risk Management | 18 \n\ne.  Validation - Setting out the d epth of review expected of validators across the \n\nareas above ; frameworks for determining the prioritisation and frequency of \n\nvalidation (including any revalidation conducted on deployed models ).\n\nf.  Mitigating model limitations - Frameworks and processes for testing key \n\nassumptions, identifying limitations and their expected impact, and \n\nestablishing appropriate mitigants which are commensurate with the impact \n\nof the limitations. \n\ng.  Monitoring and change management - Setting appropriate tests and \n\nthresholds to evaluate the ongoing performance of a deployed model, \n\nincluding the frequency of monitoring; as well as the processes to be followed \n\n(e.g. , additional validations and approvals) for changes made to a deployed \n\nmodel. \n\n6.1.3.  When implementing standards and processes for risk management of AI , most \n\nbanks establish ed baseline standards and processes that applied to all AI across \n\nthe bank , regardless of risk materiality. For AI that were of greater risk \n\nmateriality, or where there were requirements specific to the use case , baseline \n\nstandards and processes would be supplemented by enhanced standards and \n\nprocesses . For example, additional evaluation or enhanced validation standards \n\nand processes could apply to AI used for risk and regulatory use cases where \n\nthere may be heightened requirements on performance evaluation or \n\nthresholds . The alignment of baseline standards and processes across the bank \n\nhelped ensure that key model risks were addressed consistently for AI with \n\nsimilar characteristics and risks regardless of where they were used in the bank. \n\n6.2 Data Management \n\n6.2.1  Robust data management is essential to support the development and \n\ndeployment of AI . G eneral bank -wide data governance and management Artificial Intelligence Model Risk Management | 19 \n\nstandards and processes 33  would apply to data used for AI. F or example, whether \n\ndata was used for reporting purposes or for AI systems , the same data \n\ngovernance committees generally oversee approvals and management of data \n\nissues. Similarly,  standards and processes for key data management controls such \n\nas basic data quality checks would also apply . However , to address AI -specific \n\nrequirements, all banks had established additional data management standards \n\nand processes to ensure that data used for AI development and deployment are \n\nfit for purpose . An overview of key data management areas for AI development \n\nand deployment that most banks generally focus ed on are listed below. \n\nStandards or processes relating to data management that are specific to AI \n\ndevelopment, validation, deployment, monitoring or change management are \n\ncovered in the subsequent sections. \n\na.  Appropriateness of data for AI use cases - Ensuring data used for \n\ndevelopment and deployment of AI are suitable for the context in which the \n\nAI is used , including assessing the use of such data against fairness and ethical \n\nconsideration s.\n\nb.  Representativeness of data for development - Ensuring data selected for \n\ntraining and testing AI models are representative of the real -world conditions ,\n\nincluding stressed conditions, under which the AI would be used .\n\nc.  Robust data engineering during development - Ensuring data processing \n\nsteps , 34  which may include additional data quality checks  35  , feature                               \n\n> 33 Please see MAS’ information paper on Data Governance and Management Practices for more details on general data\n> governance and management standards and processes . The paper covered governance and oversight, data management\n> function, data quality and data issue smanagement, which would also apply to data used for AI. Other relevant regulations and\n> publications include t he Personal Data Protection Act (PDPA) , which comprises various requirements on data privacy governing\n> the collection, use, disclosure and care of personal data, and provides a baseline standard of protection for personal data in\n> Singapore ;and Advisory Guidelines on Use of Personal Data in AI Recommendation and Decision Systems issued by the\n> Personal Data Protection Commission (PDPC) in March 2024 . Please refer to Annex B for the relevant links.\n> 34 Examples of data processing steps include missing value imputation, replacement of outlier values and standardi sation or\n> normalisation of data values .\n> 35 To ensure data quality, key areas such as data relevance, accuracy, completeness and recency may be assessed.\n\nArtificial Intelligence Model Risk Management | 20 \n\nengineering 36 , augmentation and labelling 37  of datasets , are robust and free \n\nof b ias , and that the integrity and lineage of data are che cked and tracked \n\nacross these data engineering steps .\n\nd.  Robust data pipelines for deployment - Establishing r obust controls around \n\ndata pipelines for deployment , includ ing continuous monitoring of the quality \n\nof data passed to deployed AI , as well as checks for anomalies, drift s, and \n\npotential bias that may have an impact on performance or fairness .\n\ne.  Documentation of data -related aspects for reproducibility and auditability -\n\nEnsuring key data management steps, such as data sourc ing , data selection, \n\ndata lineage, data processing , approvals and remediation actions taken for \n\ndata issues are documented to enable reproducibility and auditability. \n\n6.2.2  Some banks have also established additional data management standards and \n\nprocesses in the areas below: \n\na.  To ensure that data is being used appropriately when developing or deploying \n\nAI, a few banks have required approvals to be obtained for high -risk data use \n\ncases , such as data use where a third party may have access to the bank’s \n\ninternal data , use of employee data for monitoring, or the collection of \n\nbiometric data to identify individuals .                                     \n\n> 36 Features refer to the attributes of data points in a dataset, e.g., for data relating to a loan, the income of the obligor and\n> outstanding value of the loan are two possible attributes or features. Feature engineering refers to the process of selecting,\n> modifying or creating new features from the original attributes of a data set to improve a n AI model’s performance , e.g.,\n> normalising income of the obligor and outstanding value of the loan to a common scale ranging from 0 to 1; or creating new\n> derived features, such as a debt -to -income ratio , from existing attributes .\n> 37 When training AI models for a specific task, such as predicting a credit default or recommending a suitable financial product\n> to a customer ,we need data that includes the input variables (e.g. ,data relating to a past loan ,or customer history ), as well\n> as a target variable (e.g. ,whether there was a credit default for the loan, or a recommendation that the customer accepted ).\n> Data labelling refers to the process of assigning such target variables, typically based on past historical data or via human\n> annotation.\n\nArtificial Intelligence Model Risk Management | 21 \n\nb.  To support data reusability and reduce the time needed for feature \n\nengineering across the bank, as well as enhance consistency and accuracy in \n\nmodel development , a few banks have also built feature marts 38 .\n\nc.  To account for the greater use of unstructured data 39 , there were also ongoing \n\nefforts to more effectively manage such unstructured data , such as improving \n\nmetadata management and tagging for unstructured data to enable better \n\ndata governance 40 . Most of the data management areas outlined in paragraph \n\n6.2.1 are also generally appli cable to unstructured data , where relevant .\n\n6.3  Development \n\nModel Selection \n\n6.3.1  Given the trade -offs of adopting more complex AI models (e.g., higher \n\nuncertainties, limited explainability), most banks required developers to justify \n\ntheir selection of a more complex AI model over a conventional model or a\n\nsimpler AI model  41  , (e.g., balancing the need for performance against \n\nexplainability for a specific use -case ). Some banks required developers to go \n\nbeyond qualitative justifications, and develop challenger models (which could be \n\neither conventional or simpler AI models) to explicitly demonstrate the \n\nperformance uplift of the AI model over the challenger model as part of this \n\njustification. \n\n> 38\n\nA feature mart is a centralised repository or database that stores curated, pre -processed and reusable features (variables or \n\nattributes) that can be used for training models . Aside from supporting data reusability , feature marts may also help improve \n\ndata governance by maintaining metadata on each feature, including details on its source s, transformations, lineage and \n\nquality. Feature marts may also allow for v ersion control, ensuring that any updates to features are tracked. \n\n> 39\n\nUnstructured data refers to information that does not follow a predefined format or organised structure, making it more \n\ndifficult to store and analyse using traditional databases or methods for structured data . Unstructured data typically includes \n\ndata types such as text, images, videos, and audio. While the use of unstructured data is not new to banks , e.g., using \n\nsurveillance videos from cameras at ATMs, the use of such data is growing due to Generative AI .\n\n> 40\n\nThese may also include updating and adapting other areas such as data discovery and classification, access rights , data \n\nlifecycle management, data saniti sation and validation , and security controls for unstructured data . \n\n> 41\n\nFor example, a developer who wishes to use a more complex neural network -based deep learning model may be required \n\nto justify the need for such an AI model over a simpler tree -based machine learning model or a logistic regression model, and \n\nconsider the trade -offs based on the use case requirements. Artificial Intelligence Model Risk Management | 22 \n\nRobustness and Stability \n\n6.3.2  In assessing the overall suitability of AI models , banks placed heavy focus on \n\nensuring that AI models were both robust and stable 42 , and accordingly paid \n\nsignificant attention to i) the selection and processing of datasets used for \n\ntraining and testing AI models ; ii) determining appropriate approaches, measures \n\nand thresholds for evaluating AI models ; and iii) mitigating overfitting risk s43  that \n\noften arise due to the complexity of AI models . We outline some of the practices \n\nin these key areas below. \n\nSelection and Processing of Datasets for Training and Testing \n\n6.3.3  Datasets chosen for training and testing or evaluati on  44  of AI models were \n\nexpected to be representative of the full range of input values and environments \n\nunder which the AI model was intended to be used. Training and testing datasets \n\nwere also checked to ensure that their dist ribu tions or characteristics are \n\nsimilar 45 .\n\n6.3.4  Most banks also invested efforts in collecting testing datasets that allowed \n\npredictions or outputs from AI models to be tested or evaluated in the bank’s \n\ncontext as far as possible . For example, curating datasets that allow ed for AI \n\nmodel generated answers to queries from customers to be compared against \n\nanswers from in -house human expert s, or getting actual feedback from the \n\nbank’s customers on the quality of these AI model generated answers .\n\nEvaluation Approaches, Measures and Thresholds                                                      \n\n> 42 The concepts of robustness and stability in AI systems often overlap and what these terms cover can vary. For the purpose\n> of this information paper, r obustness refers to AI’s ability to achieve its desired level of performance under real -world\n> conditions ,while stability refers to the consistent performance of AI across arepresentative range of real -world scenarios.\n> These concepts are also related to the reliability of the AI system or model.\n> 43 Overfitting is when an AI model learns the training data overly well , to the point where it perform sextremely well on training\n> data but very poorly on new data that it has not seen in the training dataset .Intuitively, t his may mean that the model has\n> memorised the training examples rather than learning general patterns, resulting in poor performance in real -world conditions .\n> 44 The terms “testing ”and “evaluat ion ”of AI models are commonly used interchangeably to refer to the assessment of the\n> performance of AI models on datasets that it had not been trained on .\n> 45 This issue is also commonly referred to as training -testing skew, which are discrepancies between the distribution of data\n> used to train an AI model and the distribution of data it encounters during testing .Artificial Intelligence Model Risk Management |23\n\n6.3.5  Given that AI is developed to meet specific business needs or objectives, banks’ \n\nstandards and processes on the robustness and stability of AI models generally \n\nrequired test ing or evaluation approaches to be aligned with the intended \n\noutcomes that the AI models were meant to support. The exact approaches \n\nselected could differ depending on the nature of the AI model s, as well as the \n\nneeds of the use case. For example, assessing a fraud detection model’s ability to \n\nflag out known fraud cases by comparing against ground truth in historical data, \n\nor the usefulness of a financial product recommendation model through human \n\nfeedback. \n\n6.3.6  Correspondingly, w hile there are many established performance measures for AI \n\nmodels 46 , banks pa id significant attention to aligning the choice of performance \n\nmeasures with the intended outcomes that the AI models were meant to \n\nsupport. In some cases, this could involve trade -offs between different \n\nperformance measures. For example, if the intended outcome was to detect as \n\nmany instances of fraud as possible, performance measurement would need to \n\nfocus more on the proportion of false negatives (i.e. fraudulent instances that \n\nwere not detected) , even though this may come at the expense of a higher \n\nproportion of false positives (i.e. instances falsely flagged by the model as being \n\nfraudulent) .\n\n6.3.7  Other tests that banks may include to ensure robustness and stability 47  include \n\nthe following :\n\na.  Sensitivity analysis to understand how predictions or outputs of AI models \n\nchange under different permutations of data inputs . This also helps to identify \n\nimportant features that significantly influence predictions or outputs , and \n\nfacilitate explanations of the behaviour of AI models .                   \n\n> 46 There are a wide range of performance measures for AI models, and these are often specific to the task or use -case ;for\n> example, recall, precision, or F1 for classification tasks, mean absolute error or root mean squared error for regression tasks,\n> mean average precision or mean reciprocal rank for recommendation tasks.\n> 47 The objectives of some of these tests overlap, and may also relate to data management aspects that we outlined earlier .\n> Nonetheless, we list all the tests that we observed across the banks for completeness. Artificial Intelligence Model Risk Management |24\n\nb.  Stability analysis to compare the stability of data distribution s and predictions \n\nor outputs, e.g ., assessing whether the distribution of a training dataset from \n\nan earlier period matches the distribution of testing dataset s from more \n\nrecent period s, and how differences affect the performance of AI models .\n\nc.  Sub -population analysis , which are evaluations of how AI model s perform \n\nacross different sub -populations or subsets within the datasets (e.g., to \n\nidentify any significant differences in performance between different \n\ncustomer segments). Such analysis of sub -populations or subsets within the \n\ndatasets help to identify potential issues that might not be obvious in the \n\naggregated testing dataset, as well as potential sources of bias, which could \n\nsupport fairness assessments of AI models where necessary (e.g., wh ere sub -\n\npopulations relate to protected features or attributes such as race or gender ). \n\nd.  Error analysis to identify potential patterns in prediction errors (e.g., \n\nmisclassified instances), which help s to understand the limitations of AI \n\nmodel s.\n\ne.  Stress testing the response of AI models to edge cases or inputs outside the \n\ntypical range of values used in training. This allowed banks to better \n\ndetermine performance boundaries and identify limitations of AI model s.\n\nSome banks also tested the behaviour of AI model s in the context of \n\nunexpected inputs or conditions. Examples included adversarial testing or red \n\nteaming types of exercises . Such testing is especially important in the context \n\nof AI models used in high risk or customer -facing applications , as it allowed \n\nthe bank to establish conditions under which AI models would not perform as \n\nexpected or could introduce potential security or ethical concerns .\n\n6.3.8  Most banks would establish criteria or thresholds for performance measures , to \n\ndefine what was considered acceptable performance. Such thresholds need to \n\nbe clearly defined and documented , as well as mutually agreed upon by \n\ndevelopers and validators. Such thresholds were usually use case specific, and \n\ncould also be used subsequently to facilitate validation, pre -deployment checks, \n\nas well as monitoring and change management. Artificial Intelligence Model Risk Management | 25 \n\nMitigating Overfitting Risks \n\n6.3.9  The large number of parameters and inherent complexity of AI models increases \n\nthe risks of them overfitting on training data (in -sample data) and hence \n\nperforming poorly when deployed on out -of -sample data. Banks employed a \n\nvariety of mitigants to address this risk: \n\na.  Model selection – Generally f avouring AI models of lower complexity unless \n\nthere are clear justifications to do otherwise ; or adopting approaches that \n\nconstrained the complex ity of AI models 48 .\n\nb.  Feature selection - Applying explainability methods to identify the key input \n\nfeatures or attributes that are important for the AI model predictions or \n\noutputs 49  and assess ing that they are intuitive from a business and/or user \n\nperspective 50 .\n\nc.  Model evaluation - Additional performance testing requirements to test the \n\nperformance of AI model s on unseen data where possible, such as cross -\n\nvalidation techniques 51  and testing against more out -of -sample/out -of -time 52 \n\ndatasets. \n\nExplainability \n\n6.3.10  All banks identified explainability as a key area of focus for AI, particularly for use \n\ncases where end -users or customers need to understand key features or \n\nattributes in the data influencing predictions of AI models . For example, \n\nexplainability would be more important in higher risk materiality use cases w here  \n\n> 48\n\nExamples include regulari sation techniques or limiting the number and depth of trees for gradient boosting trees. Such \n\ntechniques generally try to limit the number of parameters used so that the trained model is less complex. For example, some \n\nregularisation  techniques  force less important parameter s to values of zero.  \n\n> 49\n\nAs discussed in the next section on explainability methods.  \n\n> 50\n\nAdditional justification would typically be required to retain features or attributes that were not intuitive, or which did n ot \n\nmeaningfully contribute to the overall performance of the models. Such data may introduce more noise, and cause the AI \n\nmodel to overfit on the noise, leading to poor performance in real world conditions.  \n\n> 51\n\nCross -validation generally refers to techniques to evaluate models by resampling the dataset for training and testing . An \n\nexample would be K-fold cross -validation (which involve s splitting the dataset into K parts for K training and testing rounds) . \n\n> 52\n\nAn out -of -sample testing dataset is a sub set of data not used in model training , whereas an out -of -time testing dataset is a\n\nsubset of data obtained from a time period distinct from the time period of the subset of data used in training the model. Artificial Intelligence Model Risk Management | 26 \n\nbank staff making decisions based on predictions  of AI models need to \n\nunderstand the key features or attributes 53  contributing to the prediction; or in \n\nuse cases where a customer may ask for reasons for being denied a financial \n\nservice. Hence, development standards for AI across all banks ha d been \n\nexpanded to include a section on explainability. \n\n6.3.11  Explainability requirements in banks’ standards and processes generally required \n\ndevelopers to apply global and/or local 54  explainability methods to identify the \n\nkey features or attributes used as inputs to AI model s and their relative \n\nimportance ; assess whether these features or attributes were intuitive from a \n\nbusiness and/or user perspective ; and provide additional justification for \n\nretaining features o r attributes which were not intuitive. Such methods could \n\nalso help identify the usage of potentially sensitive features as part of fairness \n\nassessments. Some banks had set out a list of global and local explainability \n\nmethods that could be applied to explain the outputs  55  of AI models . Such \n\nmethods could be directly applied during development as part of the feature \n\nselection process, or used within explainability tools developed as part of the AI \n\nsystem so that either global and/or local explanations can be provided alongside \n\npredictions or outputs generated by AI models post -deployment. \n\n6.3.12  In terms of the level of explainability required for different use cases, some banks \n\nestablished standards and processes to clearly define the minimum level of global \n\nand/or local explainability required for different use cases. For these banks,  \n\n> 53\n\nAn example of a feature or attribute in this context could be the income of the customer.  \n\n> 54\n\nGlobal explainability is the ability to understand the overall functioning of the model by identifying how input features drive \n\nmodel outputs at an overall model level. Local explainability is the ability to identify how input feature s drive the model output \n\nfor a specific observation or instance. Taking a fraud detection model as an example, global explainability methods allow for \n\nidentification of the most important features , such as the high values of transactions, used to detect fraudulent transactions \n\nfor the model in general . However, the key features that are important for a specific transaction (i.e. the local instance) may \n\nnot necessarily be the same, e.g., the value of the transaction may be small for a specific instance but the transaction is still \n\ndetected as a fraudulent transaction due to specific characteristics of the parties involved in the transaction , such as an \n\nunfamiliar geographic location of one of the parties . Local explainability methods help to identify such features for the local \n\ninstance.  \n\n> 55\n\nCommon examples of explainability methods include SHAP (for global and local explainability) and LIME (for local \n\nexplainability). SHAP generates Shapley values for each feature based on its contribution to a given model output. A global -\n\nlevel explanation can be generated by generating a summary plot of the Shapley values of the key features, across the entire \n\nset of model outputs . LIME is based on training a separate model for the local instance that needs to be explained . The \n\nexplanation that is generated is based on the separately trained model .Artificial Intelligence Model Risk Management | 27 \n\nfactors considered  when applying a higher standard of global and/or local \n\nexplainability included risk materiality or the extent to which AI -driven decisions \n\nwere likely to require explanations (e.g. , to the bank’s customers) for the use \n\ncase . For example, AI model s used for credit decisioning could require the most \n\nexacting standards for global and local explainability, requiring developers to \n\ncarefully consider all features used as inputs and provide justifications for their \n\nuse , as well as the ability for users to easily identify key features influencing any \n\ngiven prediction post -deployment. Other banks required global and/or local \n\nexplainability to be explored across all AI, but allowed users and owners to decide \n\non the acceptable level of explainability , and justify their decision based on the \n\nuse case .\n\nFairness \n\n6.3.13  The outputs of AI models are inherently influenced by the patterns learnt from \n\nits training data. If th e training data contain ed biases that unfairly represent or \n\ndisadvantage specific groups of individuals, AI model s may perpetuate these \n\nunfair biases in its predictions or outputs. This could lead to decisions or \n\nrecommendations that disproportionately and unfairly impact certain \n\ndemographic groups. \n\n6.3.14  The earlier section on data management had outlined the need for fairness to be \n\nconsidered during development, and for checks and monitoring of potential \n\nbiases during deployment. More specifically, d uring AI development, for use \n\ncases that could have a significant impact on individuals, most banks would \n\nundertake a formal assessment on whether specific groups of individuals could \n\nbe systematically disadvantaged by AI -driven decisions. The scope of such \n\nassessments could vary between banks depending on the relevant rules, \n\nregulations or expectations applicable to the bank 56 , and between use cases \n\ndepending on the risk materiality of the AI .    \n\n> 56 Examples of such expectations on fairness for AI used by banks across jurisdictions include the Principles to Promote Fairness,\n> Ethics, Accountability and Transparency (FEAT) in the use of Artificial Intelligence and Data Analytics in Singapore’s Financial\n> Sector , published by MAS in 2018; General Principles for the use of Artificial Intelligence in the Financial Sector , published by\n\nArtificial Intelligence Model Risk Management | 28 \n\n6.3.15  Generally, the approach for assessing fairness used by banks involved the \n\nfollowing steps: \n\na.  Defining a list of protected features or attributes, for which use of such \n\nfeatures or attributes in AI models would require additional analysis and \n\njustification. Common examples of such protected features or attributes \n\ninclude gender, race or age. \n\nb.  Determin ing whether such features or attributes 57  were used in training AI \n\nmodel s. Based on this assessment, to define groups of individuals at risk of \n\nbeing systematically disadvantaged by the AI -driven decisions (at -risk groups) .\n\nc.  Where necessary, determining the extent to which AI -driven decisions \n\nsystematically disadvantaged against at -risk groups. The was usually assessed \n\nvia fairness measures (e.g., fairness measures that are available in the toolkit \n\nreleased by the Veritas Initiative) .\n\nd.  Where necessary, providing adequate justifications on the use of protected \n\nfeatures or attributes in AI models (e.g. , trade -offs against the intended \n\nobjectives of the AI model 58 ). \n\nReproducibility and Auditability \n\n6.3.16  Reproducibility and auditability 59  of AI development are essential for ensuring \n\naccountability and building trust in AI systems . To facilitate reproducibility and \n\nauditability of AI, most banks expanded existing documentation requirements to \n\nincorporate the relevant AI development processes and considerations. A list of                 \n\n> De Nederlandsche Bank in 2019; and the High -level Principles on Artificial Intelligence , published by the Hong Kong Monetary\n> Authority in 2019.\n> 57 These could include proxy attributes that are heavily correlated with such protected attributes .\n> 58 This could be supported by, for example, analysis on the difference in performance between an AI model which included\n> these protected features or attributes, and an AI model which did not. An informed assessment could then be made on whether\n> this differenc e in performance was necessary to achieving the model's intended objective, taking into consideration the level\n> of potential harm done to at -risk groups arising from the use of the AI model.\n> 59 Reproducibility refers to “the ability of an independent verification team to produce the same results using the same AI\n> method based on the documentation made by the organisation ”, while audibility refers to “the readiness of an AI system to\n> undergo an assessment of its algorithms, data and design processes ” ( Model AI Governance Framework, IMDA Singapore. )\n\nArtificial Intelligence Model Risk Management | 29 \n\nkey documentation requirements  for AI commonly seen across banks are as \n\nfollow s:\n\na.  Data - Documentation of key data management steps is important to facilitate \n\nreproducibility and auditability. During development, key information that \n\nwould usually be documented include datasets and data sources used for \n\nmodel development and evaluation, details of how these datasets were \n\nassessed as fit -for -purpose, processed ahead of model training, and split into \n\nrelevant training, testing and/or validation 60  datasets. \n\nb.  Model training - Details of how the AI model was trained or fit to the training \n\ndataset. Such details could include codes (along with software \n\npackages/environment used and their relevant versions), key settings (e.g. ,\n\nhyperparameters 61  used and the approach for selecting hyperparameters 62 ), \n\nrandom seed values 63  and any other configuration s required for a third party \n\nto reproduce the training process. \n\nc.  Model selection - Details of how the performance of the AI model was \n\nevaluated and how the final model was selecte d. Such details could include \n\nthe evaluation approaches, thresholds and datasets applied  64  and the \n\ncorresponding results, comparison s of performance across multiple AI models \n\nand justification s for selecting the final model. \n\nd.  Explainability - Global and/or local explainability methods used , feature \n\nselection process, analysis of results , as well as description of key features \n\nselected and additional justifications for inclusion of certain key features (e.g .,\n\nfeatures that may not have appear ed to be important to a human expert ).                          \n\n> 60 Testing and validation datasets refer to datasets used to evaluate the performance of the model outside of the dataset used\n> to train the model. This should be distinguished from independent validation, which is the process of independently assessing\n> the overall suitability of the model.\n> 61 E.g. ,number of trees and maximum tree depth for gradient boosted trees.\n> 62 E.g. ,grid search, random search of hyperparameters .\n> 63 AI models usually need to be in itialised with a random set of numbers (e.g., for the model parameters) before training , and\n> documenting the random seed value that is used to initialise the AI models is necessary to reproduce the AI model’s behaviour\n> and results.\n> 64 As detailed in the earlier sub -section on Robustness & Stability. Artificial Intelligence Model Risk Management |30\n\ne.  Fairness - Metrics and associated thresholds, results of fairness assessments \n\nand justifications for the use of any protected features or attributes .\n\n6.3.17  Alongside documentation requirements in the relevant standards and processes, \n\nmost banks also set up documentation templates that developers were required \n\nto follow for consistency . Such templates were typically designed by the bank’s \n\nMRM function. Templates could differ between business domains (as different \n\nperforma nce tests or metrics could apply) or between AI of different risk \n\nmaterialities (as documentation requirements could be higher for AI of higher \n\nrisk materiality ). \n\n6.4 Validation \n\n6.4.1  Independent validation provides an objective and unbiased assessment of the \n\nsuitability, performance and limitations of AI. It acts as a n important challenge to \n\ndevelo pers , and ensures that the relevant standards and processes have been \n\nadhered to when developing AI. \n\n6.4.2  The validation process typically involves an independent unit 65  reviewing the AI \n\ndevelopment process and documentation, assessing that AI performs and \n\nbehaves as intended, and undertaking pre -deployment checks. Actions to \n\naddress issues identified during validation, such as the application of suitable \n\nadjustments or other mitig ating or compensat ory controls, would typically be \n\nproposed by developers and agreed to by validators before deploying AI. \n\n6.4.3  Building on their conventional MRM processes, banks have equipped \n\nindependent validation functions with the skills and incentives needed to \n\nconduct independent review of AI used in the bank , which include invest ments \n\nin efforts to ensure that independent validation staff have the relevant technical \n\nexpertise for AI.   \n\n> 65 For example, t he Federal Reserve/Office of the Comptroller of the Currency’s SR Letter 11 -7 on Supervisory Guidance on\n> Model Risk Management states that validation should generally be done by individuals not responsible for development or use\n> and do not have a stake in whether a model is determined to be valid.\n\nArtificial Intelligence Model Risk Management | 31 \n\n6.4.4  Banks adopted a range of approaches in establishing independent validation \n\nrequirements across different AI. One bank required all AI to be subject to \n\nindependent validation, with the depth and rigour of validation varying based on \n\nthe AI’s risk materiality rating. Most other banks required independent validation \n\nonly for AI of higher risk materiality , with other AI subject only to peer review 66 .\n\nEven for AI of lower risk materiality, the involvement of either an independent \n\nvalidator or peer reviewer allowed for some degree of challenge that helped to \n\nbetter manage the added uncertainties and risks posed by AI, and check that such \n\nAI was developed in accordance with the bank’s standards and processes. \n\n6.5 Deployment , Monitoring and Change Management \n\nPre -Deployment Checks \n\n6.5.1.  Aside from checks during the validation process, p re -deployment checks and \n\ntests are important to ensure that the AI has been correctly implemented and \n\nproduces the intended results before being deployed for use . Banks placed \n\nsignificant focus on implementing controls for the deployment of AI to ensure \n\nthat the AI functions as intended in the production environment 67 . These controls \n\nwere usually based on existing technology risk management guidelines . F or \n\nexample, b anks would apply standard software development lifecycle (SDLC) \n\nprocesses to ensure that the AI application or system was secure, free from error \n\nand perform ed as intended before deployment 68 . Some banks also conduct ed \n\nadditional check s to ensure that the deployed AI ’s scope, output and \n\npe rformance , and associated controls align with that of the validated AI :\n\na.  Additional tests , such as :                 \n\n> 66 As compared to independent validation, peer reviews were usually conducted by a non -independent function (e.g. ,a\n> different development team in the same unit/reporting line as the original model developers).\n> 67 A production environment is alive operational setting where deployed systems, such as deployed AI models, are run under\n> real world condit ions to deliver services or perform tasks for end -users.\n> 68 Please see MAS’ Technology Risk Management Guidelines for further details on the adoption of sound and robust practices\n> for the management of technology risk in these areas: https://www.mas.gov.sg/regulation/guidelines/technology -risk -\n> management -guidelines\n\nArtificial Intelligence Model Risk Management | 32 \n\ni.  forward testing , which are experimental runs using a limited set of \n\nproduction data or with a limited set of users, for selected high materiality \n\nuse cases to assess the behaviour of AI in an environment similar to when \n\nthe AI is fully deployed ; and \n\nii.  live edge case test ing to assess how AI handles edge cases in the \n\nproduction environment , which helps to verify that AI can handle a variety \n\nof improbable but plausible scenarios when deployed .\n\nb.  Automated pipelines , such as setting up automated deployment and \n\ncontinuous integration/continuous deployment (CI/CD ) pipelines  69  to \n\nminimi se human error and maintain ing a consistent process for how AI is \n\ndeployed , monitored, and maintained , which is important for AI given the \n\nneed for regular data and model updates .\n\nc.  Process management , which includes checks to ensure that key processes \n\nimportant for the deployed AI, such as human oversight , backup models , and \n\nother appropriate controls and contingencies , are in place ; and business \n\nprocess change management, such as training users to understand AI \n\nlimitations and to use AI appropriately .\n\n6.5.2.  Non -AI specific pre -deployment checks  70  remain relevant, hence key control \n\nfunctions, such as those in the areas of technology, data, legal and compliance, \n\nthird -party and outsourcing , would also confirm that the checks have been \n\nundertaken and sign off before AI is deployed into production.               \n\n> 69 Continuous integration/continuous deployment (CI/CD) pipelines automate the process of building, testing, and deploying\n> code changes , and reduce the potential of errors arising from manual interventions .Approvals and checks are also usually\n> integrated into the CI/CD process to ensure that new code pushed into production are checked for errors .More details on\n> CI/CD, as well as other related terms such as MLOps and AIOps are provided in Annex A.\n> 70 For example, checks relating to cyber -security , or compliance with outsour cing policies .Artificial Intelligence Model Risk Management |33\n\nMonitoring Metrics and Thresholds \n\n6.5.3.  Monitoring is particularly critical for AI given their dynamic nature and the \n\npotential for AI model staleness due to drifts  71  in either data or the model \n\nbehaviour over time. All b anks pa id significant focus to the ongoing monitoring \n\nof their AI to ensure that they continue to operate as intended post -deployment. \n\nKey measures that were monitored generally follow those that were covered \n\nduring development and validation, and include robustness , stability, data \n\nquality, and fairness measures .\n\n6.5.4.  Measures used for monitoring were tracked against predefined thresholds ,\n\nusually determined at the development and validation stages,  to ensure models \n\nperform within acceptable boundaries . Some banks have also implemented \n\ntiered thresholds, for example, additional  early warning thresholds to pre -empt \n\nmodel deterioration, and different thresholds to determine when retraining or a \n\nfull redevelopment of the AI may be necessary. \n\n6.5.5.  Most banks also have a process or system for reporting , tracking and resol ving \n\nissues or incidents if breaches or anomalies arise from the monitoring process. \n\nBanks generally track issues or incidents from discovery to resolution, and \n\nincorporate a relevant escalation process based on the materiality of the issue or \n\nincident. The resolution process may include AI model retrai ning ,\n\nredevelopment , or decommissioning as possible outcomes. Where a major \n\nredevelopment was undertaken , revalidation and approval would be needed \n\nbefore the updated model could be redeployed .   \n\n> 71 AI models can perform poorly when they become stale due to factors such as data drift, concept drift or model drift, which\n> are essentially due to changes in the data distributions, relationships between input data and predictions/outputs, or the\n> general e nvironment in which the AI model is being used. More details on data, concept and model drifts are provided in Annex\n> A. Artificial Intelligence Model Risk Management |34\n\nContingency Plans \n\n6.5.6.  All banks would generally have standards and processes relating to contingency \n\nplans for AI, particularly those supporting high -risk or critical functions 72 . These \n\nplans , which may not be specific to AI, typically outline fallback options, such as \n\nalternative systems or manual processes, and would be subject to regular \n\nreviews and testing to ensure readiness for rapid activation when necessary. For \n\nmission -critical AI applications 73 , a few banks may also have kill switches in place. \n\nKill switches are used to deactivate AI if they exceed risk tolerances , and require \n\nclear contingency plans to be quickly rolled out .\n\nReview and Re validations \n\n6.5.7.  Aside from ongoing monitoring, banks also conduct ed periodic reviews of their \n\nportfolio of AI . Key aspects that that were usually reviewed include changes in \n\nthe model s’ materiality, risks, scope and usage, performance, assumptions and \n\nlimitations, and identification and remediation of issues .\n\n6.5.8.  Banks also have standards and processes for ongoing re validations of AI in \n\nproduction, with the intensity and frequency based on the materiality of the AI .\n\nIn general, AI deemed critical to risk management, regulatory compliance, \n\nbusiness operations, or customer outcomes are revalidated more frequently and \n\nintensely .\n\nChange Management \n\n6.5.9.  Standards and processes relating to AI change management are needed to \n\nensure that what constitutes a change is clearly defined, and that the appropriate \n\ndevelopment and validation requirements are applied. M ost banks require d    \n\n> 72 Such contingency plans may not apply specifically to AI, but to technology systems in general. Nonetheless, they may require\n> additional considerations in the case of AI, e.g., AI -specific performance monitoring thresholds to determine when to trigger\n> the contingency plan, or a backup plan that involves another AI system or model.\n> 73 For example, for AI that are used for trading .\n\nArtificial Intelligence Model Risk Management | 35 \n\nsignificant  or material changes 74  to AI in production to be reviewed and approved \n\nby the control functions prior to implementation , so as to  ensure that any \n\nmodifications made to the model do not negatively impact its performance . To \n\nmanage changes to AI , b anks have also established systems and processes for \n\nversion control of both internal and third -party AI (which do not only cover code \n\nrelating to AI , but also data and other artifacts such as hyperparameters and the \n\ntrained model parameters or weights ). Version control enable s banks to track \n\nchanges across different aspects of AI and roll -back to previous version s of AI \n\nwhere ne cessary 75 . Most b anks have also set up processes for third -party AI \n\nproviders to provide notification s of version updates 76 .\n\n6.5.10.  AI for certain use cases, such as fraud detection, may need to be changed or \n\nupdated more frequently 77 , due to drifts in the data or the behaviour of the AI \n\nmodel over time . To deal with such frequent changes, some banks have \n\nestablished systems and processes for the automatic updating of such AI. Such \n\nAI, which some banks refer to as “dynamic AI ”, need to be subject to enhanced \n\nrequirements and controls to ensure that change management is well governed .\n\nKey additional requirements and controls include justification s for enabling \n\nautomatic updating of AI , clearly defining what can be updated automatically , for \n\nexample, restricting changes to the retraining of AI model with more recent \n\ndatasets , but not allowing for changes to AI model architectures or \n\nhyperparameters . Such dynamic AI would also be subject to enhanced risk  \n\n> 74\n\nExample s of significant or material change s include fundamental changes to AI model architecture s or training technique s.\n\nSuch changes may necessitate an in -depth revalidation, compared to less significant changes , such as retraining the AI model \n\nwith more recent data , which may only require checks on AI performance to ensure the AI is still behaving as expected.  \n\n> 75\n\nWhile we cover version control here under change management whe re the AI is already deployed , it is important to note \n\nthat version control for AI also plays a key role during the development and validation stages. For example, v ersion controls \n\nare needed to support iterative improvements and collaboration during development , and also help to ensure reproducibility \n\nand auditability during validation.  \n\n> 76\n\nWhile banks generally try to require third -party providers to notify them of any changes to the AI model or service, there \n\nmay be circumstances where such notifications may not happen, e.g., the third -party provider may not notify end -users on \n\nchanges that they view as immaterial. We have observed banks trying to address this by setting out clearer terms in their legal \n\nagreement s, for example, adding a clause that requires the third -party provider to notify banks on an y upcoming change s to \n\nthe AI model or system.  \n\n> 77\n\nFor example, if we compare a fraud detection use case with a n NLP use case such as summarisation of customer call \n\ntranscripts, data relating to the behaviour of scammers would usually change much more frequently than data relating to \n\ncustomer calls due to the active efforts of scammers to evade detection. Artificial Intelligence Model Risk Management | 36 \n\nmanagement requirements, such as enhanced  data management standards, e.g., \n\nadditional checks on data quality and drifts , as well as enhanced performance \n\nmonitoring requirements , e.g., more stringent monitoring notification \n\nthresholds .\n\n# 7 Other Key Areas \n\n7.1 Generative AI \n\nOverview \n\nWhile t he use of Generative AI in banks is still in the early stages , banks generally try to \n\napply existing governance and risk management structures and processes where relevant \n\nand practicable, and balance innovation and risk management by adopting :\n\n• Strategies and approaches , where they leverage on the general -purpose nature of \n\nGenerative AI by focusing on the development of key enabling modules or services; \n\nlimit the current scope of Generative AI to use cases for assistin g/ augmenting \n\nhumans or improving internal operational efficiencies that are not direct customer \n\nfacing; and buil ding capacity and capabilities by establishing pilot and \n\nexperimentation frameworks ;\n\n• Process controls , such as setting up cross -functional risk control checks at key \n\nstages of the Generative AI lifecycle ; establishing more detailed development and \n\nvalidation guidelines for different Generative AI task archetypes ; requir ing human \n\noversight for Generative AI decisions ; and paying close attention to user education \n\nand training on the limitations of Generative AI tools ; and \n\n• Technical controls , such as selection, testing and evaluation of Generative AI \n\nmodels in the context of the bank’s use cases; developing reusable modules to \n\nfacilitate testing and evaluatio n; assessing different aspects of Generative AI model \n\nperformance and risks; establish ing input and output filters as guardrails to address \n\ntoxicity, bias and privacy issues ; and mitigating data security risks via measures \n\nsuch as the use of private clouds or on -premise servers, data loss prevention tools, \n\nand limiting the access of Generative AI to more sensi tive information. Artificial Intelligence Model Risk Management | 37 \n\n7.1.1.  In addition to the k ey areas highlighted in the prior sections, there are some \n\naspects relating to Generative AI (compared to conventional AI) that require \n\nfurther consideration: \n\na.  Higher uncertainties associated with Generative AI – The risks of \n\nhallucinations and unexpected behaviours by Generative AI given its greater \n\ncomplexity may lead to less robust and stable performance , and was a key \n\nconcern highlighted by banks . This concern was particularly pronounced for \n\nuse cases of higher risk materiality or those that are directly customer -facing ,\n\nwhere greater reliability was required .\n\nb.  Difficulties in evaluating/testing Generative AI and mitigating its limitations \n\n– Compared to conventional AI, which were typically used by banks for \n\nspecific use cases that the AI models had been trained for, Generative AI are \n\nmore general -purpose in nature and can be used in a wider range of use cases \n\nin the bank. However, there may be a lack of easily available ground truths 78 \n\nin some of these newer use cases to evaluate and test Generative AI. Use \n\ncases involving Generative AI also typically involve unstructured data , such as \n\ntext data , for which there are significantly more possible permutations ,\n\ncompared to structured data usually used for conventional AI . This makes it \n\nchallenging to foresee all potential scenarios and perform comprehensive \n\ntesting and evaluations 79 .\n\nc.  Lack of transparency from Generative AI providers - Unlike conventional AI \n\nmodels, which are often developed and trained internally by the bank’s \n\ndevelopers, Generative AI used by banks were pre -dominantly based on pre -\n\ntrained models from external providers. As disclosure standards relating to \n\nsuch AI are still evolving globally, banks may lack full access to essential risk          \n\n> 78 Ground truth refers to reliable or factual information that serves as a standard against which the outputs or predictions of\n> AI models, including Generative AI models, can be evaluated.\n> 79 For example, it is significantly harder to evaluate the quality of a summary or of an image generated by Generative AI,\n> compared to evaluating the accuracy of a simple yes/no prediction from conventional AI. It is also harder to foresee all possible\n> permutations of text or images that may be used as inputs to Generative AI, as well as all possible permutations of text or\n> images that may be generated by Generative AI.\n\nArtificial Intelligence Model Risk Management | 38 \n\nmanagement information, such as details about the underlying data used in \n\nmodel training and testing, as well as the extent of evaluation or testing \n\napplied to these models. \n\nd.  Challenges in explainability and fairness with Generative AI – The lack of \n\ntransparency from external providers may also contribute to challenges in \n\nunderstanding and explaining the outputs and behaviour of Generative AI ,\n\nand ensuring that the outputs generated by Generative AI are fair. There is \n\nalso a general lack of established methods currently for explain ing Generative \n\nAI outputs and assessing their fairness .\n\n7.1.2.  Most b anks are in the process of reviewing and updating parts of their AI model \n\nrisk management framework for Generative AI to balance the benefits and risks \n\nof its use. \n\n7.1.3. The subsequent paragraph s outline observations from the thematic on key \n\napproaches and controls that banks have adopted to balance innovation and risks \n\nbased on the current state of use of Generative AI . It should be noted that these \n\napproaches and controls will need to be updated as Generative AI technology \n\nevolves , and that risk management efforts will need to be scaled accordingly \n\nbased on the state of Generative AI use across the institution .\n\nStrategie s and Approaches \n\n7.1.4. Some banks have invested significant effort in identifying and building key \n\nenabling services and modules for Generative AI that can be utilised across \n\nmultiple use cases, e.g., vector databases 80 , retrieval systems 81 , evaluation and           \n\n> 80 Data, particularly unstructured data, such as text and images, need to be encoded into numerical representations before\n> they can be used for AI or Gen erative AI. Such numerical representations are commonly referred to as vectors. Vector\n> databases are speciali sed database systems designed to store, index, and efficiently query such data.\n> 81 Retrieval systems help to search information repositories and retrieve the most relevant information for a specific task. For\n> example, to help answer a query relating to information in a corporate information repository, the retrieval system will help\n> to search for the most relevant pieces of information in the corporate information repository . The retrieved information is then\n> usually used as context for the Generative AI model to generate an answer from. Artificial Intelligence Model Risk Management |39\n\ntesting modules 82 . Such an approach enables scalability, reduces time and costs \n\nfor implementation, and facilitates the development of more robust and stable \n\nGenerative AI. \n\n7.1.5. To manage the potential impact of Generative AI risks, such as hallucinations, \n\nmost banks have started with a more limited scope of use, focusing on the use of \n\nGenerative AI for assisting or augmenting humans, or improving inte rnal \n\noperational efficiencies, rather than deploying Generative AI in direct customer -\n\nfacing applications without a human -in -the -loop. Banks felt that such an \n\napproach would allow them to learn how to utilise Generative AI effectively and \n\nunderstand its limitations, while managing the potential impact of risks posed by \n\nGenerative AI. \n\n7.1.6. Similarly , to gain greater comfort with the use of Generative AI, most banks have \n\nestablished clear policies and procedures for Generative AI pilots and \n\nexperimentation frameworks. Aside from helping the bank to build capacity and \n\ncapabilities while managing risks a ssociated with Generative AI, such pilots and \n\nexperimentation frameworks are needed to evaluate and test Generative AI in \n\nreal -world scenarios and understand how Generative AI would behave when \n\ndepl oyed . Such pilots are typically bound by time and user lim its 83 .\n\nProcess Controls \n\n7.1.7. To address the cross -cutting nature of Generative AI use cases and risks, as well \n\nas the fast -evolving landscape, some banks have instituted cross -functional risk \n\ncontrol checks at key stages of the Generative AI lifecycle. \n\n7.1.8. As most Generative AI use cases usually fall within a few task archetypes, e.g., \n\nsummarisation, information extraction, conversational agents, question \n\nanswering, one bank established detailed development and validation guidelines                       \n\n> 82 An example of such a module could be a separately trained AI model that estimates the probability of an answer generated\n> by an LLM being a hallucination.\n> 83 Aside from setting time and user limits, other requirements that may apply to such pilots or experiments include setting\n> clear criteria for success at the end of the pilot ,conditions on the terms of use for owners and end -users , and close monitoring\n> of usage patterns and outputs for anomalies and to ensure compliance with the limited scope of usage. Artificial Intelligence Model Risk Management |40\n\nspecific to different Generative AI task archetypes to support development and \n\nvalidation processes. \n\n7.1.9. Due to the uncertainties associated with Generative AI, banks continue to require \n\nhuman oversight or have a human -in -the -loop when using Generative AI to aid in \n\ndecision -making . Extensive user education and training on the limitations of \n\nGenerative AI tools was another key area of focus. \n\nTechnical Controls \n\n7.1.10. As most Generative AI models used by banks, whether closed or open -source, \n\noriginate from third parties, selection of the appropriate model continues to be \n\nan important step for most banks. To assess suitability , some banks would \n\ntypically start by conduct ing significant research on the capabilities of these \n\nmodels for their needs , including utilising public benchmarks and the latest \n\nresearch papers to guide decisions. Testing and evaluation of Generative AI \n\nmodels in the context of the bank’s use cases was also an important area of focus. \n\n7.1.11. More advanced banks would undertake a range of assessments, from \n\nstandalone, functional to end -to -end assessments. Standalone assessments \n\ninvolve the evaluation of the Generative AI model itself. This is usually based on \n\npublicly available data or resources , such as evaluation results in research \n\narticles , model leaderboards, or using open -source evaluation datasets .\n\nFunctional assessments involve evaluati ons of Generative AI model performance \n\non tasks and contexts specific to the bank, e.g., evaluating the performance of a \n\nGenerative AI model when used for retrieval of information from the bank’s \n\nrepository . Finally, end -to -end assessments would evaluate the performance of \n\nthe entire Generative AI system, which may involve multiple Generative AI or AI \n\nmodels .\n\n7.1.12. Such banks also paid significant attention to establishing methods for assessing \n\ndifferent aspects of Generative AI model performance such as accuracy, Artificial Intelligence Model Risk Management | 41 \n\nrelevance, and bias 84 , as well as creating reusable modules to facilitate testing \n\nand evaluation .\n\n7.1.13. The more advanced banks also paid significant attention to curating testing \n\ndatasets that were specific to the use cases and tasks that Generative AI models \n\nwere being used for in the bank. Such testing datasets were critical to ensuring \n\nthat Generative AI models and systems were fit -for -purpose in the bank ’s \n\ncontext . For example, if Generative AI was used for summarising complaints from \n\nthe bank’s customers, the performance of Generative AI on general \n\nsummarisation tasks may not be indicative of its performance in the bank’s \n\ncontext as it may not have been trained on such complaints that are not in the \n\npublic domain , and the complaints may also contain information specific to the \n\nbank, e.g., the bank’s services. To ensure the proper evaluation of Generative AI \n\nin the bank’s context , t he bank will need to curate bank -specific testing datasets \n\nfrom the bank’s internal historical data, or use expert human annotators to \n\ngenerate good quality summaries for a set of customer complaints to evaluate \n\nagainst . Such testing datasets are also important for monitoring the ongoing \n\nperformance of Generative AI models, and for evaluating newer Generative AI \n\nmodels as part of the onboarding process. Other key tests that banks adopted \n\ninclude d model vulnerability testing to assess cyber security risks 85 , as well as \n\nstability and sensitivity testing to ensure consistent performance. Human \n\nfeedback also play ed a key role in testing, evaluating and monitoring Generative \n\nAI performance .\n\n7.1.14. Most banks have established input and output guardrails that utilise filters to \n\nmanage risks relating to areas such as toxicity, biasness, or leakage of sensitive \n\ninformation . Such filters may use rules or AI to detect such undesired or \n\ninappropriate information. For example, input  filters may be used to reject \n\nrequests with toxic language , or replace PII information in requests with generic                        \n\n> 84 In this context, a ccuracy refers to whether the generated text aligns with factual informatio n; r elevance refers to how\n> pertinent the generated text is to the specific query ; and b ias refers to scenarios where the generated text may be biased to\n> specific groups of people , e.g., the generated content may favour one gender over another.\n> 85 These were discussed at length in MAS’ information paper on Cyber Risks Associated with Generative Artificial Intelligence\n> and will not be repeated here. See Annex B for link to the paper.\n\nArtificial Intelligence Model Risk Management | 42 \n\nplaceholders. O utput filters may be used to detect biasness or toxic language in \n\nthe outputs of Generative AI and trigger a review by a human or another \n\nGenerative AI model , or redact PII information in the outputs of Generative AI \n\nbefore they are presented to the user . Similarly, some banks also focused efforts \n\non developing guardrails that were reusable. \n\n7.1.15. Banks mitigate d data security risks when using Generative AI by either using \n\nprivate cloud solutions for Generative AI models, or open -source models on -\n\npremise , which  keep sensitive data within controlled environments (either \n\ndedicated cloud resources not shared with other organisations, or on -premise \n\nserve rs) which can reduce the risks of exposure of data to external parties . Legal \n\nagreements with solution providers, data loss prevention tools, as well as limits \n\non the classification of data that could be used in Generat ive AI were also \n\nimportant to mitigate data security risks. \n\n7.1.16. Anoth er common area that banks were exploring to address Generative AI risks \n\nwere grou nding methods  86  such as retrieval augmented generation ( RAG ) 87 \n\nwhere the outputs of Generative AI models are constrained based on internal \n\nknowledge bases , and source citations are provided to allow end -users to check \n\nfor the accuracy of Generative AI outputs.                      \n\n> 86 Grounding methods help to ground or anchor the Generative AI outputs to factual, verifiable information, which can help\n> reduc ehallucinations and improv erobustness.\n> 87 Retrieval -Augmented Generation (RAG) methods typically retrieve relevant information from a pre -defined knowledge base,\n> and provide the retrieved information as context to the Generative AI model for the generation of outputs. For example, to\n> generate an answer to a question, information relevant to the questi on would be first retrieved, and the retrieved information\n> would then be provided as context to an LLM. The LLM would usually be instructed to answer the question based on the\n> retrieved information .Links to t he retrieved information could also be provided as source citations in the answer. There is\n> however still the possibility of hallucinations occurring even with such approaches. Artificial Intelligence Model Risk Management |43\n\n7.2 Third -Party AI \n\nOverview \n\nExisting third -party risk management standards and processes 88  continue to play an \n\nimportant role in banks’ efforts to mitigat e risks associated with third -party AI. As far as \n\npracticable, most banks also extended controls for internally developed AI to third -party \n\nAI. When considering the use of third -party AI, b anks would weigh the potential benefits \n\nagainst the risks of using third -party AI. To address the additional risks arising from third -\n\nparty AI , banks were exploring areas such as :\n\n• conducting c ompensatory testing ;\n\n• enhancing c ontingency planning ;\n\n• updating l egal agreements ; and \n\n• investing in training and other a wareness efforts .\n\n7.2.1  The u se of third -party AI is increasingly common among banks , particularly in the \n\ncontext of Generative AI where most banks utilise Generative AI models that \n\nwere pre -trained by an external party. However, the use of such third -party AI \n\nand Generative AI presents additional risks , such as unknown biases from pre -\n\ntraining data, data protection concerns , as well as concentration risks due to \n\nincreased interdependencies , e.g., from multiple FIs or even third -party providers \n\nrelying on common underlying Generative AI model s. The lack of transparency is \n\noften cited as a key challenge in managing such third -party risks. Third -party AI \n\nproviders may be reluctant to disclose proprietary information about their \n\ntraining data or algorithms, hindering banks’ efforts in risk assessment and \n\nongoing monitoring. \n\n7.2.2  To mitigate these additional risks, banks were exploring various approaches , such \n\nas:                  \n\n> 88 This includes processes required to comply with MAS’ Notice and Guidelines on Outsourcing (refer to\n> https://www.mas.gov.sg/regulation/third -party -risk -management ).Artificial Intelligence Model Risk Management |44\n\na.  Compensatory testing - conduct ing rigorous testing of third -party AI models \n\nusing various datasets and scenarios to verify the model’s robustness and \n\nstability in the bank’s context , and to detect potential biases. \n\nb.  Contingency planning - develop ing robust contingency plan s to address \n\npotential failures, unexpected behaviour of third -party AI, or discontinuing of \n\nsupport by vendors. This can include having backup systems or manual \n\nprocesses in place to ensure business continuity. \n\nc.  Legal agreements - updat ing contracts with third -party AI providers to include \n\nclauses such as those pertaining to performance guarantee s, data protection, \n\nthe right to audit, and notification when AI is introduced (or not incorporating \n\nAI without the bank’s agreement) in existing third -party providers ’ solutions. \n\nSuch clauses could facilitate clear er expectations and responsibilities. \n\nd.  Awareness efforts – invest ing in training of staff on AI literacy and risk \n\nawareness to improve understanding and mitigation of risks; conduct ing \n\nsurveys with third -party providers to gather more information about whether \n\nAI is being used in their products or services, and third -party providers’ \n\npractices , including their AI development and risk management processes. \n\n# 8 Conclusion \n\n8.1.  Robust oversight and governance of AI, supported by comprehensive \n\nidentification , inventorisation of AI and appropriate risk materiality assessment, \n\nas well as rigorous development, validation and deployment standards and \n\nprocesses are important areas that FIs need to focus on when using AI. As the AI \n\nlandscape continues to evolve , AI MRM framework s will need to be regularly \n\nreviewed and updated , and risk management efforts scale d up based on the state \n\nof AI use. Aside from AI MRM, controls in non AI -speci fic areas such as general \n\ndata governance and management, technology, cyber and third party risk \n\nmanagement, and legal and compliance will also need to be reviewed to take AI \n\ndevelopments into account. Artificial Intelligence Model Risk Management | 45 \n\n8.2.  As the AI landscape continues to evolve, MAS will continue to work with the \n\nindustry to help facilitate and uplift AI and Gen erative AI governance and risk \n\nmanagement efforts across the financial industry, through information sharing \n\nefforts such as this paper to promulgat e industry best practices, and industry \n\ncollaborations such as Project Mind Forge. MAS is also considering supervisory \n\nguidance for all FIs next year, building upon the focus areas covered in this \n\ninformation paper. Artificial Intelligence Model Risk Management | 46 \n\n# Annex A - Definitions \n\n• Model – A model  is a method, system or approach which converts assumptions and \n\ninput data into quantitative estimates, decisions, or decision recommendations \n\n(based on the Global Associate of Ris k Professionals ’ definition of a model ). Apart \n\nfrom AI models , which typically refer to machine or deep learning models which we \n\ndefine below , banks also routinely utilise conventional models , such as economic, \n\nfinancial, or statistical models. Some models , such as logistic regression model s, are \n\ncommonly used in both statistical and AI fields and may be regarded as both AI and \n\nconventional model s. \n\n> •\n\nArtificial Intelligence (AI) – An  AI system is a machine -based system that, for explicit \n\nor implicit objectives, infers, from the input it receives, how to generate outputs such \n\nas predictions, content, recommendations, or decisions that can influence physical \n\nor virtual environments. Different AI systems vary in their levels of autonomy and \n\nadaptiveness after deployment  (based on the Organisation for Economic Co -\n\noperation and Development ’s definition of AI ). Such a definition would include \n\nGenerative AI. An  AI  or Generative AI  system  can be based on one or multiple AI or \n\nGenerative AI models and may also involve other machine -based components. \n\n• AI Use Case – An AI or Generative AI use case usually refers to a  specific real -world \n\ncontext that the AI or Generative AI model or system is applied to. For example, an \n\nAI recommendation model or system that is applied to a financial product \n\nrecommendation use case. \n\n• Machine learning – Machine learning is a subset of AI where the AI directly learn s\n\nfrom data. The machine learning model learns model parameters (or model weights) \n\nto transform inputs into estimates or outputs from the data by updating these \n\nparameters iteratively based on an objective. For example, the machine learning \n\nmodel may be provided with histori cal data that consists of the information on \n\ncustomer s, e.g., income and existing value of debt (which we refer to as input data) ,\n\nand whether the customer had defaulted on a loan obligation (which we refer to as \n\nthe target variable or label) . The machine learning model can then be trained by \n\nlearning model parameters that allow it to transform input data to target variables \n\nor labels with maximum accuracy (or minimum error). Artificial Intelligence Model Risk Management | 47 \n\n• Deep learning – Deep learning is a subset of machine learning, usually based on \n\nneural networks (that were inspired by how neurons in the brain recognise complex \n\npatterns in data) that comprise multiple layers of neurons. Deep learning models are \n\nable to learn more comple x patterns due to the many layers of neurons in the model. \n\n• Discriminative versus Generative AI models – AI models that generate predictions, \n\ne.g., predicting a credit default based on customer information, or recommending a \n\nfinancial product based on customer information, are usually referred to as \n\ndiscriminative AI models. This is in contrast to Generative AI models that are usually \n\nused to generat e content such as text, images, audio or videos .\n\n• CI/CD , DevOps, MLOps, AIOps, LLMOps – Continuous integration/continuous \n\ndeployment (CI/CD) or DevOps pipelines automate the process of building, testing, \n\nand deploying code changes. These terms are closely related to the term MLOps, \n\nwhich is used to describe tools and systems that help to automate the process of \n\nbuilding, testing, deploying and monitoring the performance of machine learning \n\nsystems. More recent terms such as AIOps and LLMOps have also been used to \n\ndescribe such tools and systems for AI in g eneral or for Large Language Models \n\n(LLM). \n\n• Data Drift - This occurs when the statistical properties of the distribution of the data \n\nchanges. For example, the underlying distribution of customer data may have drifted \n\nor changed over time due to changes in the lifestyles of customers . Hence, an AI \n\nmodel that was trained on data from a more distant time period may not perform \n\nas well on data from a more recent time period due to data drift . A common measure \n\nof how much a population distribution has changed over time is the Population \n\nStability Index (PSI). \n\n• Concept Drift - This occurs when the underlying relationships between the features \n\nin input data and what the AI model is being used to predict or generate changes .\n\nFor example, customer preferences for financial products may have shifted due to \n\nbroad industry changes ( e.g., a shift in the relationships between customer \n\ninformation and their preferences for financial products ), and an AI model used to \n\ngenerate financial product recommendations may no longer perform as well due to Artificial Intelligence Model Risk Management | 48 \n\nsuch  concept drift s. A common measure of concept drift is the Characteristic \n\nStability Index (CSI). \n\n• Model Drift - Model drift is a broader term that usually encompasses both data drift \n\nand concept drift, as well as other factors that can cause a model's performance to \n\ndegrade over time. Aside from measures such as PSI and CSI, monitoring the \n\nstatistical characteristics of AI predictions can also be used to detect drifts in general. \n\n• Supervised learning – Supervised learning is a machine learning approach where a\n\nmodel is trained on a labelled dataset. In this process , each data point  includes input \n\nfeatures paired with the corresponding output (label).  The model learns to map \n\ninputs to outputs  by comparing its predictions with the actual labels and updating \n\nthe model parameters iteratively . Classification , which involves the prediction of \n\nclasses or categories, and regression , which involves the prediction of continuous \n\nvalues, are common examples of supervised learning. \n\n• Unsupervised learning – Unsupervised learning is a machine learning approach \n\nwhere a model discovers patterns in data without the use of label s. An e xample of \n\nunsupervised learning is clustering, where data points are grouped together based \n\non their inherent similarities or dissimilarities .Artificial Intelligence Model Risk Management | 49 \n\n# Annex B - Useful References \n\nPublications for the Financial Sector issued by MAS \n\n• MAS FEAT Principles:  https://www.mas.gov.sg/publications/monographs -or -information -\n\npaper/2018/feat \n\n• Veritas Initiative : https://www.mas.gov.sg/schemes -and -initiatives/veritas \n\n• Project MindForge: https://www.mas.gov.sg/schemes -and -initiatives/project -mindforge \n\n• Information Paper on Implementation of Fairness Principles in Financial Institutions’ use of \n\nArtificial Intelligence/Machine Learning:  https://www.mas.gov.sg/publications/monographs -or -\n\ninformation -paper/2022/implementation -of -fairness -principles -in -financial -institutions -use -of -\n\nartificial -intelligence -and -machine -learning \n\n• Information Paper on Cyber Risks Associated with Generative Artificial Intelligence: \n\nhttps://www.mas.gov.sg/regulation/circulars/cyber -risks -associated -with -generative -artificial -\n\nintelligence \n\n• Information Paper on Data Governance and Management Practices: \n\nhttps://www.mas.gov.sg/publications/monographs -or -information -paper/2024/data -\n\ngovernance -and -management -practices \n\n• Technology Risk Management Guidelines :\n\nhttps://www.mas.gov.sg/regulation/guidelines/technology -risk -management -guidelines \n\n• Business Continuity Management Guidelines :\n\nhttps://www.mas.gov.sg/regulation/guidelines/guidelines -on -business -continuity -management \n\n• Notice and Guidelines on Third -Party Risk Management :\n\nhttps://www.mas.gov.sg/regulation/third -party -risk -management \n\n• Information Paper on Operational Risk Management - Management of Third Party Arrangements: \n\nhttps://www.mas.gov.sg/publications/monographs -or -information -paper/2022/operational -risk -\n\nmanagement ---management -of -third -party -arrangements Artificial Intelligence Model Risk Management | 50 \n\nNon -Financial Sector Specific Publications \n\n• AI Verify : AI governance testing framework and software toolkit: \n\nhttps://www.aiverifyfoundation.sg/what -is -ai -verify/ \n\n• Project Moonshot:  https://www.aiverifyfoundation.sg/project -moonshot/ \n\n• Model Governance Framework for Generative AI :\n\nhttps://www.aiverifyfoundation.sg/resources/mgf -gen -ai/ \n\n• Trusted Data Sharing Framework:  https://www.imda.gov.sg/how -we -can -help/data -\n\ninnovation/trusted -data -sharing -framework \n\n• Personal Data Protection Act (PDPA):  https://www.pdpc.gov.sg/overview -of -pdpa/the -\n\nlegislation/personal -data -protection -act \n\n• Advisory Guidelines on use of Personal Data in AI Recommendation and Decision Systems :\n\nhttps://www.pdpc.gov.sg/guidelines -and -consultation/2024/02/advisory -guidelines -on -use -of -\n\npersonal -data -in -ai -recommendation -and -decision -systems \n\n• Guidelines and Companion Guide on Securing AI Systems : https://www.csa.gov.sg/Tips -\n\nResource/publications/2024/guidelines -on -securing -ai",
  "fetched_at_utc": "2026-02-08T19:07:02Z",
  "sha256": "aa4ced6d9f7b41ac579e07dea2f87952b5eb3158f3bd0de54ba0162322b0fc36",
  "meta": {
    "file_name": "AI Risk Management - Singapore.pdf",
    "file_size": 765210,
    "relative_path": "pdfs\\AI Risk Management - Singapore.pdf",
    "jina_status": 20000,
    "jina_code": 200,
    "usage": {
      "tokens": 22410
    }
  }
}