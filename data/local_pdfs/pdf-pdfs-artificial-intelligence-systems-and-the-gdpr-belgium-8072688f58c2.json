{
  "doc_id": "pdf-pdfs-artificial-intelligence-systems-and-the-gdpr-belgium-8072688f58c2",
  "source_type": "local_pdf",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Artificial Intelligence Systems and the GDPR - Belgium.pdf",
  "title": "Artificial Intelligence Systems and the GDPR - Belgium",
  "text": "(Original version – version December 2024) \n\n> 1\n> Artificial Intelligence Systems and the GDPR\n> A Data Protection Perspective\n\n# Data Protection Authority of \n\n# Belgium \n\n# General Secretariat \n\n# Artificial Intelligence Systems and the GDPR \n\n# A Data Protection Perspective (Original version – version December 2024) \n\n2\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nEXECUTIVE SUMMARY  ................................ ................................ ................................ .................... 3\n\nOBJECTIVE OF THIS INFORMATION BROCHURE  ................................ ................................ .... 4\n\nAUDIENCE FOR THIS INFORMATION BROCHURE  ................................ ................................ ... 5\n\nWHAT IS AN AI SYSTEM ?  ................................ ................................ ................................ ............... 6\n\nGDPR & AI ACT REQUIREMENTS  ................................ ................................ ................................ .. 8\n\nLAWFUL , FAIR , AND TRANSPARENT PROCESSING ................................ ................................ ................................ ........ 8\n\nPURPOSE LIMITATION AND DATA MINIMISATION  ................................ ................................ ................................ ......... 9\n\nDATA ACCURACY AND UP -TO -DATENESS  ................................ ................................ ................................ ....................... 9\n\nSTORAGE LIMITATION  ................................ ................................ ................................ ................................ ............................ 9\n\nAUTOMATED DECISION -MAKING  ................................ ................................ ................................ ................................ ...... 10 \n\nSECURITY OF PROCESSING  ................................ ................................ ................................ ................................ ................. 11 \n\nDATA SUBJECT RIGHTS  ................................ ................................ ................................ ................................ ....................... 13 \n\nACCOUNTABILITY  ................................ ................................ ................................ ................................ ................................ .. 14 \n\nMAKING COMPLIANCE STRAIGHTFORWARD: USER STORIES FOR AI SYSTEMS IN \n\nLIGHT OF GDPR AND AI ACT REQUIREMENTS  ................................ ................................ ...... 16 \n\nREQUIREMENTS OF LAWFUL , FAIR , AND TRANSPARENT PROCESSING  ................................ ............................... 16 \n\nREQUIREMENTS OF PURPOSE LIMITATION AND DATA MINIMIZATION  ................................ ................................ . 17 \n\nREQUIREMENTS OF DATA ACCURACY AND UP -TO -DATENESS  ................................ ................................ .............. 18 \n\nREQUIREMENT OF SECURE PROCESSING  ................................ ................................ ................................ ....................... 19 \n\nREQUIREMENT OF (THE ABILITY OF DEMONSTRATING ) ACCOUNTABILITY  ................................ ....................... 20 \n\nREFERENCES ................................ ................................ ................................ ................................ ..... 21 (Original version – version December 2024) \n\n3\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Executive summary \n\nThis information brochure outlines the complex interplay between the General Data \n\nProtection Regulation (GDPR) i and the Artificial Intelligence (AI) Act ii in the context of AI \n\nsystem development. The document emphasizes the importance of aligning AI systems \n\nprocessing personal data with data protection principles while addressing the unique \n\nchallenges posed by AI technologies. \n\nKey points include: \n\n• GDPR and AI Act alignment: the brochure highlights the complementary nature of \n\nthe GDPR and AI Act in ensuring lawful, fair, and transparent processing of personal \n\ndata in AI systems. \n\n• AI system definition: the document provides a clear definition of AI systems and \n\noffers illustrative examples to clarify the concept. \n\n• data protection principles: the brochure delves into core GDPR principles such as \n\nlawfulness, fairness, transparency, purpose limitation, data minimization, accuracy, \n\nstorage limitation, and data subject rights in the context of AI systems. \n\n• accountability: the importance of accountability is emphasized, with specific \n\nrequirements outlined for both the GDPR and AI Act. \n\n• security: the document highlights the need for robust technical and organizational \n\nmeasures, to protect personal data processed by AI systems. \n\n• human oversight: The crucial role of human oversight in AI system development \n\nand operation is emphasized, particularly for high -risk AI systems. \n\nBy providing insights into the legal framework and practical guidance, this information \n\nbrochure aims to empower legal professionals, data protection officers, technical \n\nstakeholders, including controllers and processors, to understand and comply with the \n\nGDPR and AI Act requirements when developing and deploying AI systems. (Original version – version December 2024) \n\n4\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Objective of this information brochure \n\nThe General Secretariat of the Belgian Data Protection Authority monitors social, \n\neconomic, and technological developments that impact the protection of personal data iii .\n\nIn recent years, AI technologies have experienced exponential growth, revolutionizing \n\nvarious industries and significantly impacting the way data is collected, processed, and \n\nutilized. However, this rapid advancement has brought about complex challenges \n\nre garding data privacy, transparency, and accountability. \n\nIn this context, the General Secretariat of the Belgian Data Protection Authority publishes \n\nthis information brochure to provide insights on data protection and the development and \n\nimplementation of AI systems. \n\nUnderstanding and adhering to the GDPR principles and provisions is crucial for ensuring \n\nthat AI systems operate ethically, responsibly, and in compliance with legal standards. This \n\ninformation brochure aims to elucidate the GDPR requirements specifically applicable to \n\nAI systems that process personal data , offering more clarity and useful insights to \n\nstakeholders involved in the development, implementation, and (internal) regulation of AI \n\ntechnologies. \n\nIn addition to the GDPR, the Artificial Intelligence Act (AI Act), which entered into force on \n\n1st of August 2024 , will also significantly impact the regulation of AI system development \n\nand use. This information brochure will also address the requirements of the AI Act. \n\nThe examples included in this brochure serve a purely pedagogical purpose, they are \n\nsometimes hypothetical and do not take into account certain exceptions iv and \n\nimperfections v in the regulation. (Original version – version December 2024) \n\n5\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Audience for this information brochure \n\nThis information brochure is intended for a diverse audience comprising legal \n\nprofessionals, Data Protection Officers (DPOs), and individuals with technical backgrounds \n\nincluding business analysts, architects, and developers. It also targets controllers and \n\nprocessors involved in the development and deployment of AI systems. Given the \n\nintersection of legal and technical considerations inherent in the application of the G DPR \n\nto AI systems, this information brochure seeks to bridge the gap between legal \n\nrequirements and technical implementation. \n\nLegal professionals and DPOs play a crucial role in ensuring organizational compliance with \n\nGDPR obligations, specifically those relevant to AI systems that process personal data. By \n\nproviding insights into GDPR requirements specific to AI, this information brochure equips \n\nlegal professionals and DPOs with useful knowledge to navigate the complexities of AI -\n\nrelated data processing activities, assess risks, and implement appropriate measures. \n\nAt the same time, individuals with technical backgrounds such as business analysts, \n\narchitects, and developers are integral to the design, development, and deployment of AI \n\nsystems. Recognizing their pivotal role, this information brochure aims to elucidate GDPR \n\nrequirements in a manner accessible to technical stakeholders. \n\nConcrete examples are incorporated into the text to illustrate how GDPR principles \n\ntranslate into practical considerations during the lifecycle of AI projects. By offering \n\nrelatively simple and actionable insights, this information brochure empowers \n\nprofessionals with various backgrounds to design AI systems that are compliant with \n\nGDPR obligations , embed data protection -by -design principles, and mitigate potential legal \n\nand ethical risks. (Original version – version December 2024) \n\n6\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# What is an AI system ? \n\nThe term \"AI system\" encompasses a wide range of interpretations. \n\nThis information brochure will not delve into the intricacies and nuances that distinguish \n\nthese various definitions. \n\nInstead, we will begin by examining the definition of an AI system as outlined in the AI Act vi :\n\nFor the purposes of this Regulation, the following definitions apply: \n\n(1) ‘AI system’ means a machine -based system that is designed to operate with varying \n\nlevels of autonomy and that may exhibit adaptiveness after deployment, and that, for \n\nexplicit or implicit objectives, infers, from the input it receives, how to generate outputs \n\nsuch as predictions, content, recommendations, or decisions that can influence physical \n\nor virtual environments; \n\nIn other terms : \n\nAn AI system is a computer system specifically designed to analyze data, identify patterns, \n\nand use that knowledge to make informed decisions or predictions. \n\nIn some cases, AI systems can learn from data and adapt over time. This learning capability \n\nallows them to improve their performance, identify complex patterns across different data \n\nsets, and make more accurate or nuanced decisions. \n\nExamples of AI systems in everyday life: \n\nSpam filters in email : spam filters analyze incoming emails and identify patterns that \n\ndistinguish spam messages from legitimate emails. Over time, as people mark emails as \n\nspam or not spam, the AI system can learn and improve its filtering accuracy. This is an \n\nexample of an AI system that meets the criteria of an AI system :\n\n• machine -based system: it's a computer program. \n\n• analyzes data: it analyzes the content of emails. \n\n• identifies patterns: it identifies patterns in emails that suggest spam. \n\n• makes decisions: it decides whether to categorize an email as spam or not. \n\nRecommendation systems on streaming services : movie streaming services utilize AI \n\nsystems to generate recommendations for users. These systems analyze a user's past \n\nviewing habits, along with the habits of similar users, to recommend content they might be \n\ninterested in. This is another example of an AI system : (Original version – version December 2024) \n\n7\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n• machine -based system: it's a computer program. \n\n• analyzes data: it analyzes a user's viewing/listening history. \n\n• identifies patterns: it identifies patterns in user preferences and those of similar \n\nusers. \n\n• makes recommendations: it recommends content based on the identified patterns. \n\nVirtual assistants : virtual assistants respond to voice commands and complete tasks like \n\nsetting alarms, playing music, or controlling smart home devices. These systems use \n\nspeech recognition and natural language processing to understand user requests and take \n\naction. This is again an example of an AI system :\n\n• machine -based system: it's a computer program. \n\n• analyzes data: it analyzes user voice commands. \n\n• identifies patterns: it identifies patterns in speech to understand user requests. \n\n• makes decisions: it decides how to respond to commands based on its \n\nunderstanding. \n\n• may exhibit adaptiveness: some virtual assistants can learn user preferences and \n\nadapt their responses over time. \n\nAI -powered medical imaging analysis : many hospitals and healthcare providers are utilizing \n\nAI systems to assist doctors in analyzing medical images, such as X -rays, CT scans, and \n\nMRIs. These systems are trained on vast datasets of labeled medical images, allowing them \n\nto identify patterns and potential abnormalities. \n\n• machine -based system: it's a computer program. \n\n• analyzes data: it analyzes the digital medical images. \n\n• identifies patterns: it identifies patterns in the images that might indicate the \n\npresence of a disease or abnormality. \n\n• supports decision -making: the system highlights potential areas of concern in the \n\nimages, which can help doctors make more informed diagnoses. (Original version – version December 2024) \n\n8\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# GDPR & AI Act requirements \n\n## Lawful, fair, and transparent processing \n\nThe GDPR requires lawfulness, fairness and transparency. \n\nLeveraging GDPR lawfulness of processing : The GDPR establishes six legal bases for \n\nprocessing personal data in Article 6 (consent, contract, legal obligation, vital interests, \n\npublic interest , and legitimate interest svii ). These same legal bases remain applicable for AI \n\nsystems that process personal data under the AI Act. \n\nProhibited AI Systems : t he AI Act introduces additional prohibitions beyond the GDPR for \n\ncertain high -risk AI systems. While the GDPR focuses on protecting personal data through \n\nvarious principles, the AI Act directly prohibits specific types of high -risk AI applications. \n\nHere ar e some examples: \n\n• Social scoring systems: these systems assign a score to individuals based on \n\nvarious factors, potentially leading to discrimination and limitations on \n\nopportunities. \n\n• AI systems for real time remote biometric identification for the purpose of law \n\nenforcement in public places (with limited exceptions) : these systems raise \n\nconcerns about privacy, freedom of movement, and potential misuse for mass \n\nsurveillance. \n\nFairness: \n\n• While the AI Act doesn't have a dedicated section titled “fairness”, it builds upon \n\nthe GDPR's principle of fair processing (art. 5.1.a) as the AI Act focuses on \n\nmitigating bias and discrimination in the development, deployment, and use of AI \n\nsystems. \n\nTransparency: \n\n• the AI Act requires a baseline level of transparency for certain AI systems. This \n\nmeans users should be informed that they're interacting with an AI system. For \n\ninstance, a chatbot could begin an interaction with a message like \"Hello, I am \n\nNelson, a chatbot . How can I assist you today?\" \n\n• the AI Act requires a higher transparency level for high -risk AI systems. This \n\nincludes providing clear and accessible information about how data is used in these \n\nsystems, particularly regarding the decision -making process. Users should \n\nunderstand the factors influencing AI -based decisions and how potential bias is \n\nmitigated. (Original version – version December 2024) \n\n9\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Purpose limitation and data minimi sation \n\nThe GDPR requires purpose limitation (art. 5.1.b) and data minimi sation (art. 5.1.c) . This \n\nmeans personal data must be collected for specific and legitimate purposes, and limited to \n\nwhat is necessary for those purposes. Th ese principles ensure that AI systems don't use \n\npersonal data for purposes beyond their intended function or collect excessive data .\n\nThe AI Act strengthens the principle of purpose limitation – from the GDPR – for high -risk \n\nAI systems by emphasizing the need for a well -defined and documented intended purpose. \n\nHypothetical e xample : A loan approval AI system of a financial institution , in addition to \n\nstandard identification data and credit bureau information, also utilizes geolocation data \n\n(e.g., past locations visited) and social media data (e.g., friends' profiles and interests) of a \n\ndata subject . This extensive data collection, including geolocation and social media data, \n\nraises concerns about the system's compliance with the GDPR. \n\n## Data accuracy and up-to -dateness \n\nThe GDPR requires personal data to be accurate and, where necessary, kept up -to -date \n\n(art. 5.1.d) . Organizations must take reasonable steps to ensure this. The AI Act builds upon \n\nthis principle by requiring high -risk AI systems to use high -quality and unbiased data to \n\nprevent discriminatory outcomes. \n\nHypothetical example : a financial institution develops a n AI system to automate loan \n\napprovals. The system analyzes various data points about loan applicants, including credit \n\nhistory, income, and demographics (postal code). However, t he training data for the AI \n\nsystem unknowingly reflects historical biases : the data stems from a period when loans \n\nwere more readily granted in wealthier neighborhoods ( with a higher average income) . The \n\nAI system perpetuates these biases as l oan applicants from lower -income neighborhoods \n\nmight be systematically denied loans, even if they are financially qualified. This results in a \n\ndiscriminatory outcome , and might raise serious concerns about the system's compliance \n\nwith the AI Act. \n\n## Storage limitation \n\nThe GDPR requires personal data to be stored only for as long as necessary to achieve the \n\npurposes for which it was collected (art. 5.1.e) . The AI Act doesn't explicitly introduce \n\nan other or an extra requirement on storage limitation for high -risk AI systems. (Original version – version December 2024) \n\n10 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Automated decision -making \n\nThe GDPR and the AI Act both address the importance of human involvement in automated \n\ndecision -making processes that impact individuals. However, they differ in their focus: \n\n• The GDPR grants individuals the right not to be subject solely to automated \n\nprocessing for decisions that produce legal effects concerning them ( art . 22). This \n\nmeans data subjects have the right to request a reconsideration of an automated \n\ndecision by a human decision -maker. This functions as an individual right to \n\nchallenge decisions perceived as unfair or inaccurate. \n\n• The AI Act strengthens the focus on human involvement by requiring meaningful \n\nhuman oversight throughout the development, deployment, and use of high -risk AI \n\nsystems. This acts as a governance measure to ensure responsible AI development \n\nand use. Human ove rsight under the AI Act encompasses a broader range of \n\nactivities than just reconsideration of individual decisions. It includes, for example, \n\nreviewing the AI system's training data and algorithms for potential biases, \n\nmonitoring the system's performance, and intervening in critical decision -making \n\npathways. \n\nIn essence, the GDPR empowers individuals to object to solely automated decisions, while \n\nthe AI Act requires proactive human oversight for high -risk AI systems to safeguard \n\nagainst potential biases and ensure responsible development and use of such systems. \n\nHypothetical example : a government agency uses an AI system to assess eligibility for \n\nsocial welfare benefits based on income, employment status, and family situation .\n\nFollowing the GDPR, i ndividuals have the right not to be subject solely to automated \n\nprocessing for social welfare benefits eligibility ( art. 22). This means they can request a\n\nreconsideration of an automated decision by a human decision -maker .\n\nFollowing the AI Act, this AI system is classified as an high -risk system (as it has a \n\nsignificant impact on individuals' livelihoods). This requires the government agency to \n\nimplement human oversight throughout the development, deployment, and use of the AI \n\nsystem. (Original version – version December 2024) \n\n11 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Security of Processing \n\nBoth the G DPR and the AI Act emphasize the importance of securing personal data \n\nthroughout its processing lifecycle. However, AI systems introduce specific risks that \n\nrequire additional security measures beyond traditional data protection practices. \n\nThe GDPR requires organizations to implement technical and organizational measures \n\n(TOMs) that are appropriate to the risk associated with their data processing activities. This \n\ninvolves conducting risk assessments to identify potential threats and vulnera bilities. The \n\nselected TOMs should mitigate these risks and ensure a baseline level of security for \n\npersonal data. \n\nThe AI Act builds upon this foundation by mandating robust security measures for high -\n\nrisk AI systems. This is because AI systems introduce specific risks that go beyond \n\ntraditional data processing, such as: \n\n• potential bias in training data: biased training data can lead to biased decisions by \n\nthe AI system, impacting individuals unfairly. \n\n• manipulation by unauthorized individuals: for example, a hacker could potentially \n\nmanipulate the AI system's training data to influence its decisions in a harmful way. \n\nImagine a system trained to approve loan applications being tricked into rejecting \n\nqualified applicants based on irrelevant factors. \n\nTo address these unique risks, the AI Act emphasizes proactive measures such as: \n\n• identifying and planning for potential problems: This involves brainstorming what \n\ncould go wrong with the AI system and how likely it is to happen (risk assessment). \n\nThis is a core practice under both the GDPR and AI Act. \n\n• continuous monitoring and testing: This involves regularly evaluating the AI \n\nsystem's performance for several aspects including: \n\no security flaws: identifying vulnerabilities in the system's code or design that \n\ncould be exploited by attackers. \n\no bias: checking for potential biases in the system's training data or decision -\n\nmaking processes. \n\n• human oversight: the AI Act emphasizes the importance of meaningful human \n\noversight throughout the development, deployment, and use of high -risk AI \n\nsystems. This ensures that humans are involved in critical decisions and (Original version – version December 2024) \n\n12 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nunderstand the system's vulnerabilities. Human oversight under the AI Act goes \n\nbeyond just security processes and encompasses various aspects, such as: \n\no reviewing training data and algorithms for potential biases. \n\no monitoring the system's performance for fairness, accuracy, and potential \n\nunintended behaviour .\n\no intervening in critical decision -making pathways, especially when they \n\ncould significantly impact individuals. \n\nExample : AI -powered Lung Cancer Diagnosis System .\n\nAn AI system used by a hospital to diagnose lung cancer exemplifies a high -risk AI system \n\ndue to several factors: \n\n• highly sensitive data: it processes highly sensitive personal data, including patients' \n\nhealth information (lungs) and diagnoses (special category data under article 9 of \n\nthe GDPR) ;\n\n• data breach impact: a data breach could expose critical health information about \n\npatients, potentially leading to privacy violations and reputational harm for the \n\nhospital ;\n\n• life -altering decisions: the system's output directly impacts patients' lives. A \n\ndiagnosis based on inaccurate or compromised data could have serious \n\nconsequences for their health and well -being. \n\nBoth the GDPR and the AI Act emphasize the importance of security measures for data \n\nprocessing activities, especially those involving sensitive data. \n\n• the GDPR establishes a foundation for data security: It requires organizations to \n\nimplement appropriate technical and organizational measures (TOMs) to protect \n\npersonal data based on a risk assessment. For health data, these measures would \n\nbe particularly strong due to its sensitive nature. Examples under the GDPR could \n\ninclude: \n\no data encryption: encrypting patient data at rest and in transit ensures its \n\nconfidentiality even if a breach occurs ;\n\no access controls: implementing strict access controls limits who can access \n\nand modify patient data ;\n\no penetration testing: regularly conducting penetration tests helps identify \n\nand address vulnerabilities in the system's security posture ;\n\no logging and auditing: maintaining detailed logs of system activity allows for \n\nmonitoring and investigation of any suspicious behavior. \n\n• The AI Act builds upon this foundation for high -risk AI systems: recognizing the \n\nspecific risks of AI, the AI Act mandates robust security measures. These might (Original version – version December 2024) \n\n13 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\ninclude additional measures tailored to the  specific vulnerabilities of the AI system, \n\nsuch as data validation and quality assurance : t he AI Act emphasizes the \n\nimportance of ensuring the quality and integrity of the data used to train and \n\noperate the AI system. This could involve techniques for: \n\no data provenance: tracking the origin of data to identify potential sources of \n\nbias or manipulation in the training data, such as incorrect X -ray labeling. \n\no anomaly detection: identifying and flagging unusual patterns in the training \n\ndata that might indicate malicious tampering, such as a sudden influx of X -\n\nrays with unrealistic characteristics. \n\no human review of high -risk data points: Having healthcare professionals \n\nreview critical X -rays before they are used to train the AI system, especially \n\nthose that show unusual features or could significantly impact patient \n\noutcomes. \n\nBy implementing these security measures the hospital can mitigate the risks associated \n\nwith the AI -powered lung cancer diagnosis system and ensure patient privacy, data \n\nsecurity, and ultimately, the best possible patient outcomes. \n\n## Data Subject Rights \n\nThe GDPR grants natural persons data subject rights, empowering them to control their \n\npersonal data and how it's used. These rights include access (seeing what data is \n\nprocessed, art. 15 ), rectification (correcting inaccurate data and completing data , art. 16 ), \n\nerasure (requesting data deletion , art. 17 ), restriction of processing (limiting how data is \n\nused , art. 18 ), and data portability (transferring data to another service , art. 20 ). \n\nTo effectively exercise these rights, natural persons need to understand how their data is \n\nbeing processed . The AI Act reinforces this by emphasizing the importance of clear \n\nexplanations about how data is used in certain AI systems. With this transparency, \n\nindividuals can make informed decisions about their data and utilize their data subject \n\nrights more effectively. \n\nExample : an AI system used to determine premiums for life insurance assigns a relatively \n\nhigh premium to a particular customer (data subject). The AI Act entitles this customer to \n\na clear explanation of how their premium is calculated. For example, the insurer (data \n\ncontroller) could explain that various data points were u sed, such as medical problems \n\ncustomers have faced in the past. This information, in turn, allows the customer to exercise \n\ntheir data subject rights under the GDPR , such as the right to rectification (correction of \n\ninaccurate personal data or completion of personal data ). (Original version – version December 2024) \n\n14 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Accountability \n\nThe GDPR requires (organizations to demonstrate ) accountability for personal data \n\nprocessing through several measures , such as : \n\n• Transparent processing: individuals must understand how their data is collected, \n\nused, stored and shared (f.e. by a clear and concise data protection statement , by \n\ndata subject access rights , …) . This transparency allows them to see if their data is \n\nbeing handled lawfully and fairly ;\n\n• Policies and procedures for handling personal data: documented policies ensure \n\nconsistent data handling practices across the organization ;\n\n• Documented legal basis for processing : for each data processing activity, \n\norganizations need documented proof of the lawful justification (consent, contract, \n\nlegitimate interest, etc.) ;\n\n• Keeping different records (like the Register Of Processing Activities ( ROPA ), data \n\nsubject requests, data breaches) is required: maintaining accurate records \n\ndemonstrates a commitment to accountability and allows organizations to prove \n\ncompliance during audits or investigations ;\n\n• Security measures : implementing and correctly maintaining appropriate technical \n\nand organizational measures (TOMs) to protect personal data is crucial for \n\ndemonstrating accountability ;\n\n• A Data Protection Impact Assessment ( DPIAs ) is required in some cases: these are \n\nmandatory when processing high -risk data or implementing new technologies ;\n\n• A Data Protection Officer ( DPO ) is required in some cases: f.e. governmental \n\norganizations, regardless of their core activities, are required to have a DPO. \n\nWhile the AI Act doesn't have a dedicated section on demonstrating accountability, it \n\nbuilds upon the GDPR's principles. The AI Act requires organizations to implement : \n\n• a two -step risk management approach for AI systems. First, there's an initial \n\nclassification process that categorizes the risk the AI poses to individuals (ranging \n\nfrom minimal to high). \n\nFor high -risk systems, a more in -depth risk assessment is required in some cases .\n\nThis dives deeper into the specific risks and identifies potential harms associated \n\nwith the AI system , and is also called a FRIA (Fundamental Rights Impact \n\nAssessment) ;(Original version – version December 2024) \n\n15 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n• clear documentation of the design and implementation of AI systems ;\n\n• processes dealing with human oversight in high -risk AI systems. This could involve \n\nhuman intervention or approval for critical decisions made by the AI system ;\n\n• a formal incident reporting process for reporting incidents related to AI system \n\nmalfunctions or unintended behaviour .(Original version – version December 2024) \n\n16 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Making compliance straightforward: user \n\n# stories for AI systems in light of GDPR and AI Act \n\n# requirements \n\nTranslating regulatory requirements into technical specifications for AI systems presents \n\nsignificant challenges. This document focuses on using user stories to bridge the gap \n\nbetween legal obligations and system development. \n\nUser stories offer a practical approach to understanding and addressing regulatory \n\nrequirements in the context of AI system design. By adopting a user -centric perspective, \n\norganizations can effectively translate legal obligations into actionable steps. \n\nThis document uses a life insurance premium calculation system as an example to illustrate \n\nthe application of user stories in the AI domain. \n\n## Requirements of lawful, fair, and transparent processing \n\nUser story : ensuring lawfulness – correct legal basis \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremium s, we need to conduct a thorough legal basis assessment to determine the most \n\nappropriate legal justification for collecting and using customer data in our AI system. This \n\nis important to comply with the GDPR principle of lawfulness. \n\nUser story : ensuring lawfulness - prohibited data \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure our system complies with the GDPR and AI Act prohibitions \n\non processing certain types of personal data. This includes special categories of personal \n\ndata such as racial or ethnic origin, political opinions, religious beliefs, health, etc. This is \n\nimportant to comply with the GDPR's protection of sensitive personal data and the AI Act's \n\nemphasis on preventing discriminatory outcomes. \n\nUser story : ensuring fairness \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure fair and non -discriminatory processing of customer data. \n\nThis is important to comply with the GDPR principle of fairness and the specific AI Act's \n\nfocus on preventing biased outcomes that could disadvantage certain groups. (Original version – version December 2024) \n\n17 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nThe life insurance company can achieve fairness by: \n\n• data source review: analyze the data sources used to train the AI system to identify \n\nand mitigate potential biases based on factors like postal code, gender, age , … .\n\nEnsure these factors are used in a way that is relevant and necessary for premium \n\ncalculations , avoiding any discriminatory outcomes. \n\n• fairness testing: regularly test the AI system for potential biases in its outputs. This \n\nmight involve comparing life insurance premium calculations for similar customer \n\nprofiles to identify any unexplainable disparities. \n\n• human oversight: implement a human review process for high -impact decisions \n\nmade by the AI system, such as significant life insurance premium increases or \n\neven policy denials .\n\nUser story : ensuring transparency \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to be transparent about how our customers' data is used. This is \n\nimportant to comply with the general GDPR principle of transparency and the specific AI \n\nAct’s focus on transparency for high -risk AI systems. \n\nThe life insurance company can achieve transparency by : \n\n• a data protection statement : clearly explain in the company's data protection \n\nstatement how customer data is collected, used, and stored in the AI system for \n\npremium calculations .\n\n• easy -to -understand explanations : provide customer -friendly explanations of the \n\nAI premium calculations process. This could involve using simple language, visuals, \n\nor FAQs to demystify the AI's role in determining life insurance premiums. \n\n• right to access information : implement mechanisms for customers to easily access \n\ninformation about the data points used in their specific premium calculations. \n\n## Requirements of purpose limitation and data minimization \n\nUser story : ensuring purpose limitation \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure that the data we collect from our customers is limited to what \n\nis strictly necessary for the accurate premium calculations . This is important to comply with \n\nthe principle of purpose limitation under the GDPR. (Original version – version December 2024) \n\n18 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nUser story : ensuring data minimization \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to implement a data minimization strategy to ensure we only collect \n\nand use the minimum amount of customer data necessary for the accurate premium \n\ncalculations . This is important to comply with the principle of data minimization under the \n\nGDPR. \n\n## Requirements of data accuracy and up -to -dateness \n\nUser story : ensuring data accuracy and up -to -dateness \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to implement processes to ensure the accuracy and up -to -dateness of \n\ncustomer data used in the system. This is important to comply with the principle of data \n\naccuracy under the GDPR. \n\nThe life insurance company can achieve accuracy and up -to -dateness of customer data by: \n\n• data verification mechanisms: offer customers easy -to -use mechanisms to verify \n\nand update their personal data within the life insurance system. This could be \n\nthrough an online portal, mobile app, or dedicated phone line. \n\n• regular data refresh: establish procedures for regularly refreshing customer data \n\nused in the AI system. This might involve requesting customers to update their \n\ninformation periodically or integrating with external data sources to automatically \n\nupdate relevant data points. \n\n• data quality alerts: implement alerts for missing or potentially inaccurate data \n\npoints in customer profiles. This allows the company to proactively reach out to \n\ncustomers and request updates. \n\n• clearly communicate to customers their right to rectification under the GDPR. This \n\nright allows them to request corrections of any inaccurate personal data or \n\ncompletion of missing data used in the premium calculations system. \n\nUser story : ensuring use of unbiased data \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure that the data used to train and operate the system is of free \n\nfrom bias. This is important to comply with the specific AI Act's focus on preventing biased \n\noutcomes that could disadvantage certain groups. (Original version – version December 2024) \n\n19 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nThe life insurance company can achieve u nbiased data for fair AI premium calculations by : \n\n• data source evaluation: Analyze the sources of data used to train the AI system. \n\nIdentify potential biases based on factors like socioeconomic background in the \n\ndata collection process. \n\n• regular monitoring and bias testing: Continuously monitor the AI system's \n\nperformance for potential biases in its outputs. Conduct regular bias testing to \n\nidentify and address any discriminatory outcomes in premium calculations .\n\n• human oversight: implement a human review process for high -impact decisions \n\nmade by the AI system, such as significant life insurance premium increases or \n\neven policy denials . This allows human intervention to prevent biased out comes. \n\n• transparency with customers: Inform customers in the data protection statement \n\nabout the company's commitment to using high -quality, unbiased data in the AI \n\nsystem. \n\n## Requirement of secure processing \n\nUser story : implementing appropriate security measures for life insurance AI \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to conduct a thorough risk assessment to identify potential threats and \n\nvulnerabilities that could impact our customer data . This assessment will consider various \n\nfactors, including the type of data ( health data vs. basic customer information), processing \n\nactivities, and potential impact of a security breach. Based on this assessment, we will \n\nimplement appropriate technical and organizational measures (TOMs) to mitigate these \n\nrisks and ensure the security of our customer data. This is important to comply with the \n\nrequirement of security of the processing under the GDPR .\n\nExamples of TOMs may include: \n\n• data encryption: encrypting customer data at rest and in transit to protect \n\nconfidentiality ;\n\n• access controls: implementing strict access controls to limit who can access and \n\nmodify customer data ;\n\n• regular penetration testing: conducting penetration tests to identify and address \n\nvulnerabilities in the system's security posture ;\n\n• logging and auditing: maintaining detailed logs of system activity for monitoring \n\nand investigation of any suspicious behavior .(Original version – version December 2024) \n\n20 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nUser story : implementing specific security measures for life insurance AI \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we recognize that AI systems introduce specific risks beyond traditional data \n\nprocessing. These risks might include potential bias in training data or manipulation by \n\nunauthorized actors. To address these specific risks we will implement additional \n\nmeasures in conjunction with the baseline GDPR -compliant TOMs. This is important to \n\ncomply with the requirement of security of the processing under the AI Act .\n\nExamples of these additional measures may include: \n\n• data validation and quality assurance: implementing processes to ensure the \n\nquality and integrity of the data used to train and operate the AI system. This could \n\ninvolve data provenance tracking and anomaly detection to identify potential \n\nbiases or manipulation attempts. \n\n• human oversight: establishing a framework for human oversight throughout the AI \n\nsystem's lifecycle. This could involve human review of high -risk data points, \n\nmonitoring the system's performance for fairness and accuracy, and intervening in \n\ncritical decision -making pathways. \n\n## Requirement of (the ability of demonstrating) accountability \n\nUser story : documenting the legal basis \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to have a clear and concise record of the legal basis for collecting and \n\nusing customer data in the AI system. This is important to comply with the GDPR principle \n\nof (demonstrating) accountability (also in the context of audits or investigations). \n\nUser story : conducting a Fundamental Rights Impact Assessment (FRIA) \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to develop and maintain a comprehensive FRIA (Fundamental Rights \n\nImpact Assessment) to proactively identify and mitigate potential risks associated with \n\nthis AI system. This is important to comply with the AI Act's requirements for high -risk AI \n\nsystems and promote fair and non -discriminatory premium calculations for our customers. \n\nThis obligation is in addition to the GDPR rules on data protection impact assessment. \n\n* * *(Original version – version December 2024) \n\n21 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# References  \n\n> 0\n\nThis paper also utilized spelling and grammar checking, and a large language model, as a \n\ntool for refining and correcting initial text section s.  \n\n> i\n\nRegulation (EU)2016/679 of the European Parliament and of the Council of 27 April 2016 \n\non the protection of natural persons with regard to the processing of personal data and on \n\nthe free movement of such data, and repealing Directive 95/46/EC (General Data \n\nProtection Regulation),  Official Journal of the European Union L 119/1, 4.5.2016, p. 1 –88. \n\nii  Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June \n\n2024 laying down harmonised rules on artificial intelligence  (Artificial Intelligence Act), \n\nOfficial Journal of the European Union L 199/1, 12.7.2024, p. 1 –120. \n\niii  Art. 20, §1, 1°, Data Protection Authority Act of 3 December 2017, amended by the Act of \n\n25 December 2023. \n\niv In the examples discussing high -risk AI systems listed in Annex III of the AI act, the \n\npossible exceptions referred to in section 6.3 of the AI act are not taken into account. \n\nv In the examples addressing life insurance, possible gaps in the legal basis (see decision \n\n109/2024 of the Litigation Chamber of the Belgian DPA) are not taken into account. \n\nvi  Artificial Intelligence Act, Article 3 (1) \n\nvii For more information on the legal basis ‘legitimate interest’, see the following opinion of \n\nthe European Data Protection Board: Opinion 28/2024 on certain data protection aspects \n\nrelated to the processing  of personal data in the context of AI models \n\n(https://www.edpb.europa.eu/news/news/2024/edpb -opinion -ai -models -gdpr -principles -\n\nsupport -responsible -ai_en )",
  "fetched_at_utc": "2026-02-08T19:07:18Z",
  "sha256": "8072688f58c2242ab3989cdf7d78db7dfec1eb82dd1953c8ab68f0227e59f3d3",
  "meta": {
    "file_name": "Artificial Intelligence Systems and the GDPR - Belgium.pdf",
    "file_size": 489694,
    "relative_path": "pdfs\\Artificial Intelligence Systems and the GDPR - Belgium.pdf",
    "jina_status": 20000,
    "jina_code": 200,
    "usage": {
      "tokens": 8308
    }
  }
}