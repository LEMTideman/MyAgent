{
  "doc_id": "web-ai-act-service-desk-ec-europa-eu-en-ai-act-recital-73-f3551514995f",
  "source_type": "web",
  "source": "https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-73",
  "title": "Recital 73 | AI Act Service Desk",
  "text": "```markdown\n# Recital 73\n\n## Overview\n\nHigh-risk AI systems should be designed and developed in such a way that natural persons can oversee their functioning, ensure that they are used as intended and that their impacts are addressed throughout the system's lifecycle. To that end, appropriate human oversight measures should be identified by the provider of the system before its placement on the market or putting into service. In particular, where appropriate, such measures should guarantee that the system is subject to in-built operational constraints that cannot be overridden by the system itself and is responsive to the human operator, and that the natural persons to whom human oversight has been assigned have the necessary competence, training and authority to carry out that role. It is also essential, as appropriate, to ensure that high-risk AI systems include mechanisms to guide and inform a natural person to whom human oversight has been assigned to make informed decisions if, when and how to intervene in order to avoid negative consequences or risks or stop the system if it does not perform as intended. Considering the significant consequences for persons in the case of an incorrect match by certain biometric identification systems, it is appropriate to provide for an enhanced human oversight requirement for those systems so that no action or decision may be taken by the deployer on the basis of the identification resulting from the system unless this has been separately verified and confirmed by at least two natural persons. Those persons could be from one or more entities and include the person operating or using the system. This requirement should not pose unnecessary burden or delays, and it could be sufficient that the separate verifications by different persons are automatically recorded in the logs generated by the system. Given the specificities of the areas of law enforcement, migration, border control and asylum, this requirement should not apply where Union or national law considers the application of that requirement to be disproportionate.\n\n### Related Articles\n\n*   [Article 14: Human oversight](/en/ai-act/article-14)\n*   [Article 19: Automatically generated logs](/en/ai-act/article-19)\n\n---\n\n## Contact Us\n\n*   [Contact us](mailto:CNECT-AIOFFICE@ec.europa.eu)\n\n---\n\n## About Us\n\n*   [Copyright notice](/en/copyright-notice)\n*   [Legal notice](/en/legal-notice)\n*   [Privacy statement](/en/privacy-statement-ai-act-service-desk-website)\n*   [About Directorate-General CONNECT](https://commission.europa.eu/about-departments-and-executive-agencies/communications-networks-content-and-technology_en)\n\n---\n\n## Additional Links\n\n[Contact Europe's Commission](https://commission.europa.eu/contact-eu/social-media-channels_en#/search?page=0&institutions=european_commission)\n\n[Resources for partners](https://commission.europa.eu/resources-partners_en)\n\n[Report an IT vulnerability](https://commission.europa.eu/legal-notice/vulnerability-disclosure-policy_en)\n\n[Languages on our websites](https://commission.europa.eu/languages-our-websites_en)\n\n[Cookies](https://commission.europa.eu/cookies_en)\n\n[Privacy policy](https://commission.europa.eu/privacy-policy_en)\n\n[Legal notice](https://commission.europa.eu/legal-notice_en)\n```",
  "fetched_at_utc": "2026-02-10T08:46:44Z",
  "sha256": "f3551514995ffe62f8f2763e28dc38741a58de6a0814a9fee446ae1f1205bc48",
  "meta": {
    "jina_status": 20000,
    "jina_code": 200,
    "description": null,
    "links": null,
    "images": null,
    "usage": {
      "tokens": 12000
    }
  }
}