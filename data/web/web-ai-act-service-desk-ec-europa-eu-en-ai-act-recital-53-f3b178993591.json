{
  "doc_id": "web-ai-act-service-desk-ec-europa-eu-en-ai-act-recital-53-f3b178993591",
  "source_type": "web",
  "source": "https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-53",
  "title": "Recital 53 | AI Act Service Desk",
  "text": "```markdown\n# Recital 53\n\nIt is also important to clarify that there may be specific cases in which AI systems referred to in pre-defined areas specified in this Regulation do not lead to a significant risk of harm to the legal interests protected under those areas because they do not materially influence the decision-making or do not harm those interests substantially. For the purposes of this Regulation, an AI system that does not materially influence the outcome of decision-making should be understood to be an AI system that does not have an impact on the substance, and thereby the outcome, of decision-making, whether human or automated. An AI system that does not materially influence the outcome of decision-making could include situations in which one or more of the following conditions are fulfilled. The first such condition should be that the AI system is intended to perform a narrow procedural task, such as an AI system that transforms unstructured data into structured data, an AI system that classifies incoming documents into categories or an AI system that is used to detect duplicates among a large number of applications. Those tasks are of such narrow and limited nature that they pose only limited risks which are not increased through the use of an AI system in a context that is listed as a high-risk use in an annex to this Regulation. The second condition should be that the task performed by the AI system is intended to improve the result of a previously completed human activity that may be relevant for the purposes of the high-risk uses listed in an annex to this Regulation. Considering those characteristics, the AI system provides only an additional layer to a human activity with consequently lowered risk. That condition would, for example, apply to AI systems that are intended to improve the language used in previously drafted documents, for example in relation to professional tone, academic style of language or by aligning text to a certain brand messaging. The third condition should be that the AI system is intended to detect decision-making patterns or deviations from prior decision-making patterns. The risk would be lowered because the use of the AI system follows a previously completed human assessment which it is not meant to replace or influence, without proper human review. Such AI systems include for instance those that, given a certain grading pattern of a teacher, can be used to check ex post whether the teacher may have deviated from the grading pattern so as to flag potential inconsistencies or anomalies. The fourth condition should be that the AI system is intended to perform a task that is only preparatory to an assessment relevant for the purposes of the AI systems listed in an annex to this Regulation, thus making the possible impact of the output of the system very low in terms of representing a risk for the assessment to follow. That condition covers, inter alia, smart solutions for file handling, which include various functions from indexing, searching, text and speech processing or linking data to other data sources, or AI systems used for translation of initial documents. In any case, AI systems used in high-risk use-cases listed in an annex to this Regulation should be considered to pose significant risks of harm to the health, safety or fundamental rights if the AI system implies profiling within the meaning of [Article 4](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-4), point (4) of Regulation (EU) 2016/679 or [Article 3](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-3), point (4) of Directive (EU) 2016/680 or [Article 3](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-3), point (5) of Regulation (EU) 2018/1725. To ensure traceability and transparency, a provider who considers that an AI system is not high-risk on the basis of these conditions should draw up documentation regarding their assessment before placing them on market or putting them into service and provide this documentation upon request from national competent authorities. Such a provider should register their AI system in Europe's database established under this regulation.\n\n### This Recital relates to\n\n*   [Article 6: Classification rules for high-risk AI systems](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-6)\n*   [Article 7: Amendments to Annex III](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-7)\n\nView the [official text](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:L_202401689). The text used in this tool is based on \"Artificial Intelligence Act (Regulation (EU) 2024/1689), Official version dated June 13th 2024\".\n```",
  "fetched_at_utc": "2025-12-29T09:03:30Z",
  "sha256": "f3b1789935919612938bb689ad5931b7b9ee2bc3ae119d7d8dfe163caa402064",
  "meta": {
    "jina_status": 20000,
    "jina_code": 200,
    "description": null,
    "links": null,
    "images": null,
    "usage": {
      "tokens": 12000
    }
  }
}