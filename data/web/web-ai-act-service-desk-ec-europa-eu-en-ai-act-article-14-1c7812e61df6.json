{
  "doc_id": "web-ai-act-service-desk-ec-europa-eu-en-ai-act-article-14-1c7812e61df6",
  "source_type": "web",
  "source": "https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-14",
  "title": "Article 14: Human oversight | AI Act Service Desk",
  "text": "```markdown\n# Article 14: Human oversight\n\n## Chapter III: High-Risk AI Systems\n### Section 2: Requirements for High-Risk AI Systems\n\n#### Summary\nHigh-risk AI systems must be designed to allow human oversight during their operation to minimize risks to health, safety, and fundamental rights. Oversight measures should be proportional to the system's risks, autonomy, and context of use, including built-in safeguards and deployer-implemented controls. Humans must be able to monitor, interpret, and override the system, with awareness of potential over-reliance on AI outputs. For remote biometric identification systems, critical decisions must be verified by at least two competent individuals, except in specific law enforcement or border control scenarios.\n\nThe summaries are meant to provide helpful explanation but are not legal binding.\n\n#### Article 14: Human oversight\n\n1. High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which they are in use.\n   \n2. Human oversight shall aim to prevent or minimize the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular where such risks persist despite the application of other requirements set out in this Section.\n   \n3. The oversight measures shall be commensurate with the risks, level of autonomy and context of use of the high-risk AI system, and shall be ensured through either one or both of the following types of measures:\n   - measures identified and built into the high-risk AI system by the provider before it is placed on the market or put into service;\n   - measures identified by the provider before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the deployer.\n   \n4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be provided to the deployer in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate:\n   - properly understand the relevant capacities and limitations of the high-risk AI system and be able to duly monitor its operation, including in view of detecting and addressing anomalies, dysfunctions and unexpected performance;\n   - remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations for decisions to be taken by natural persons;\n   - correctly interpret the high-risk AI systemâ€™s output, taking into account, for example, the interpretation tools and methods available;\n   - decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override or reverse the output of the high-risk AI system;\n   - intervene in the operation of the high-risk AI system or interrupt the system through a 'stop' button or a similar procedure that allows the system to come to a halt in a safe state.\n   \n5. For high-risk AI systems referred to in point 1(a) of [Annex III](https://ai-act-service-desk.ec.europa.eu/en/ai-act/annex-3), the measures referred to in paragraph 3 of this Article shall be such as to ensure that no action or decision is taken by the deployer on the basis of an identification resulting from the system unless that identification has been separately verified and confirmed by at least two natural persons with necessary competence, training and authority.\n\nThe requirement for a separate verification by at least two natural persons shall not apply to high-risk AI systems used for purposes specified as being law enforcement-related (migration/border control/asylum).\n\n### Relevant recitals\n- [Recital 27](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-27)\n- [Recital 66](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-66)\n- [Recital 73](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-73)\n- [Recital 91](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-91)\n\nView the [official text](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:L_202401689). The text used in this tool is based on \"Artificial Intelligence Act (Regulation (EU) 2024/1689), Official version of 13 June 2024\".\n```",
  "fetched_at_utc": "2025-12-28T19:48:04Z",
  "sha256": "1c7812e61df60048ddb79dbab7c91d4b07913cbe61c5954c6d0f441301a49880",
  "meta": {
    "jina_status": 20000,
    "jina_code": 200,
    "description": null,
    "links": null,
    "images": null,
    "usage": {
      "tokens": 12000
    }
  }
}