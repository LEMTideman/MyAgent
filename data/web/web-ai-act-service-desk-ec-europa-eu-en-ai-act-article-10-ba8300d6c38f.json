{
  "doc_id": "web-ai-act-service-desk-ec-europa-eu-en-ai-act-article-10-ba8300d6c38f",
  "source_type": "web",
  "source": "https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-10",
  "title": "Article 10: Data and data governance | AI Act Service Desk",
  "text": "```markdown\n# Article 10: Data and data governance\n\n## Chapter III: High-Risk AI Systems\n### Section 2: Requirements for High-Risk AI Systems\n\n#### Summary\nHigh-risk AI systems must use high-quality training, validation, and testing datasets that are relevant, representative, and free of errors. Data governance and management practices should address, inter alia, design choices, data collection, preparation, bias detection, and mitigation. Datasets must consider the specific context of use, and reflect the specific geographical, contextual, behavioural, or functional characteristics of the environment in which the system will operate. These requirements apply to all datasets used in high-risk AI systems, even those not involving model training.\n\nThe summaries are meant to provide helpful explanation but are not legal binding.\n\n1. High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such data sets are used.\n\n2. Training, validation and testing data sets shall be subject to data governance and management practices appropriate for the intended purpose of the high-risk AI system. Those practices shall concern in particular:\n   - (a) the relevant design choices;\n   - (b) data collection processes and the origin of data, and in the case of personal data, the original purpose of the data collection;\n   - (c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and aggregation;\n   - (d) the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and represent;\n   - (e) an assessment of the availability, quantity and suitability of the data sets that are needed;\n   - (f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations;\n   - (g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);\n   - (h) the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those gaps and shortcomings can be addressed.\n\n3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible,\n   - (a) free of errors,\n   - (b) complete in view of\n     - (i)\n       * The intended purpose.\n     - (ii)\n       * Characteristics or elements that are particular to\n         * The specific geographical,\n         * Contextual,\n         * Behavioural or Functional setting within which\n           * The high-risk AI system is intended to be used.\n     - (iii)\n       * Persons or groups of persons in relation to whom\n         * The high-risk AI system is intended to be used.\n4. To the extent that it is strictly necessary for the purpose of ensuring bias detection and correction in relation to the high-risk AI systems in accordance with paragraph 2,\n   - (a) points (f) and (g),\n   - (b)\n     * Processing special categories of personal data,\n     * Subject to appropriate safeguards for\n       * The fundamental rights\n       * And freedoms\n       * Of natural persons.\n     - This applies only if:\n       - (i)\n         * Bias detection and correction cannot be effectively fulfilled by processing other data,\n         * Including synthetic or anonymized data;\n       - (ii)\n         * Special categories of personal data are subject to technical limitations on re-use,\n         * And state-of-the-art security\n         * And privacy-preserving measures,\n         * Including pseudonymization;\n       - (iii)\n         * Special categories of personal data are subject to measures to ensure that personal\n           **data processed**\n           **are secured**, protected,\n           **subjected** suitable safeguards,\n           including strict controls\n           **and documentation**\n           **of access**, avoiding misuse,\n           Ensure that only authorized persons have access\n             [to avoid](https://commission.europa.eu/ \"avoid\") misuses\n           **and ensure** that only authorized persons have access\n             [access](https://commission.europa.eu/ \"access\")\n               [to avoid](https://commission.europa.eu/ \"avoid\") misuses.\n                 [to avoid](https://commission.europa.eu/ \"avoid\") misuses.\n6. For the development of high-risk AI systems not using techniques involving the training of AI models,\n\n#### Relevant Recitals\n\n- [Recital 66](/en/ai-act/recital-66)\n- [Recital 67](/en/ai-act/recital-67)\n- [Recital 68](/en/ai-act/recital-68)\n- [Recital 69](/en/ai-act/recital-69)\n- [Recital 70](/en/ai-act/recital-70)\n- [Recital 76](/en/ai-act/recital-76)\n- [Recital 115](/en/ai-act/recital-115)\n\nView the [official text](https://eur-lex.europa.eu/legal-content/EN/TXT/html/?uri=OJ:L_202401689). The text used in this tool is the â€˜[Artificial Intelligence Act (Regulation (EU) 2024/1689), Official version of 13 June 2024](https://eur-lex.europa.eu/legal-content/EN/TXT/html/?uri=OJ:L_202401689)'.\n```",
  "fetched_at_utc": "2025-12-29T10:06:08Z",
  "sha256": "ba8300d6c38faf218f242fb7bdf53519dce285ea24145d44d9506fd67479635b",
  "meta": {
    "jina_status": 20000,
    "jina_code": 200,
    "description": null,
    "links": null,
    "images": null,
    "usage": {
      "tokens": 12000
    }
  }
}