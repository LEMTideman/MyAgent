{
  "doc_id": "web-ai-act-service-desk-ec-europa-eu-en-ai-act-article-3-ec148545f457",
  "source_type": "web",
  "source": "https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-3",
  "title": "Article 3: Definitions | AI Act Service Desk",
  "text": "```markdown\n# Article 3: Definitions\n\n## Summary\n\n[Article 3](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-3) of the AI Act provides definitions for terms used throughout the AI Act.\n\nThe summaries are meant to provide helpful explanation but are not legal binding.\n\n### Definition 1: 'AI system'\n\n**Definition:** A machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.\n\n### Definition 2: 'Risk'\n\n**Definition:** The combination of the probability of an occurrence of harm and the severity of that harm.\n\n### Definition 3: 'Provider'\n\n**Definition:** A natural or legal person, public authority, agency or other body that develops an AI system or a general-purpose AI model or that has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge.\n\n### Definition 4: 'Deployer'\n\n**Definition:** A natural or legal person, public authority, agency or other body using an AI system under its authority except where the AI system is used in the course of a personal non-professional activity.\n\n### Definition 5: 'Authorised Representative'\n\n**Definition:** A natural or legal person located or established in the Union who has received and accepted a written mandate from a provider of an AI system or a general-purpose AI model to perform and carry out on its behalf the obligations and procedures established by this Regulation.\n\n### Definition 6: 'Importer'\n\n**Definition:** A natural or legal person located or established in the Union that places on the market an AI system that bears the name or trademark of a natural or legal person established in a third country.\n\n### Definition 7: 'Distributor'\n\n**Definition:** A natural or legal person in the supply chain, other than the provider or the importer, that makes an AI system available on the Union market.\n\n### Definition 8: 'Operator'\n\n**Definition:** A provider, product manufacturer, deployer, authorised representative, importer or distributor.\n\n### Definition 9: 'Placing on the Market'\n\n**Definition:** The first making available of an AI system or a general-purpose AI model on the Union market.\n\n### Definition 10: 'Making Available on the Market'\n\n**Definition:** The supply of an AI system or a general-purpose AI model for distribution or use on the Union market in the course of a commercial activity, whether in return for payment or free of charge.\n\n### Definition 11: 'Putting into Service'\n\n**Definition:** The supply of an AI system for first use directly to the deployer or for own use in the Union for its intended purpose.\n\n### Definition 12: 'Intended Purpose'\n\n**Definition:** The use for which an AI系统是设计的，包括预期的使用场景和条件。\n\n### Definition 13: 'Reasonably Foreseeable Misuse’\n\n**Definition**: Use of an AI system in a way that is not in accordance with its intended purpose but may result from reasonably foreseeable human behavior or interactions with other systems, including other AI systems.\n\n### Definition 14: 'Safety Component’\n\n**Definition**: A component of a product (or general-purpose AI model) which fulfills a safety function for that product (or general-purpose AI model), ensuring it meets basic safety needs. Failure in these functions can lead to serious health and safety consequences.\n\n### Definition 15: ‘Instructions for Use’\n\n**Definition**: Information provided by the provider to inform users about what an AI system's intended purpose is and proper usage methods.\n\n### Definition 16: ‘Recall of an AI System’\n\n**Definition**: Any measure aimed at achieving immediate removal from service if needed due to concerns regarding compliance with required requirements set out in Chapter III Section 2.\n\n### Definition 17: ‘Withdrawal of an AI System’\n\n**Definition**: Any measure aimed at preventing further availability if there is no longer intent to continue using it within its original scope.\n\n### Definition 18: ‘Performance of an AI System’\n\n**Definition**: Ability to achieve its intended purpose effectively without causing unintended side effects like malfunctions (failure).\n\n### Definition 19: ‘Notifying Authority’\n\n**Definition**: National authority responsible for setting up and implementing necessary procedures for assessing conformity assessment bodies and monitoring them according to relevant Union harmonization legislation.\n\n### Definition 20: ‘Conformity Assessment’\n\n**Definition**: Process demonstrating whether requirements set out in Chapter III Section 2 relating to high-risk systems have been fulfilled. This includes testing services (certification), inspections etc., performed by third parties (conformity assessment bodies).\n\n### Definition 21: ‘Conformity Assessment Body’\n\n**Definition**: Third-party organization performing tests related to conformity assessments. These include testing services (certification), inspections etc., conducted by certified organizations (conformity assessment bodies).\n\n### Definition 22: ‘Notified Body’\n\n**Definition**: Conformity assessment body notified according to this regulation and other relevant Union harmonization legislation providing means to demonstrate conformity with specific requirements set out under this Regulation.\n\n### Definition 23: ‘Substantial Modification’\n\n**Definition**: Change introduced after placement on market leading to changes affecting compliance with requirements set out in Chapter III Section 2. It involves modifications altering intended purposes beyond those anticipated during initial conformity assessments. Such modifications must be addressed promptly upon becoming aware through appropriate mechanisms such as alerts concerning potential systemic risks identified by scientific panels within prescribed timelines. For example:\n\n* **Example**\n    * An update significantly alters user interface functionality.\n    * New features introduce new risks.\n    * Changes affect core functionalities.\n    * Additional data sources are added.\n    * Training datasets undergo substantial updates.\n    * Model architecture shifts.\n    * Integration issues arise.\n    * Performance metrics evolve.\n    * Security vulnerabilities emerge.\n    * Privacy regulations change.\n    * Regulatory frameworks shift direction.\n    * Legal precedents are altered.\n    * Legislative amendments are enacted.\n    * Regulations become obsolete due to technological advancements.\n    * New laws replace old ones requiring updated compliance strategies.\n    * Technological developments necessitate reevaluating existing policies and practices.\n* **Note**\n    * These examples illustrate broad scenarios rather than specific instances. Each case should be evaluated individually considering factors like industry standards applied prior to alterations compared with current best practices emerging post-altering processes. Specific circumstances might require additional analysis beyond simple comparisons between pre-existing versions versus subsequent iterations based solely on theoretical considerations alone.\n\n\n---\n\n## Relevant Recitals\n\n* [Recital 12](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-12)\n* [Recital 13](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-13)\n* [Recital 14](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-14)\n* [Recital 15](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-15)\n* [Recital 16](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-16)\n* [Recital 17](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-17)\n* [Recital 18](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-18)\n* [Recital 19](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recialn--00%EF%BC%A9%E2%80%AF%E2%80%AC%E2%80%B5%E2%80%A6%E2%80%B5%E2%80%A6%E2%80%B5%E2%80%A6%E2%80%B5%E2%80%A6%E2%80%B5%E2%80%A6%E2%80%B5%E2%80%A6%E2%80%B5.html?uri=OJ:L_&amp;=&amp;=&amp;=&amp;=&amp;=&amp;=&amp;=&amp;=&amp;=&amp;&amp;%E9%A9%B3&amp;%E7%A7%B0&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&amp;%E4%B9%A6&)\n\n## Other Resources\n\n[Guidelines on the definition of an AI system](https://ai-act-service-desk.ec.europa.eu/sites/default/files/2025-08/commission_guidelines_on_the_definition_of_an_artificial_intelligence_system_established_by_regulation_eu_&quot;) ([PDF Version](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:L_&amp;=&amp;=&amp;=&amp;=&amp;=&amp;=&amp;=&amp;=&amp;&quot;) / [DOCX Version](https://eur-legislation.int/doc/document.do?text=https:%F0%9F%A7B7_%C3%C3%C7%DDEU_Legislative_document_text_with_summary.pdf))\n\nView the official text at:\n[Artificial Intelligence Act (Regulation (EU) \\[Regulation\\] \\[Directive\\] \\[Commission Decree\\]) \\[Official version\\]](https://eur-legis.declaration.ejustice.justice.gov.uk/directives/regulation/(eu)/&quot;\\&quot;/&quot;/&quot;/&quot;/&quot/)\n```",
  "fetched_at_utc": "2025-12-28T21:19:36Z",
  "sha256": "ec148545f457bbc62d3342dd916bcfa2e309ee5f9da0c3571a19f8c90af316e5",
  "meta": {
    "jina_status": 20000,
    "jina_code": 200,
    "description": null,
    "links": null,
    "images": null,
    "usage": {
      "tokens": 12000
    }
  }
}