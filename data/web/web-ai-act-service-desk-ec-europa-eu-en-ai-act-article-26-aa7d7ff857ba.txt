```markdown
# Article 26: Obligations of deployers of high-risk AI systems

## Summary

Deployers of high-risk AI systems shall ensure they use the systems according to instructions, with competent human oversight, and monitor their operation. They must manage input data, keep logs for at least six months, and inform providers and authorities of any risks or incidents. Deployers must also notify workers’ representatives and the affected workers if the system is used in the workplace. Public authorities deploying high-risk AI systems must comply with registration obligations (see [Article 49](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-49)) and ensure the system is registered in the EU database. Deployers must inform individuals subject to AI-assisted decisions and cooperate with authorities on regulatory actions.

Law enforcement authorities using high-risk AI system for post-remote biometric identification in targeted searches shall obtain authorisation in advance or not later than within 48 hours, save some exceptions. Each use must be documented, and annual reports submitted to authorities. 

The summaries are meant to provide helpful explanation but are not legal binding.

1. Deployers of high-risk AI systems shall take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems, pursuant to paragraphs 3 and 6.
2. Deployers shall assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support.
3. The obligations set out in paragraphs 1 and 2, are without prejudice to other deployer obligations under Union or national law and to the deployer’s freedom to organise its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider.
4. Without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over the input data, that deployer shall ensure that input data is relevant and sufficiently representative in view of the intended purpose of the high-risk AI system.
5. Deployers shall monitor the operation of the high-risk AI system on the basis of the instructions for use and, where relevant, inform providers in accordance with [Article 72](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-72). Where deployers have reason to consider that the use of the high-risk AI system in accordance with the instructions may result in that AI system presenting a risk within the meaning of [Article 79(1)](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-79), they shall, without undue delay, inform the provider or distributor and the relevant market surveillance authority, and shall suspend the use of that system. Where deployers have identified a serious incident, they shall also immediately inform first the provider, and then the importer or distributor and the relevant market surveillance authorities of that incident. If the deployer is not able to reach the provider, [Article 73](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-73) shall apply _mutatis mutandis_. This obligation shall not cover sensitive operational data of deployers of AI systems which are law enforcement authorities.

For deployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law, the monitoring obligation set out in paragraph 5 shall be deemed to be fulfilled by complying with rules on internal governance arrangements, processes and mechanisms pursuant to relevant financial service law.

6. Deployers of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system to a period appropriate for its intended purpose of use, for at least six months unless provided otherwise in applicable Union or national law, particularly in Union law on personal data protection.

Deployers that are financial institutions subject to requirements regarding their internal governance arrangements processes under Union financial services law shall maintain these logs as part of their documentation kept pursuant to relevant Union financial service law.

7. Before putting into service or using a high-risk AI system at workstations employed by employees (employing companies), deployors who employ employees shall inform them about being subject to this high-risk AI system's usage. This information should be provided according to existing rules on employee information transparency.

8. Deployers who are public authorities or entities under Union law governing communications networks content technology (Connect Directorate-General) shall comply with registration obligations referred to in [Article 49](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-49). When such entities find that a new high-risk AI system planned for use has not been registered in Annex III (the list), they cannot utilize that system until authorization is obtained through judicial authority or administrative authority whose decision is legally binding subject to judicial review.

9. Where applicable, entities deploying high-risk AI systems can access information provided under [Article 13](https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-13) concerning their responsibilities under personal data protection impact assessments under Regulation (EU) No 2016/679 or Directive (EU) No 2016/680.

10. Without prejudice to Directive (EU) No 2016/680 (personal data protection impact assessment), when investigating targeted searches involving potential suspects suspected or convicted of committing crimes via remote biometrics identification technologies like post remote biometric identification (PRBI), entities deploying PRBI technologies would need an authorisation granted ex ante before utilizing those technologies unless approved otherwise by judicial authority or administrative authority whose decision is legally binding subject to judicial review.

If an entity requests authorization due to authorized investigations involving targets suspected or convicted of crimes via remote biometrics identification technologies like PRBI but receives a denial from judicial authority or administrative authority whose decision is legally binding subject to judicial review:

* The entity would stop using associated POST remote biometric identification technologies linked directly thereto.
* Personal data related exclusively used for these purposes would be deleted.
* These technologies could only be utilized for initial identity verification based on objective verifiable facts directly linked specifically towards crime-related offenses rather than targeting specific individuals without connection.
* Decisions produced solely based on outputs from these technologies would not produce adverse legal effects against individuals without explicit justification.
* Judicial supervision bodies would play roles similar as per Directive (EU) No 2016/680 regarding supervisory authorities' duties.

This clause does not restrict how entities might operate beyond what mandatory provisions require under Directive (EU) No 2016/680 regarding supervisory bodies' functions.

Entities deploying PRBI technologies would submit annual reports detailing their utilization patterns while ensuring confidentiality over sensitive operational data pertaining exclusively related with justice endeavors conducted through these technologies – except disclosures needed during audits directed towards justice-related matters requiring compliance with privacy regulations.

Members states may establish additional restrictions on PRBI technology usage consistent with Union laws on personal data protection.

11. Entities deploying high-risk AI systems listed in Annex III must disclose all such information within three years after placing them onto commercial markets unless mandated otherwise by Union legislation.
12. Entities deploying high-risk AI systems listed in Annex III must collaborate closely with competent authorities involved in handling such issues as required by Union legislation.

### Relevant Recitals

* [Recital 84](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-84)
* [Recital 91](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-91)
* [Recital 92](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-92)
* [Recital 93](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-93)
* [Recital 95](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-95)
* [Recital 96](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-96)
* [Recital 134](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recital-134)
* [Recital 136](https://ai-act-service-desk.ec.europa.eu/en/ai-act/recialtment%EF%B8%8F%EF%B8%AD%E2%80%A6%E2%80%A6%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0_%C4%BEv.%C3%BCr.%C3%ACn.+L._+II+%E5%B7%B4+%E5%B7%B4+%E5%B7%B4+%E5%B7%B4+%E5%B7%B4+%E5%B7%B4+%E5%B7%B4+%E5%B7%B4+%E5%B7%B4+%E5%B7%B4+%E5%B7%B4+%E5%B7%B4+%E5+BfB&amp;utm_source=sharethis.com&amp;utm_medium=social&amp;utm_content=sharethis_social_sharing_button&amp;ttw.share-this-share-buttons-button&amp;url=https:%2F%2Fwww.ai.act.service.desksite.ee%2Fen%2Faifaq-questionnaire-answer" rel="nofollow noopener noreferrer" target="_blank">Share this article</a>
```