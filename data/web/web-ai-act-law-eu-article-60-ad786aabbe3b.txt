```markdown
# Art. 60 AI Act - Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes

## Table of Contents

[Back to AI Act](https://ai-act-law.eu)

### Main Content

1. **Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes may be conducted by providers or prospective providers of high-risk AI systems listed in [Annex III](https://ai-act-law.eu/annex/3/), in accordance with this Article and the real-world testing plan referred to in this Article, without prejudice to the prohibitions under [Article 5](https://ai-actlaw.eu/article/5/).**

    1. The Commission shall, by means of implementing acts, specify the detailed elements of the real-world testing plan.
    2. Those implementing acts shall be adopted in accordance with the examination procedure referred to in [Article 98(2)](https://ai-actlaw.eu/article/98/).
    
    This paragraph shall be without prejudice to Union or national law on the testing in real world conditions of high-risk AI systems related to products covered by Union harmonisation legislation listed in [Annex I](https://ai-actlaw.eu/annex/1/) (e.g., law enforcement, migration, asylum and border control management).

2. Providers or prospective providers may conduct testing of high-risk AI systems referred to in [Annex III](https://ai-actlaw.eu/annex/3/) in real world conditions at any time before the placing on the market or the putting into service of the AI system on their own or in partnership with one or more deployers or prospective deployers.

3. The testing of high-risk AI systems in real world conditions under this Article shall be without prejudice to any ethical review that is required by Union or national law.

4. Providers or prospective providers may conduct the testing in real world conditions only where all of the following conditions are met:

    1. The provider or prospective provider has drawn up a real-world testing plan and submitted it to the market surveillance authority in the Member State where the testing in real world conditions is to be conducted.
    2. The market surveillance authority in the Member State where the testing in real world conditions is to be conducted has approved the testing in real world conditions and the real-world testing plan; if there is no explicit response within 30 days, it is understood that both have been approved; national law may provide for tacit approval.
    3. The provider or prospective provider, except for providers or prospective providers of high-risk AI systems listed in points 1, 6 and 7 of [Annex III](https://ai-actlaw.eu/annex/3/) for areas like law enforcement, migration, asylum and border control management, and high-risk AI systems listed in point 2 of [Annex III](https://ai-actlaw.eu/annex/3/) for areas like law enforcement, migration, asylum and border control management have registered their testing plans with a Union-wide unique single identification number according to [Article 71(4)](https://ai-actlaw.eu/article/71/) with specified information; these providers also have registered their tests using a secure non-public section of the EU database according to [Article 49(4)](https://ai-actlaw.eu/article/49/) point (d); these providers are established within the EU or have appointed a legal representative who is based within Europe.
    4. The provider or prospective provider conducting the testing in real world conditions is established within the EU or has appointed a legal representative who holds suitable qualifications for handling relevant fields and possesses sufficient capacity, training and authority.
    5. Data collected and processed during the testing must be transferred securely to third countries only upon appropriate safeguards being implemented.
    6. The duration of each test should not exceed what is necessary to achieve its objectives; if needed, it can be extended for another period of six months with prior notice to inform potential users about such extensions.
    7. Subjects involved must belong to vulnerable groups due to their age or disability; they must receive adequate protection; and personal data collected cannot be used negatively against them.
    8. Where possible, tests are overseen by both the provider/contractor conducting them and by those responsible for deploying them via individuals equipped with suitable qualifications.
    9. Predictions, recommendations or decisions made by an AI system can be reversed and disregarded.

5. Any subjects involved must withdraw from a test at any time without consequences through revocation of informed consent; requests for immediate removal upon termination will not affect previous operations.

6. A serious incident identified during a test must be reported promptly to a national market surveillance authority as per [Article 73](https://ai-actlaw.eu/article/73/) (or as per local laws).

7. If deployed together with one or more deployers, all parties must agree on roles and responsibilities regarding compliance with requirements set forth under this regulation.

8. Subject companies participating directly or indirectly (including end-users) must give informed consent as per [Article 61](https://ai-actlaw.eu/article/61/) (or comply with local laws).

9. Providers are liable under applicable Union and national liability laws for damages arising from their tests.

---

### Suitable Recitals

[(141) Requirements for testing high-risk AI systems under real-world conditions](https://ai-act-law.eu/recital/141/)
```

This refined version maintains all essential content while improving clarity and organization.
```