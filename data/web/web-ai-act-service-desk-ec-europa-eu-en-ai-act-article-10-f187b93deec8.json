{
  "doc_id": "web-ai-act-service-desk-ec-europa-eu-en-ai-act-article-10-f187b93deec8",
  "source_type": "web",
  "source": "https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-10",
  "title": "Article 10: Data and data governance | AI Act Service Desk",
  "text": "```markdown\n# Article 10: Data and data governance\n\n## Chapter III: High-Risk AI Systems\n### Section 2: Requirements for High-Risk AI Systems\n\n#### Summary\nHigh-risk AI systems must use high-quality training, validation, and testing datasets that are relevant, representative, and free of errors. Data governance and management practices should address, inter alia, design choices, data collection, preparation, bias detection, and mitigation. Datasets must consider the specific context of use, and reflect the specific geographical, contextual, behavioural, or functional characteristics of the environment in which the system will operate. These requirements apply to all datasets used in high-risk AI systems, even those not involving model training.\n\nThe summaries are meant to provide helpful explanation but are not legal binding.\n\n1. High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such data sets are used.\n\n2. Training, validation and testing data sets shall be subject to data governance and management practices appropriate for the intended purpose of the high-risk AI system. Those practices shall concern in particular:\n   - (a) the relevant design choices;\n   - (b) data collection processes and the origin of data, and in the case of personal data, the original purpose of the data collection;\n   - (c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and aggregation;\n   - (d) the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and represent;\n   - (e) an assessment of the availability, quantity and suitability of the data sets that are needed;\n   - (f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations;\n   - (g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);\n   - (h) the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those gaps and shortcomings can be addressed.\n\n3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible,\n   - (a) free of errors,\n   - (b) complete in view of\n     - (i)\n       * The intended purpose.\n     - (ii)\n       * Characteristics or elements that are particular to\n         * **Specific geographical**\n         * **Contextual**\n         * **Behavioural** or **Functional settings** within which\n           * **The high-risk AI system is intended to be used**.\n     - (iii)\n       * That personal data is collected from individuals who have provided their consent.\n4. To comply with paragraph 2(e),\n   - (a) Bias detection cannot be effectively fulfilled by processing other data,\n   - (b) Special categories of personal data are subject to technical limitations on re-use,\n   - (c) Special categories of personal data are subject to measures to ensure they remain secure,\n   - (d) Special categories of personal data are not transmitted or otherwise accessed by other parties,\n5. For purposes related to bias detection and correction in accordance with paragraph 2(f), points( f )and(g )of this Article,\n   - (a) Processing special categories requires exceptional processing based on appropriate safeguards for natural person's rights.\n6. This exceptional processing applies when:\n   - (a) It is strictly necessary for detecting or correcting biases.\n7. The providers' exceptions require additional safeguards:\n   - (a) The reason why special categories were processed strictly necessary for detecting or correcting biases.\n8. Records documenting these reasons must include:\n   - (a) Reasons why other types of processing were not effective.\n9. If there is no alternative way than processing special categories,\n10. The record also includes:\n    - (a)\n      * Reason why processing other types was not effective.\n    - (b)\n      * Reasons why processing other types would not achieve its goal.\n\n#### Relevant Recitals\n- [Recital 66](/en/ai-act/recital-66)\n- [Recital 67](/en/ai-act/recital-67)\n- [Recital 68](/en/ai-act/recital-68)\n- [Recital 69](/en/ai-act/recital-69)\n- [Recital 70](/en/ai-act/recital-70)\n- [Recital 76](/en/ai-act/recital-76)\n- [Recital 115](/en/ai-act/recital-115)\n\nView the [official text](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:L_202401689). The text used in this tool is \"Artificial Intelligence Act (Regulation (EU) 2024/1689)\" published on June 13th 2024.\n```",
  "fetched_at_utc": "2025-12-28T21:23:14Z",
  "sha256": "f187b93deec89b5aa47c97d19dcc5acf1eb1752e6e6ede982b7de7965cc07037",
  "meta": {
    "jina_status": 20000,
    "jina_code": 200,
    "description": null,
    "links": null,
    "images": null,
    "usage": {
      "tokens": 12000
    }
  }
}