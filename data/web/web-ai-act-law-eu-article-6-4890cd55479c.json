{
  "doc_id": "web-ai-act-law-eu-article-6-4890cd55479c",
  "source_type": "web",
  "source": "https://ai-act-law.eu/article/6/",
  "title": "Art. 6 AI Act - Classification rules for high-risk AI systems - AI Act",
  "text": "```markdown\n# Art. 6 AI Act - Classification rules for high-risk AI systems\n\n## Irrespective of whether an AI system is placed on the market or put into service independently of the products referred to in points (a) and (b), that AI system shall be considered to be high-risk where both of the following conditions are fulfilled:\n\n1.  The AI system is intended to be used as a safety component of a product, or the AI system is itself a product, covered by the Union harmonisation legislation listed in [Annex I](https://ai-act-law.eu/annex/1/).\n2.  The product whose safety component pursuant to point (a) is the AI system, or the AI system itself as a product, is required to undergo a third-party conformity assessment, with a view to the placing on the market or the putting into service of that product pursuant to the Union harmonisation legislation listed in [Annex I](https://ai-act-law.eu/annex/1/).\n\nIn addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in [Annex III](https://ai-act-law.eu/annex/3/) shall be considered to be high-risk.\n\nBy derogation from paragraph 2, an AI system referred to in [Annex III](https://ai-act-law.eu/annex/3/) shall not be considered to be high-risk where it does not pose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by not materially influencing the outcome of decision making.\n\nThe first subparagraph shall apply where any of the following conditions is fulfilled:\n\n1.  The AI system is intended to perform a narrow procedural task.\n2.  The AI system is intended to improve the result of a previously completed human activity.\n3.  The AI system is intended to detect decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment, without proper human review; or\n4.  The AI system is intended to perform a preparatory task to an assessment relevant for the purposes of the use cases listed in [Annex III](https://ai-act-law.eu/annex/3/).\n\nNotwithstanding the first subparagraph, an AI system referred to in [Annex III](https://ai-act-law.eu/annex/3/) shall always be considered to be high-risk where the AI system performs profiling of natural persons.\n\n**Note:** A provider who considers that an AI system referred to in [Annex III](https://ai-act-law.eu/annex/3/) is not high-risk shall document its assessment before that system is placed on the market or put into service. Such provider shall be subject to the registration obligation set out in [Article 49(2)](https://ai-act-law.eu/article/49/(2)/). Upon request of national competent authorities, the provider shall provide the documentation of the assessment.\n\nThe Commission shall, after consulting the European Artificial Intelligence Board (the ‘Board’) and no later than 2 February 2026, provide guidelines specifying the practical implementation of this Article in line with [Article 96](https://ai-act-law.eu/article/96/) together with a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk.\n\nThe Commission is empowered to adopt delegated acts in accordance with [Article 97](https://ai-act-law.eu/article/97/) in order to amend paragraph 3, second subparagraph, of this Article by adding new conditions to those laid down therein, or by modifying them, where there is concrete and reliable evidence of the existence of AI systems that fall under the scope of [Annex III](https://ai-act-law.eu/annex/3/), but do not pose a significant risk of harm to the health, safety or fundamental rights of natural persons.\n\nThe Commission shall adopt delegated acts in accordance with [Article 97](https://ai-act-law.eu/article/97/) in order to amend paragraph 3, second subparagraph, of this Article by deleting any of the conditions laid down therein, where there is concrete and reliable evidence that this is necessary to maintain the level of protection of health, safety and fundamental rights provided for by this Regulation.\n\nAny amendment to the conditions laid down in paragraph 3, second subparagraph, adopted in accordance with paragraphs 6 and 7 of this Article shall not decrease the overall level of protection of health, safety and fundamental rights provided for by this Regulation and shall ensure consistency with the delegated acts adopted pursuant to [Article 7(1)](https://ai-act-law.eu/article/7/(1)), and take account of market and technological developments.\n\n## Suitable Recitals\n\n*   [(26)](https://ai-act-law.eu/recital/26/) Risk-based approach\n*   [(46)](https://ai-act-law.eu/recital/46/) Mandatory security requirements for high-risk AI systems in Europe\n*   [(47)](https://ai-act-laweu/recital/)\n*   [(48)](https://ai-actlaweu/recital/)\n*   [(49)](https://ai-actlaweu/recital/)\n*   [(50)](https://ai-actlaweu/recital/)\n*   [(51)](https://ai-actslaweu/recital/)\n*   [(52)](https://aislaweu/recital/)\n*   [(53)](https://aislaweu/recital/)\n*   [(54)](https://aislaweu/recital/)\n*   [(55)](https://aislaweu/recital/)\n*   [(56)](https://aislaweu/recital/)\n*   [(57)](https://aislaweu/recital/)\n*   [(58)](https://aislaweu/recital/)\n*   [(59)](https://aislaweu/recital/)\n*   [(60)](https://aislaweu/recial)\n```",
  "fetched_at_utc": "2026-02-09T21:24:51Z",
  "sha256": "4890cd55479c5ff933de04054b71e553e1b06692be9b5366aa2268f1c5098210",
  "meta": {
    "jina_status": 20000,
    "jina_code": 200,
    "description": null,
    "links": null,
    "images": null,
    "usage": {
      "tokens": 12000
    }
  }
}