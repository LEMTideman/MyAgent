{
  "doc_id": "web-ai-act-service-desk-ec-europa-eu-en-ai-act-article-10-cafc1a71606b",
  "source_type": "web",
  "source": "https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-10",
  "title": "Article 10: Data and data governance | AI Act Service Desk",
  "text": "```markdown\n# Article 10: Data and data governance\n\n## Chapter III: High-Risk AI Systems\n### Section 2: Requirements for High-Risk AI Systems\n\n#### Summary\nHigh-risk AI systems must use high-quality training, validation, and testing datasets that are relevant, representative, and free of errors. Data governance and management practices should address, inter alia, design choices, data collection, preparation, bias detection, and mitigation. Datasets must consider the specific context of use, and reflect the specific geographical, contextual, behavioural, or functional characteristics of the environment in which the system will operate. These requirements apply to all datasets used in high-risk AI systems, even those not involving model training.\n\nThe summaries are meant to provide helpful explanation but are not legal binding.\n\n1. High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such data sets are used.\n\n2. Training, validation and testing data sets shall be subject to data governance and management practices appropriate for the intended purpose of the high-risk AI system. Those practices shall concern in particular:\n   - (a) the relevant design choices;\n   - (b) data collection processes and the origin of data, and in the case of personal data, the original purpose of the data collection;\n   - (c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and aggregation;\n   - (d) the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and represent;\n   - (e) an assessment of the availability, quantity and suitability of the data sets that are needed;\n   - (f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations;\n   - (g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);\n   - (h) the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those gaps and shortcomings can be addressed.\n\n3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible,\n   - (a) free of errors,\n   - (b) complete in view of\n     - (i)\n       * The intended purpose.\n     - (ii)\n       * Characteristics or elements that are particular to\n         * **Specific geographical**\n         * **Contextual**\n         * **Behavioural** or **Functional settings** within which\n           * **The high-risk AI system is intended to be used**.\n     - (iii)\n       * That personal data is collected from individuals who have provided their explicit consent.\n4. To comply with paragraph 2(e),\n   - (a) Bias detection cannot be effectively performed by processing other types of\n     - (i)\n       * Data.\n     - (ii)\n       * Synthetic or anonymized data.\n5. For purposes related to bias detection,\n   - (a) Special categories of personal data may be processed if they cannot be effectively detected through other types of\n     - (i)\n       * Data.\n8. If it is strictly necessary for detecting or correcting biases,\n   - (a) Processing special categories requires appropriate safeguards for respecting fundamental rights.\n8. The safeguards include:\n   - (a) Protection against misuse.\n4. The safeguards also include:\n   - (a) Strict controls over access to personal information.\n4. The safeguards also include:\n   - (a) Documentation regarding access by authorized personnel.\n4. The safeguards also include:\n   - (a) Limitations on re-use so as not to violate privacy protections.\n4. The safeguards also include:\n   - (a) Measures designed to ensure no adverse effects on individuals' rights due to improper handling or loss of personal information.\n4. The safeguards also include:\n   - (a) Records documenting reasons for processing special categories before making decisions about them.\n\n#### Relevant Recitals\n- [Recital 66](/en/ai-act/recital-66)\n- [Recital 67](/en/ai-act/recital-67)\n- [Recital 68](/en/ai-act/recital-68)\n- [Recital 69](/en/ai-act/recital-69)\n- [Recital 70](/en/ai-act/recital-70)\n- [Recital 76](/en/ai-act/recital-76)\n- [Recital 115](/en/ai-act/recital-115)\n\nView the [official text](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:L_202401689). The text used in this tool is \"Artificial Intelligence Act.\"\n```",
  "fetched_at_utc": "2025-12-28T20:12:13Z",
  "sha256": "cafc1a71606b9a178b5b02c81edb96bb1e8df5656b2db3f3592fc473a80d380e",
  "meta": {
    "jina_status": 20000,
    "jina_code": 200,
    "description": null,
    "links": null,
    "images": null,
    "usage": {
      "tokens": 12000
    }
  }
}