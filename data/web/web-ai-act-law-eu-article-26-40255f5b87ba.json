{
  "doc_id": "web-ai-act-law-eu-article-26-40255f5b87ba",
  "source_type": "web",
  "source": "https://ai-act-law.eu/article/26/",
  "title": "Art. 26 AI Act - Obligations of deployers of high-risk AI systems - AI Act",
  "text": "```markdown\n# Art. 26 AI Act - Obligations of deployers of high-risk AI systems\n\n1. Deployers of high-risk AI systems shall take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems, pursuant to paragraphs 3 and 6.\n2. Deployers shall assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support.\n3. The obligations set out in paragraphs 1 and 2 are without prejudice to other deployer obligations under Union or national law and to the deployer's freedom to organise its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider.\n4. Without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over the input data, that deployer shall ensure that input data is relevant and sufficiently representative in view of the intended purpose of the high-risk AI system.\n5. **Deployers shall monitor the operation of the high-risk AI system on the basis of the instructions for use and, where relevant, inform providers in accordance with [Article 72](https://ai-act-law.eu/article/72/).**  \n   **Where deployers have reason to consider that the use of the high-risk AI system in accordance with the instructions may result in that AI system presenting a risk within the meaning of [Article 79(1)](https://ai-act-law.eu/article/79/), they shall, without undue delay, inform the provider or distributor and the relevant market surveillance authority, and shall suspend the use of that system.**\n   **Where deployers have identified a serious incident, they shall also immediately inform first the provider, and then the importer or distributor and the relevant market surveillance authorities of that incident.**\n   **If the deployer is not able to reach the provider, [Article 73](https://ai-act-law.eu/article/73/) shall apply mutatis mutandis.**\n   **This obligation shall not cover sensitive operational data of deployers of AI systems which are law enforcement authorities.**\n\n   For deployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law, the monitoring obligation set out in paragraph 1 shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to the relevant financial service law.\n\n6. Deployers of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system to the extent such logs are under their control, for a period appropriate to the intended purpose of the high-risk AI system, of at least six months, unless provided otherwise in applicable Union or national law, in particular in Union law on the protection of personal data.\n\n   Deployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain these logs as part of their documentation kept pursuant to any relevant Union financial service law.\n\n7. **Deployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain these logs as part of their documentation kept pursuant to any relevant Union financial service law.**\n\n8. Before putting into service or using a high-risk AI system at workstations (employers), deployers who are employers shall inform workers' representatives and affected workers that they will be subject to this high-risk AI system.\n\n   This information should be provided according to Union and national law and practice on information for workers' representatives.\n\n9. Deployers of high-risk AI systems that are public authorities (or Union institutions) shall comply with registration obligations referred to in Article 49.\n\n   When such deployers find that there has not yet been a registration record made through an EU database (referred to in Article 71), they shall not use this system but must notify both providers (as per Article 73) and importors/distributors about this situation.\n\n10. Where applicable, deployers of high-risk AI systems shall use information provided under Article 13 of this Regulation for compliance with their obligation to carry out a data protection impact assessment under Article 35 (in Regulation (EU) No 2016/679) or Article 27 (in Directive (EU) No 2016/680).\n\n11. Before putting into service or using a high-risk AI system at workplaces (financial institutions subject only if required by mandatory provisions concerning internal governance), employers must inform workers' representatives and affected workers about whether they will be subjected to this high-risk AI system.\n\n    This information should be provided according to Union and national law and practice on information for workers' representatives.\n\n12. Deployers must submit annual reports detailing how they operate their high-risk AI systems while ensuring compliance with all applicable regulations.\n\n    These reports can be aggregated across multiple deployments if needed.\n\n    Member States may introduce additional restrictions laws on uses involving post-remote biometric identification systems based on Union legislation.\n\n13. Deployers must provide users with access details so they can report issues arising from their usage via email addresses associated with user accounts registered with them.\n\n    Users must also receive regular updates about changes affecting their account security settings due to vulnerabilities discovered during periodic audits conducted by supervisory authorities.\n\n    Supervisory authorities must act promptly following breaches reported by users regarding their account security settings.\n\n### Suitable Recitals\n\n* ([91]) Definition of special responsibilities and obligations for deployers\n* ([92]) Duty of employers to inform and consult employees or their representatives\n* ([93]) Notification and information of persons by部署者\n* ([94]) Application of Directive (EU) No 2016/680 when using AI系统进行生物识别的法律程序\n* ([95]) 兼容性评估时使用高风险AI系统的后续远程生物识别技术\n\n[←Art. 25](https://ai-act-law.eu/article/25/) - Responsibilities along-the-AI-value-chain\n\n[Art. 27](https://ai-act-law.eu/article/27/) - Fundamental rights impact assessment for high-risk AI systems\n\n[AI Act Table Of Contents](https://ai-act-law.eu)\n```",
  "fetched_at_utc": "2026-02-09T21:34:41Z",
  "sha256": "40255f5b87ba56ebceddb5498cc7d6638e3d681cd97ae8db8346f8a92be51f33",
  "meta": {
    "jina_status": 20000,
    "jina_code": 200,
    "description": null,
    "links": null,
    "images": null,
    "usage": {
      "tokens": 12000
    }
  }
}