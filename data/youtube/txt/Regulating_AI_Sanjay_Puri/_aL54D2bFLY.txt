AI regulation and technology regulation
in general about the world is the US
innovates. Yeah. China copies and the EU
uh regulates. Regulates. Yeah. Now I
think that is way too simplistic. If you
train your algorithm on this data
without any further uh awareness, then
you will create a system that is uh
biased toward favoring white men and uh
not not favoring black women. Now if
there is one uh single principle that
could underly everything is something
like do no harm.
So whatever you do do no harm. Uh what
is true is that the nature of the jobs
that we're doing that will definitely
change. Yeah. Because if you think in
the in in in in the task that can be
automated if I have 100 tasks in my job
Yeah. and 25 can be automated. Of
course, I I I'm still doing 75% of my
tasks, but my job has changed.
Welcome to the Regulating AI podcast.
Join host Sanjay Pury as he explores the
dynamic and developing world of
artificial intelligence governance. Each
episode features deep dives with global
leaders at the forefront of regulating
AI responsibly, tackling the challenges
using AI can bring about head-on and
enabling balance without hindering
[Music]
innovation. Welcome to Regulating AI
podcast where we delve into the
intricate world of AI regulation aiming
to foster fair and equitable frameworks
across the globe. In today's episode, we
are honored to have Dr. Dr. Richard
Benjamins, a prominent figure in the
field of ethical AI and regulation. Dr.
Benjamins is the co-founder and CEO of
the Spanish Observatory for ethical and
social impacts of AI. And until
recently, he served as the chief
responsible AI officer at Telefonica.
With a wealth of experience and
expertise, Dr. Benjamin has played a
pivotal role in shaping Telefonica's
approach to the ethical use of
artificial intelligence. He's also
actively involved in various advisory
boards and expert groups advocating for
responsible AI governance and
regulation. Join us as we explore his
insights and perspectives on the
challenges and opportunities in AI
regulations. Richard, welcome to the
regulating AI podcast.
Thank you very much Sanjay. It's a
pleasure to be here. Wonderful. Uh
Richard, our audience is global. uh
audience is made up of lawmakers, policy
makers, think tanks, uh leaders of
industry, advocacy groups around the
world. Can you tell them about your
journey and how you came upon
responsible AI?
Uh like in like six or seven years ago,
I was working in a large insurance
company. I was the chief data officer
and head of innovation in data and I had
a small team working on looking at
discrimination and uh blackbox models.
Yeah, at that time still very in the
research phase but it intrigued me uh
and I thought it was indeed very
important with the rise of artificial
intelligence. So when I moved back to
Telefonica
u I kind of spearheaded that thinking
into Telefonica at that time um as a
company it was very vocal on using AI
and data across all of its operations.
So I thought it important not only think
about opportunities but also about
responsible opportunities and that's why
I this is how I came to responsible AI.
Wonderful. uh you were the uh chief
responsible AI officer at Telefonica.
Can you tell our audience some of the
key lessons you learned from that
experience, Richard?
Well, I think if you speak about
responsible AI, uh usually people think
only about avoiding negative impacts.
Yeah. So doing ethical AI but I think
responsible AI is as much uh about
creating positive impact using this
technology to solve important societal
and planetary problems which are all
reflected in the sustainable development
goals where we only have six years to go
yeah to achieve them.
Having said that, um I think responsible
AI in terms of ethical AI is much more
advanced than the use of uh AI for
social purposes simply because uh there
is regulation coming and there is a
business behind it. Whereas for the
let's say the AI for good part there is
currently less of a business model. It's
more a philanthropical thing. And then I
I have many many lessons in general
about how to apply responsible but I
can't tell them all at this moment
because that would occupy the whole
podcast. So we'll leave that for later.
Wonderful. Uh Richard, you have a a key
seat to watching how AI regulation is
happening around the world. You've seen
the EU AI act that has come about. uh
the Chinese have been working on uh
their uh regulatory framework. Uh even
uh Spain has been doing a lot of work in
there and what the US has been doing.
What are your thoughts uh on the current
state of AI regulation globally and what
are some of the challenges uh that you
see uh Richard for that? Yeah. So I
think the usual u statement about AI
regulation and technology regulation in
general about the world is the US
innovates. Yeah. China copies and the
and the EU no and the U and the EU uh
regulates. Regulates. Yeah. Now I think
that is way too simplistic. Yeah. To to
state the world. Uh it's not that the US
is not regulating. There's more and more
regulation coming up. Fines have been uh
um issued for large big tech. China is
actually uh where more AI regulation is
already in force. Yeah. Uh already since
2018 or 2019, there were regulations on
recommendation algorithms. And now with
generative AI, there are two
regulations. Yeah. They're not final,
but they are regulations to protect
citizens. Yeah. Of course the final goal
of all this whether it's human rights or
whether it's uh let's say state power
that's a different thing and then Europe
is of course regulating a
lot not only AI but many technologies
and many uses because human rights are
are very important and human rights in a
balanced way. Yeah. If you look at the
US doesn't mean human rights are not
important. Actually the biggest
difference uh one of the biggest
difference between US regulation and EU
regulation is that the fundamental right
of free speech in the US is kind of the
most important fundamental right whereas
in Europe uh free speech is very
important but if it's at the expense of
uh harming other people or more groups
then maybe it is you need to balance it.
Yeah. So it's it's a nuanced thing and
actually uh there is a great book uh
that speaks about digital empires where
there is a lot of detail about those
three ways of regulating technology and
this book states that actually the the
three different ways of regulating they
come closer and closer together. Yeah.
because the US is regulating more.
uh human uh the eur Europe is also
thinking about uh what can happen if
they regulate too too much and then
China is also regulating but the the the
difference is is is basically US and uh
and Europe are for fundamental rights
whereas China is more for the political
party right so uh let's just take the
example of telefonica it's a global
company right uh Richard most most
companies these days you know they don't
see any borders uh technology companies
etc. Now US is doing its own regulation
China uh EU etc. What are your thoughts
on
international
collaboration for AI regulation? Because
otherwise if I'm a company I have okay
in the US we even have some states that
are coming up with regulations. There is
going to be this crazy patchwork of uh
rules and regulations and the only
people that are going to win in this is
lawyers who are going to help you figure
out how to do work. What are your
thoughts on international collaboration
on this topic? Of course, so there are
two parts in this uh in this answer. One
is if you're a global company, of course
uh it is good to have a global yeah uh
AI governance uh policy or or internal
regulation that you apply across the
globe. Yeah. Because otherwise you have
to uh you have to invent the wheel many
different times. Mhm. Having said that,
of course, locally this global
regulation needs to be flexible to be
able to adapt to local regulations
because it it is it is the case. Yeah.
So the world is not one uh one common
thing. It is is very different. So the
approach we have in telephone is we have
a global approach. This is not only AI,
this is also privacy. It's also human
rights. It's also climate change. It's
also supply fair supply chain. There is
the global policy and then depending on
in local regulations there is
flexibility and adaptation but always
taking uh stock of the global approach
which is agreed as the standard for for
the company. Now having said that
uh uh if you look at the the the
different countries actually very
important to come together yeah as a as
a as a globe and have let's say strive
to have a harmonized regulation even
though today that seems impossible.
Yeah. But there are some um let's say
some some green leaves that we can show
up because if you look at for instance
the OECD is combining 42 or 43
countries. So it's not countries it's 43
across Europe and the world and if you
look at UNESCO's ethical AI
recommendation that
represents 194 countries. Yeah, that's
almost all of the world and that
includes China, Iran, Russia, the United
States. So let's say all the conflictive
uh right countries as well. Now this is
an international recommendations where
countries have said publicly we will
follow those rules. If they would follow
those rules all of them, we would have a
harmonized AI regulation. Yeah, of
course the the weak thing of this it's a
recommendation. So you can even say you
follow it but then you don't follow it
in practice and that's the challenge but
there are uh movements global movements
to bring things together. Yeah. And if
com if organizations like UNESCO or the
OECD they manage Yeah. to do it the
other way to bring countries together
and then slowly start high level slowly
go down and show that it actually
worked. There is hope. Yeah. And this
hope is very important because if you
look at the geopolitical situation we
are in and AI is also suffering from
that. This is actually a big problem.
Yeah. And all indic all all other
indications are that the world is
actually separating. Yeah. And instead
of having a global internet we may have
splinter nets where China has its own
internet, Russia has its own internet
and they're not even connected in
between. And AI is of course important
in that respect. what we what we what we
have to avoid that there are kind of
free havens where you can do bad things
with AI without any punishment.
Uh Richard you talked you were at
Telefonica and large you talked about
global companies uh do you think uh AI
ethics boards uh are an important
aspect? Did Telefonica have an AI ethics
board?
Um we do have an AI ethics uh committee.
Committee. Okay. Or expert group. Yeah.
Depending on how you want to call it and
that is a u let's say a group of
multidisiplinary experts who have
knowledge about different perspectives
on AI. So it has privacy, it has legal,
it has AI and data, it had ESG, yeah,
environment sustainable governance. Um,
and then on demand it can have other uh
areas as well depending on what the
issue are. Now our ethics committee is
an internal so it's not an external
committee with high level. It's really
hands-on knowing the about the products
that the company have and the systems we
have and and then it this takes
discussions uh when problems arrive. It
is not a uh permissionbased committee.
So it's not we want to do this then we
bring it to the committee and they say
yes you can do it or you cannot. Yeah,
it is you want to do something uh
somebody in organizations due to the
governance model detect that there might
be a risk in certain areas then there is
a discussion if the discussion is not
clear this goes to the ethics committee
and there this debate is uh is held is
is u is held and a recommendation is
given. Yeah. Now if that recommendation
is taken up then there is everything
continues. If there is however a
disagreement between the ethics
committee and the business then there is
an escalation to the higher level where
business and ethics come together and an
explicit decision is taken. So the
importance is not so much the decision
that is taken but that it's an explicit
decision with the pros and the cons and
it's a documented decision in case
something something goes wrong. So that
is how we how we are currently
implementing the the AI governments and
I'm speaking in the past. Yeah. past.
Yeah, of course. But I want you to talk
in the future, Richard, just to follow
up on that point. Now, you are at a
think tank, a very prestigious think
tank. Uh when global companies are
implementing AI, some of them have
started exploring this whole concept of
an ethics board or like you called it an
ethics committee. What what would your
recommendation be to our audience?
people who are uh you know leaders at
large global companies they listen in
and listening into you. Uh should there
be one an ethics com uh board and should
it be right in Telefonica you had an
internal one your recommendation now in
this think tank should it be made up of
external and what kind of uh role should
they play this is for CEOs listening in
for to you right now okay so here I give
you my very personal opinion yeah yeah
we want your personal opinion yes so
first of all I think there is a
difference between big tech which are
these huge companies which there are
about 10 in the world and there are
large multinational companies like
Telefonica or like pharmaceutical
companies or like retail companies. So
this is not the same uh these are not
the same companies what we've seen in
the
past AI ethics committees where these
big tech yeah they appoint highlevel
reputated external people to oversee
etc. Now uh I don't think that any of
those ethics companies worked out
because most of them were dismantled in
within a few months. Uh but if you are
an an MNC multinational company an
external ethics
board doesn't make so much sense apart
from having high level advice. Yeah.
Mhm. The problem is not so much for
those companies uh that people get high
level advice. The problem is how to make
it happen, how to implement this advice.
Yeah. And there external people who are
very far away from the day-to-day
business, they can't really help. So you
have to translate that. So my
recommendation is that if you are a big
tech company and you have uh billions of
customers Yeah. then maybe you can have
an external advisory or board or an
ethics board that oversees things and
can also uh make statements. Yeah. In a
free let's say it has to be a
uh a free a free space. Yeah. where they
can freely speak without any uh ties to
business business objectives. So of
course you have to avoid the ethics
washing. Yeah. That just by naming a
committee you think well I'm done and
now everybody thinks we are ethics but
that's actually not the case. Yeah. If
you are a large company, not as big as
the big tech, but still a large company,
then I think an today at the state of
play today, it makes much more sense to
have an internal ethics committee where
are people who know the business, who
know the projects, who know the products
because they can be much uh more
explicit about what has to be done and
what should not be done. Yeah. Then one
other recommendation I just mentioned
that before. Yeah. you have a problem
based approach of an ethics committee or
a permissionbased approach. So again I
think if you are a big tech where you
have billions of customers then maybe a
permission based uh approach would not
be so uh would not be so bad because
there are not so many projects products
maybe there are 10 or 20 and they all
have a huge impact in society and on
people so it's better to find out hey if
we launch this we I directly reach
billions of people so we better be very
sure that this is what we want to do.
Yeah. Now if you are a large company uh
then you reach maybe a few millions
still a lot of people but it's a
different uh it's a different ballgame.
Yeah. And then a permissionbased
uh approach actually will not work.
Yeah. Big tech is delivering AI to
society and to customers. Large
companies use AI internally internally
to optimize their business or to create
new business. Now for instance we have
like more than 600 AI systems running
every day globally. Now we can't ask
permission 650 times. I mean that
doesn't work. If there are problems we
need uh we need expert people to think
about it. But also think about I mean if
you speak about AI and risk and
responsibility governance we should not
forget that the large majority of AI
systems have no risk at all. Yeah. There
are many many systems that just do
simple things like filtering spam or
maybe recognizing my my face here
within within this browser. Uh and there
are a few that have more high risk.
Yeah, those higher risk ones there it's
very important to be very uh alert on
what what what you are doing. So it's
not one sizefits-all. Yeah, you have to
do a risk based approach. So for our
audience, I think Richard is saying if
you're a big tech company, uh have an
external uh ethics board. If you're
large company, have it internal. If
you're big tech, uh it should be maybe
probably be permission based. If you're
a large company, it's a problem based
approach. Right, Richard? Just to
summarize summarizing at a very high
level of course there are of course many
nuances where it is different of course
but that from with my experience would
make sense. Wonderful Richard uh let's
talk a little bit about u some of the
potential risks of AI uh how do we
address them such as bias privacy
concerns etc. What are your thoughts? Uh
how do we address some of these things
and what kind of ethical principle
should guide the development and
deployment of AI systems?
Well, we speak about two specific uh
risks of AI which is privacy and bias.
Yeah. And bias is a problem because of
potential discrimination, right?
Um now actually those are two risks but
those are two principles. So many
companies have in their AI principles
should not be biased. Yeah. And it
should not lead to discrimination and
also it should respect privacy. Yeah. So
in that sense it's quite
straightforward. Yeah. What is the what
is the principle? Now how do you act on
those principles? I think first I'll
start with bias with because that's
something typical of artificial
artificial intelligence in the sense
that artificial intelligence learns from
data. Yeah. can be structured data or a
generative AI generative AI it can be
unstructured data but this data may have
bias in it. Yeah. So there are many uh
things that you can do to uh mitigate
bias. Yeah. First of all, one thing uh
if your uh if some of the groups that
you have in your uh in your data set are
under represented like uh just a typical
example is uh black women versus white
men. uh if you if you train the data if
you train your algorithm on this data
without any further uh awareness then
you will create a system that is uh
biased toward favoring white men and uh
not not favoring black women. Yeah,
that's the typical thing that we've seen
in face facial
recognition where all systems are
usually trained on a database where 80%
is Caucasian people. Yeah. And therefore
it works much better for Caucasian
people than for for other people. Now we
know that we've learned that. So we can
do something about it. We can try to
find out hey is our data representative
what we use for training for the target
audience. Yeah. If we train uh school
abandonment uh school levers in one
certain neighborhood maybe a very rich
neighborhood of a city and then we apply
the model to the whole city. Of course
it doesn't work because it was not
representative. Yeah, there is also an
issue about whether the false positives
and false negatives. So many cases,
yeah, if you do diagnosis, you have
false positives. So you think somebody
is ill but he or she is not or you have
a false negative. So you let go a very
ill person. Yeah, those uh those those
are error rates of AI. We should not
forget that there is no AI system that
is 100% accurate. There is always an
error. And it's very important that the
the proportion of false positive rates
and false negative rate is similar or
equal across different vulnerable
groups. Yeah. For minorities, for
children, for elderly people, for women,
for so and that's something you can
check. Yeah. You can just look at your
data as long as you have the data of
course uh you can check is uh is this uh
is this true or not? Yeah. So that's
something that you can do against bias.
Now there is much more bias and actually
we are very biased in our personal
opinions. Yeah. So there's also this
bias in terms of if you have a a white
white male team developing all the AI in
the world. Of course maybe other areas
of the planet are not represented
equally. Yeah. We have the same with
generative AI in terms of large language
models which are dominantly trained on
English representing the the US culture.
Yeah. Now of course there's much more
content on the internet in English and
also much from those areas. So what this
model learns is a statistical
representation of co-occurrences of
words and of course the nuances of the
US society versus uh the nuances of a
country like uh Estonia. Yeah. Uh I mean
there is also a bias in that sense that
we're becoming one equal part of of
similar things. So bias is is is
happening in many cases. Yeah. H having
also uh representative teams uh is of
course nice. Yeah. But if you're living
in a certain area, it's going to be very
hard to have a representation in all
respect in gender, in terms of color, in
terms of uh global south of the north,
in terms of uh political preferences.
That's very difficult, right? But you
have to strive towards it with within
your means. So those are all things you
can do uh regarding bias. you cannot
avoid it but you can be aware of it and
you can mitigate a lot of issues and
sometimes you can detect things that you
think okay this is too uh this is too
bad I don't want to continue with this
kind of bias now the other thing you
mentioned was privacy privacy in Europe
is already regulated since many years in
the the general data protection
regulation
uh which is about personal data yeah so
as when you use personal data in AI then
the GDPR applies in Europe and it's also
to some extent extended to other other
countries and other regions. So that to
some extent has been taken care of u
from a this regulation perspective.
Yeah, of course there are still many
issues uh I mean we've seen that with uh
uh cookie uh acceptance in websites.
Yeah, they hide they put the accept very
high and then they hide the reject.
That's actually now in in that happened
also in Europe. Now there is a new
interpretation of the regulation that
actually you have to put accept reject
at the same level and the same color.
Yeah. So you cannot nudge people in
accepting what have companies done and
especially newspapers I'm sure you've
seen that if you now read a free
newspaper it's either accept or pay.
Yeah. So and then and then everybody
accept. So this is compliant with the
law. Yeah. But you can ask whether this
is ethical or not. Yeah. Anyway, it's
it's very being very transparent. Yeah.
Either we use your data to to earn money
or you pay us then we also earn money.
So to some extent it it puts the
discussion where it is and it make
people aware that hey if I'm not paying
then actually yeah they have to do
something with my information otherwise
uh I would have to pay also.
So those is those are these two um two
um risks that of of the use of AI. Yeah.
So Richard uh you made some very uh
important points. If I may follow up on
a couple of them. One is you brought up
this point about Estonia and how uh the
in you know the AI is driven the
internet is kind of driven by uh English
and maybe American culture. Uh there is
the fear of small countries who have
large histories. You talked about
Estonia. There are countries in Africa
in Asia etc. that AI should not just
become a Silicon Valley dominated. So
there are two aspects to it. There is a
cultural aspect and then there is this
monopoly aspect which you kind of
touched on these 10 big companies. So I
want your perspective. I mean you are in
Spain right now again a country with an
incredible history. uh is there a danger
of culture or language getting washed
out uh with the current framework in AI
and what can we do to avoid that and the
second related question to that is a
monopoly that currently exists so to
speak within social media and tech that
gets extended to AI. So if you can just
address those two questions uh Richard.
No regard regarding culture. I mean
luckily Spanish is spoken by 600 million
people. Yes. And so maybe not the best
example. Sorry. And in at least 23
countries and 21 countries as a modern
language, right? So actually Spanish is
the second or the third language in the
internet. So is reasonably well
represented compared to smaller
countries and even within Yeah. because
there are so many more languages than
there are countries and because of
multiple languages. So um if you look in
let's say in the developed world in the
high inome countries then even though
there is a small uh small group those
people are usually very active to
maintain their culture. Now we have in
we have the the best country in Spain
and uh and Cataloonia. Yeah. They have
their own language. They have their own
culture and there are lots of
research research groups, universities,
startups that actually work very hard to
bring that language to the forefront.
Yeah. Mhm. So if there are resources and
interest then that there is of course a
risk because you get the the the large
cultures you get out of the box doing
nothing and the small cultures you have
to hard you have to work very hard. But
if there are resources uh and people
work very hard to make that happen. Now
if you go to low income and middle inome
countries that's completely different
because there's no skills, no profiles,
no resources. So those cultures they run
the risk not being included. Yeah. And
then of course those big companies they
can actually work on they can do an
agreement with a certain country and
said okay we'll take care of your
culture as well. So there are all kinds
of possibilities.
uh but in the end uh the current uh
let's say current generation of AI is
driven by profits. It's driven by
businesses. So why are people investing
so much in this? Because they think
there's lots of profits and money to
make. Yeah. So the rest is kind of they
do it for reputation or they there is
somebody in the company who feels that
they have to do responsibility
uh but it's the best effort. Yeah.
So there definitely a risk that smaller
cultures will disappear especially with
these large language models and I think
we have to stand up against that and to
support other cultures in also being
able to join and I also foresee that in
the future there will be maybe a few of
those huge large language models like we
know from uh the big players but there
will be many many smaller models who
consume much less energy who are very
more much more specific and you can
represent a certain language. Yeah. So I
think there will also be a trend as
usually first things become bigger
become bigger. Yeah. And then they
become smaller and more targeted for
certain areas and this will happen here
as well. Now on the monopoly side I
think that that that is a definitely a
danger. Yeah. And uh it's not only
regarding culture uh but it's actually
uh who is taking decisions that have
that are impacting the world at large.
Yeah. Um and then there is also the the
the issue that this technology uh
creates a huge amount of wealth in the
world. They say that AI and generative
AI together maybe can increase uh 20%
the global GDP which is a huge amount of
of of economic wealth but then how this
wealth is distributed is very much
concentrated in a few countries and in a
few companies. Yeah. So there is a um an
enormous concentration of wealth which
is actually increasing inequalities
across the world and um that is still
going on. Yeah. So that's nobody is
doing anything to stop maybe the Europe
European European Union with some of its
regulations like the data the digital
services act or the digital market act
they are doing something and now in the
US is also questioning some of the
monopolies of the big tax but uh those
are still small things compared to the
enormous wealth very few actors are
gathering.
So uh Richard in your view uh should
what should be some basic ethical
principles that guide the development of
uh AI? What are in your view what are
the basic foundational principles of AI
ethics? If you if you look at the AI
principles that are currently in the
world in the OCD, European Union, even
in the United States, uh in the UNESCO,
they're all about uh it should be fair.
Yeah. No discrimination, no bi should be
explainable, understandable, transparent
uh when it has impact on people's lives.
Um it should have the right uh human
oversight. So again, if it's in the
medical field where it detects serious
diseases, of course, there has to be a
person in the loop, but if it's a
recommendation of a video, maybe of
course you can't put a person to to to
tell to approve every single
recommendation across the globe. So um
it should be accountability,
uh privacy, security. So there is a
number of 8 10 principles that
everybody more or less agrees about. Now
whether they name it one thing or
another but is a common thinking uh and
and I think the only thing they have in
common is that it's about human rights.
Yeah. Because ethics are very different
across the globe depending on cultures.
But hey, almost almost all countries
signed up for human rights even not even
though not all countries comply with
human rights. It's the same like the
UNESCO AI recommendation. Yeah. Mhm. 194
countries signed up but not all are
executing on it. Now if there is one uh
single principle that could underly
everything is something like do no harm.
So whatever you do do no harm. And uh of
course
there that is a very simple one because
sometimes
uh harm to you. Yeah. Because if you
take a decision then maybe you harm some
people and you don't harm other and if
you don't take the decision it's the
other way around. So maybe it's
impossible not to do harm but as a
guideline it's very important to do no
harm. And then to act to act on that
principle means that you should
constantly think about if I'm building
this AI system for this particular use
is there any chance that it could do
harm in a in a way that I have not
foreseen. So think hard about that in
the process by design not once you're
finished but also from the beginning who
could be harmed who could I harm with
this system. If you do that
systematically actually I think there
would be uh create a lot of system less
systems with uh with potential
problems. Uh Richard uh what are your
thoughts on how do we futureproof uh AI
regulation whether it's an EU AI act or
the US uh coming up with regulation or
other countries given the fast pace of
technological change that's happening. I
mean you know I have never seen such
change happen so fast you know large
language models multimode agents uh you
have Sora we're talking about AGI how do
we make sure that these regulations can
withstand the changes that are coming in
there how do we future proof some of
these
well that's a tough question yeah I have
no answer I have definitely no answer to
that maybe we could build a system that
tries to help us an AI system that try
to help with future proof regulation.
Um anyway, so what you have to regulate
is uh a definition of AI that is future
proof. Yeah. So you cannot definitely
say an AI system is a system that uses
generative AI or large language models
of machine learning because that's today
AI and maybe tomorrow there is a new
thing. you should abstract from that and
I think that has the whole world has
learned from that uh uh in the in the in
the recent years. Yeah. So definitions
are now much more in terms of what they
are cap capable of rather than in terms
of uh what they do. And then if you um
if you do a risk based approach and you
define some of the high-risk
applications then you have to be open to
future new high-risk applications or
maybe high-risk applications of today
are becoming less risky in the future.
So I think that is one way of trying to
be flexible. Yeah. adaptive. Of course,
you can't wait to have the perfect
regulation and then you start regulating
because the world moves on and then you
will have never a regulation. So even
though all regulations have problems and
they are not perfect, um maybe it's
better to have something uh that sets
the direction rather than have nothing.
Yeah. Uh that makes uh a lot of sense.
Richard, you were at a global company in
the chief responsible uh AI position
when you now you've stepped back into um
a think tank. When you look at large
companies implementation of AI, you
know, every today when when I we have
guests come in, every large company is
under pressure from their board in terms
of what are you doing about AI? We need
you to do uh AI, it's their stock price,
blah blah blah.
What are your recommendations from a
personal standpoint? Should they have a
chief AI responsible officer, a chief AI
officer? Uh or a lot of these companies
just are saying to their CIO, chief uh
information officer, now you take on
this role or chief technology officer.
If you were to just look at a big
picture personal view uh
organizationally and where should these
people be reporting
into? Any thoughts on that?
Yeah, I actually I wrote a book on how
to become a datadriven company and one
of the chapters is exactly about this.
Yeah. Well, then you're the right person
to answer this question because our
audience is CEOs of comp large companies
listening.
So, uh in my experience, if you are
starting as a company to work on AI, it
doesn't matter where it sits as long as
you have an enthusiastic and
knowledgeable person. Yeah. Because you
have to get the ball rolling. If you
think oh we have to give this to the CIO
because then it has the the message that
this is important and the the CIO or the
CTO have no knowledge or have no
enthusiasm about it. I mean that is not
so important. It should work in the
beginning. Now if you start moving and
you become a bit more mature, you have
done some things of course if this one
sits in marketing then all the
applications are for marketing. If it
sits in uh in in the business area then
it's all for a certain business area. Um
of course very important that a person
in this role has a transversal reach. So
typical are the chief operating officer
uh or the uh chief digital officer or
chief transformation officer. This these
are let's say temporary roles. Yeah. To
become more digital and create value
from new technology. uh and that doesn't
apply only to B2C or B2B. It applies
across. So if you want to scale, you
need to be an you need to be in a cross
area. Yeah. Can also be the chief data
officer. Many chief data officers uh
take the uh artificial intelligence
responsibility on because if you have
great data but nothing is happening with
the data, it's only half of the half of
the picture. Yeah.
So Richard uh most uh think tanks uh
consulting firms McKenzie IMF and others
are predicting that we are headed for
major transformation in our workforce.
They're saying it could be 30% 60% 50%
changes that are coming in. um you uh
had a view internally and externally.
Now is major transformation coming and
if so what can we do about it and what
is the role of regulation or government
in something like this for our audience
who are concerned about uh what's going
to happen to my job. Yeah. Okay. So I
think here you can find studies that go
in all all different kinds of
directions. So there is no this is not a
science. Yeah. Uh and um M so many
studies what they do is they they look
for the the tasks that are typical of a
uh job. Yeah. And then they look for
each of those tasks how much can it be
automated with AI or generative AI and
that gives that gives those numbers 26%
or 30% depending on the study. Yeah. Now
there are the studies that say if you
look at the unemployment rate in history
it has always gone down. Yeah. And now
with the baby boomers retiring actually
there will be a lack of people to do the
job. So either we have AI or we cannot
even maintain our productivity. So it's
a completely opposite one. Yeah. The one
is AI is taking our jobs and the other
one is we have not enough people to do
our jobs. Yeah. Mhm. Completely the
opposite and very respective people. Um
they they argue for for both. Yeah. What
is I think this is my opinion. Yes. So
I'm not I don't know whether I'm I'm
right or not. Uh what is true is that
the nature of the jobs that we are doing
that will definitely change. Yeah.
Because if you think in the in in in in
the task that can be automated if I have
100 tasks in my job. Yeah. And 25 can be
automated. Of course I I I'm still doing
75% of my tasks but my job has changed.
Yeah. And if I automate 60% then my job
is changing even more. It's not that the
job as such disappears. Yeah.
Um, of course, education is very
important from a governmental
perspective so that people Yeah. are not
left behind. The ability to upskill and
the capability. Now, of course, it is
true that this revolution is so going so
fast that not everybody will be able to
be upskilled. Yeah. Because in the past
of revolutions, they took many years.
So, generations had time to adapt and
this is happening in just a few years.
So you have to do have to be very very
fast and that will create problems.
Yeah. Now in the in the in relation to
if you look at the the the risk of uh
losing our jobs because AI increases
productivity in order to maintain or
grow our productiv productivity as
countries the GDP needs to grow. We can
do that with less people uh because we
automate. So we have a group of people
that work and that contribute and we
have a group of people that doesn't work
not because they cannot they do not want
they should not they simp it's not
needed that they do it. Yeah. So what do
you do with those these people? How
where do they get their income from?
Yeah. How does it uh will it bankrupt uh
the financial situation of a state
because there is less income tax because
less people are working but they have to
uh maintain more people who are not
working. So who is paying for that? Do
we need a robo tax? Yeah. Tax on on AI
systems or do we need a universal basic
income? So these are all very very high
level and and and broad discussions
where we are only scratching the surface
of how serious this might be or maybe
will not be serious at all.
So a lot of things to think about as you
said a robo tax. uh you talked about
upskilling, you talked about maybe
um you know universal basic income, all
those things will need to be considered
in the future. Uh Richard, finally what
advice would you give to policy makers
uh researchers and uh you know people
who are listening in regarding the
responsible development deployment of AI
technologies in terms of uh putting
together regulations uh to our audience.
Well, I think policy
makers should reach out to to to have a
have a um a broad understanding of the
technology a bit uh a bit more than the
typical I think they should read books
uh about AI about the potential impacts
and have some understanding of how it
works internally in order to be for
themselves judge whether uh what is
being said uh makes sense or is not
making sense. Yeah, because there are so
many things in the press what AI can do
and what AI cannot do. And actually AI
is not doing anything. It's the people
behind or the organizations behind that
makes AI do something. Yeah. So I think
that that is very important and and and
and researchers they should keep on
researching. Yeah. There are so many
open issues uh how to uh work on uh
transparent
AI large language models. Generative is
completely blackbox. Yeah. nobody
understands how it works and how you can
explain how you can trust it. Um, and so
building tools that help find bias and
correct bias. So there's so much work to
do for researchers as well in order to
have a let's say a a good use and a
responsible use of artificial
intelligence.
Wonderful. That's uh great insight.
Reach out, research, and find out what's
behind the black box. As we wrap up
today's episode of Regulating AI, we
extend our heartfelt gratitude to you,
Dr. Richard Benjamins, for sharing your
invaluable insights and expertise with
us. Thank you to our guests for tuning
in and until next time, stay curious and
keep advocating for a better AI future.
Thank you so much, Richard, for being on
our podcast. It's a It has been a
pleasure, Sanjay, and uh stay in touch.
Thank you.
[Music]
