Generative AI just kind of was a step
change in the possibilities that it
could bring to the world in solving
problems, complex problems, and offering
a multitude of optionality in creating
solutions. This is an iterative
technology, right? It's one that's
continually changing and learning. And
regulation, as a lawyer, I can tell you,
is kind of static, right? So here we
have a mechanism in the law or in
regulation that's somewhat static trying
to regulate a technology that is going
to be everchanging. It's not equal,
right? It does not reflect all of
society across the world equally. It's
slanted toward the wealthy. That's who's
used technology. It's slanted toward the
western world. It's slanted toward I
hate to say it, white right men.
Welcome to the Regulating AI podcast.
Join host Sanjay Puri as he explores the
dynamic and developing world of
artificial intelligence governance. Each
episode features deep dives with global
leaders at the forefront of regulating
AI responsibly, tackling the challenges
using AI can bring about head-on and
enabling balance without hindering
innovation.
Welcome to regulating AI the podcast
that brings global voices together to
discuss fair and responsible AI
regulation. I'm your host Sanjay Puri
and today we are thrilled to have Joanne
Stoner with us. Joanne is an executive
vice president and a fellow of data and
AI at Mastercard. She's a global data
expert and strategist with extensive
experience in data governance, privacy
and ethical AI practices. Her background
spans financial services, technology and
academia making her uniquely qualified
to discuss the complex landscape of AI
regulation. Welcome to the show Joanne.
Thank you Sanjay. It's a pleasure to be
here. Excellent. Joanne, uh we have a
global audience uh policy makers, think
tanks, uh uh technology leaders uh
around the world. Can you share for them
uh your journey as to how you became uh
a fellow of data and AI at Mastercard
and what does that role actually mean? I
get that question a lot. So uh thank you
for for letting me answer that. Uh so
I've had a long career um in both data
and privacy and as well as in uh more
recently artificial intelligence and uh
so I've had a journey at Mastercard
first as their first chief privacy
officer and then as their first chief
data officer and uh a couple last year
um after we spent a full year on
generative AI um with a lot of
conversations with our board our senior
leaders um there was a recognition for
both me as the chief data officer and uh
Ron Green who is our uh chief uh
information security officer that both
of us have played a role um in those
positions being engaged external to the
organization to understand both the
risks of our respective areas as well as
the opportunities and innovations in the
space and given that generative AI is a
real um pivot point uh technologically
both for risk and innovation. uh there
was a decision made that uh a new kind
of expert would be created for the firm
and we were designated to be Mastercard
fellows and the idea behind the
fellowship is that it frees us up from
uh kind of day in and dayout operational
responsibilities to kind of um really
have us focus on our external roles as
well as u free us up to take on
responsibilities with external
organizations
um including the world economic forum,
the USCIB which is the international
chamber of commerce and other um
organizations of uh companies and
consortiums where we can lead um thought
leadership projects on for me data data
policy artificial intelligence both on
the innovation regulation and risk space
so that we can really push and lead on
practices related to innovation and
regulation policy and risk. So that's
what I've been doing. Um, again, I was
the first privacy officer, the first uh,
uh, data officer, and now I'm one of the
first fellows. And so, we're defining it
a little bit as we go. But what I'm
finding is that I'm at the leading edge
of many of the conversations that we're
all having about this new technology and
what it means for all of us. And I guess
the main thrust for us is that we handle
this new technology responsibly for our
customers, our card holders as well as
our employees and um our banks, our
merchants around the globe. Oh, that's
uh fantastic. Uh so external facing
working with strategic partners uh is
what the role entails and and then of
course bringing all that information
back to Mastercard so that we can
continue to design and lead in our
practices as we continue to change and
grow as a firm. Yeah. So, uh Joanne, you
obviously uh you know have had multiple
roles and now you are engaged with some
key strategic uh organizations. So when
you look across in your view what are
the most pressing challenges in uh in
terms of regulating AI while also making
sure innovation flourishes so that you
know we get the benefits that the
transformative nature of AI can provide
whether it's in healthcare and education
and other things. Yeah. No, I think that
you know this is um I think everybody's
recognized that um while data analytics
and artificial intelligence have
continued to really change um that you
know our world from a digital
perspective, generative AI just kind of
was a step change in the possibilities
that it could bring to the world in
solving problems, complex problems and
offering a multitude of optionality in
creating solutions. But it also has some
risks in the amount of data, the amount
of resources that it consumes and the
amount of change that's it's engendering
um in society and as a result also the
amount of data that it consumes and how
it uses that data and how it then
creates conclusions and then uses those
conclusions to keep generating
information and keep learning. And so I
think that from a regulatory
perspective, we're seeing global
regulators and governments wanting to
make sure that this time we protect our
citizens, right, our people from any
kind of unintended consequences or
unintended harm. Now, of course, that's
a little challenging when this is an
iterative technology, right? It's one
that's continually changing and learning
and regulation, as a lawyer, I can tell
you is kind of static, right? So here we
have a mechanism in the law or in
regulation that's somewhat static trying
to regulate a technology that is going
to be everchanging. So I think right
there we have a tension. So I think
lawmakers are trying to get ahead of a
technology that's going to be changing
while industry is also experimenting and
trying to figure out how it fits into
their business models. And we also have
a citizenry that is also trying to
learn. So, it's kind of a perfect storm
for we're not going to get any of this
right. But I do think that everybody for
the first time is trying to figure it
out together, which is a first of all a
very important step in the right
direction. Right. What I think most
organizations um including regulators,
governments, commercial enterprises,
academic institutions are trying to do
is create frameworks of thought that
will be adaptable as we continue to see
the change that's going to be engendered
by this technology of generative AI. And
so making us all accountable in the
development process, in the outcomes, in
examining the quality of the data that's
being consumed, recognizing that all the
data that's been generated up until this
point by
civilization was not created for this
use. It's not equal, right? It does not
reflect all of society across the world
equally. It's slanted toward the
wealthy. That's who's used technology.
It's slanted toward the western world.
It's slanted toward, I hate to say it,
white, right, men in general. But
increasingly, we recognize those flat
sides and we can adjust for that. So, we
can make things better, but at the same
time, we recognize that we have other
issues that we still haven't solved on.
Everybody doesn't have electricity,
everybody doesn't have running water.
So, I think we in recognizing some of
this, we can also deploy the technology
in a more responsible way. I think
that's what regulators want. I think
that's what most organizations want. But
how do we do that in a way that really
helps and makes sense in a thoughtful
way even as everything is changing all
at the same time? Wow. Uh there's a lot
to unpack and lot to follow on those
different threads. So let me uh at least
I have three different uh questions
there. But let me start uh with one. You
said you know we need to come up uh with
a framework. The technology Joan as you
said is moving very very fast which it
is. You know we started with large
language models. We have now uh
multimodal now we're talking about
agents etc. and regulations and
lawmakers and we've had many of them on
our show and their challenge is how do
we keep pace? Um what are your
suggestions? uh
because you know law law making takes a
long time you know I'm here in
Washington DC or in Brussels etc. How uh
can it be an evolutionary law or as one
of the members of Congress said that we
are only going to be doing it
incrementally. So any suggestions uh and
that's my first question from what you
talked about. Sure. So I think we are
getting better at thinking through how
to navigate this landscape as lawmakers,
right? So I'll put on my legal hat first
and say that, you know, we have things
like the NIST standards that are being,
you know, um we have the first draft of
those. I think that even the EUAI act
tried to do it from the standpoint of
harms that could be created rather than
trying to regulate technology today
that's going to change tomorrow. And I
think that those are the right
approaches. I also think that there's a
lot of discussion around do we create a
centralized AI regulation or do we allow
for AI in
context for example let it be regulated
by financial regulators as it's applied
so that we understand the harms that AI
can cause in financial services in
health care in communications right in
manufacturing in retail so that we can
actually understand it in the context in
which this tool is being applied. I s I
suspect we're going to wind up in a
place where there will be some general
standards of understanding harms that
can be that need to be defined across
sectors. And then within each sector, we
also will understand how this tool can
amplify harms both intentionally and
unintentionally. and we will need
methodologies to pro protect citizens
and um of of countries and individuals
to make sure that we actually handle
that. I suspect that's going to be the
methodology that we're going to get to.
That's where I think we'll get to. I
think for regulators and legislators
right now, I do agree it's going to be
incremental. They're struggling right
now with what's the baseline law, what's
the baseline requirement. And for the
first time they also have to try to do
this across an ecosystem of kind of the
large language model creators right we
talk about the model creators and
consumers we have a whole new right
developers and deployers we have a whole
new language being created of what are
the respective responsibilities right of
someone who's just creating a model
versus someone who's deploying a model
for a specific purpose and context and
what are the related responsibilities I
I think all of these discussions are
super important so that everybody
understands the responsibilities but
also works together to prevent the harms
that are being identified. And I do
think eventually we will align on the
harms that the harms definitions will
begin to be more unified both nationally
in the United States but then also
globally because I think that's going to
be the place where we're going to agree
on what is harmful to an individual or
to a group in society.
So you're saying basically the way the
EU AI act has you know identified the
levels of harm etc. Uh but you also were
saying about industry specific what
would your recommendation be? Should we
have industry specific regulation for AI
or a comprehensive AI regulation or some
folks say uh regulate uh the outcome not
the technology uh if you because some
lawmakers are listening lawmakers are
listening to you right now so Joan what
would your recommendation so I guess my
recommendation is that it's going to
ultimately be a hybrid approach right
that I don't recommend one law I think
one law I understand the EU act and I
think the EU act first of all it started
before generative AI right and then they
tried to retrofit that bill so that it
includes generative AI and I think a
harms-based approach a risk based
approach which is what it ultimately
that law is is a good idea but even the
EU right then said that they were going
to hand off that act to their sectors to
define that further which I think is
exactly what I'm I'm in agreement about
that you cannot have risks without
context, right? You you really need to
define risks within a given context
because even the risk levels that are in
the act need greater clarity and
consistency in a in a given sector or
industry. So I think that is actually
almost what Europe is going to wind up
doing and I suspect we will do that
here. The question is what are the
common definitions right do we need and
it may be around the harms and then
actually though in context I think the
different stakes are very different in
healthcare and financial services it's
why those two sectors are highly
regulated in the United States today
versus other sectors but I again the
consumer products has regulation in the
United States transportation has
regulation agriculture has regulation
and AI is going to be deployed in those
sectors. I suspect we will have that
incorporated into the regulations in
those sectors eventually as the use is
understood in those industries.
No, so that's a an excellent point. One
of the points that you made Joan um uh
obviously is about data equity. you
started talking about how the data
currently you know coming back from a
long period of time and you've done a
lot of work in that
um talk a little bit about your work in
data equity and what what do you mean
when you say data equity and how does it
relate uh to that for our listeners who
don't know what data equity is. So
you're right, data equity is a concept
that hasn't really been fully defined,
but it goes to this notion that AI and
generative AI may not create even
outcomes for everybody. We talk about
bias and hallucinations in AI because we
talk about how we do not get even
outputs or outcomes from every AI
activity. And part of that is because of
several factors. I talked about how the
data that we're using to learn from is
not reflective of all members of society
and it's partly just because of how the
data has been created up until this
time. And so if we want to create models
for all parts of society, we need data
that reflects those those members of
society. But we also need to make sure
that we're designing algorithmic
processes using generative AI or
traditional AI that actually are
impactful for the communities we are
trying to design a solution for. Whether
that's a financial services product,
whether that's a transportation product,
a communications product, etc. And so
I've been working with the World
Economic Forum. We've had a council that
they've uh generously sponsored that
consists of data experts as well as
civil society experts and social
scientists to understand what are the
impacts on different communities that
aren't always reflected in the data.
When the data doesn't always reflect a
community and therefore you get
unintended outcomes, right? ones that
don't reflect that community and
therefore the conclusions drawn are
wrong. So data equity is this idea that
a series of actions may be necessary as
you are doing a data analysis as you're
creating a new data solution or product.
You need to think about what are the
outcomes you're trying to achieve or a
community or a set of individuals that
may not be reflected in the data or may
not even have been reflected in how
you're thinking about the problem. And
what we're doing is we're offering up a
toolkit of a series of action steps that
organizations can take as they create
new products, new solutions to make sure
they're designing for the communities
that they're trying to impact. And so it
may require creating synthetic data. It
may require collecting data that they do
not have. It may require thinking about
that the data may be slanted naturally
just by how it's been created since the
beginning of time. That the data may be
slanted to the wrong kind of geographic
location. Maybe it's more male than
female and you're creating a female uh
product. Those type of issues need to be
part of our product and thinking and
product development process early on and
be looked at as we look at the output of
all these models to adjust the outcomes
to make sure that we actually are
thinking it through from the beginning
to the end of the process. And so that's
what data equity means. It really means
a series of adjustments to make sure
your product is ultimately fit for
purpose for the communities in which
you're designing for.
uh you this is an important point Joan
for our listeners is there an example
that you can give of the harm that could
be done if this toolkit for example or a
framework is not adopted just a simple
example to bring it to life. Sure. So uh
one of the members of the council was uh
given a great example uh she she lives
in Africa and how um a uh there was a
water problem in one of the villages and
so an international agency came in and
planted more trees because it was a very
arid area and the idea was that more
shade would help the soil retain water
and so they planted a whole bunch of
trees
and went away thinking that this was
going to help create more shade which
was going to help the soil retain water.
And then they came back about 6 months
later and found that all the villagers
had chopped down all the trees. And the
question from this agency was well why
did you do that? We thought we were
helping the water problem. Well, they
didn't do their research appropriately.
They had done all the analysis on an
adjacent area. And in this location,
what the trees actually did was soak up
all the water from the roots and
actually hurt the village and dried up
all the wells. And so this is an example
of where without having the right data
for the right community, a conclusion
was drawn, an action was taken, and
actually harm was done to the location.
Now, this is a small example, but it was
so vivid when Angela told the story,
right, that it stayed with me as a
perfect example, small example, right,
of all of us wanting to do the right
thing and yet the action not fitting the
community we're designing for. And so,
if we had the data about that particular
village, that solution wouldn't have
been deployed right. It was a solution
that had been deployed in lots of other
villages and worked but was the wrong
solution for that location. So that's an
example hopefully that will stick in
people's minds. Well, that's a great
example. Uh Joanne, uh you briefly
touched on a topic that's become a big
discussion point which is synthetic
data. What are your views on that?
because they some people argue that
synthetic data is like making a copy of
a copy of a copy and it gets degraded
over time.
U because as uh you know regulators are
working towards it synthetic data has
become an important thing. So tell our
listeners a little bit about your views
on this. So um I encountered synthetic
data first as a privacy officer, right?
Um so synthetic data was first developed
as a way to actually solve the privacy
dilemma of personal information and as a
way to get at characteristics of data
without disclosing information about you
Sanjay or me Joanne or any population.
Um and it was actually a solution for a
different problem. Okay. and um which
was wonderful but uh and it was
generated data that would give you a set
of characteristics similar to an
underlying data set without disclosing
anything sensitive or personal about the
underlying data set and I thought it was
a wonderful thing for privacy and I
still think it is. I think the challenge
now is that synthetic data is also being
looked at as a solution for missing data
which it also can help. But the
challenge is this with artificial
intelligence and generative artificial
intelligence in particular is that if it
is overused right your the solution can
be problematic in the conclusion it
draws because synthetic data is
engineered data for certain
characteristics and if it is
overamplified in the data set it will
cause outcomes that are not necessarily
clearly reflecting the norms of what we
will call natural data data from all of
us users that is being pulled in and it
may if it overrides the natural data you
have to be very careful to see what your
output is going to be I think as
engineers we have to understand when is
it appropriate to supplement with
synthetic data the problem is when
synthetic data becomes mixed with
natural data and we don't know which is
and neither and we need to make sure
generative AI can tell the difference in
in our problem sets and solutions when
it goes out into the wild. That's that's
the problem I think everybody's worried
about. How will I be able to tell the
difference and how will I be able to
validate the conclusions I'm drawing if
there's too much synthetic data and not
enough natural data in the data set? So,
I think there's some positive uses for
synthetic data. I understand how it can
be a very useful tool, but I think
there's also ways that it can be
overused and then it will we will come
to the wrong conclusions.
Well, that's uh very good to know
because that's a big topic of discussion
and regulators are also looking at maybe
there are some standards towards that.
Uh Joanne, uh what kind of core ethical
principles should be embedded in AI
systems in your view?
Oh, so I think you know the OECD has
done really good work in this area. Um,
and this is one where I think um, you
know, I was I was part of the group when
the OECD came up with their privacy
principle, their refresh of privacy
principles way back when. Um, that's how
long I've been doing data work. And um,
I do think that transparency and
explanability are really important. And
I know and I I know I think people think
that transparency means we're going to
show all the algorithms which I always
uh as somebody who works for Mastercard
I I used to get the question can you
show me your fraud algorithm and it's
like think about that just think about
it for five minutes and understand why
that's a bad idea but I do think
transparency of thought and process and
governance and reviews and and how
organizations um are strategizing. I do
think being transparent and explainable
and accountable is really important
for making sure your partners, your
customers and your employees and your
regulators understand what you are
doing. And if you can't explain it, well
then what how how do does anybody have
any confidence in what you're doing? And
so I think those principles, those three
in particular are super important. I
mean privacy and security are kind of
table stakes at this point. You should
be paying attention to those. But the
other three and then having some type of
integrity in your process meaning
governance that somebody ultimately has
to be responsible and able to then I
think with all of the new regulations
requiring some level of auditability.
You must be able to explain what you're
doing and why and what the outcomes are
and what your review process is on a
regular basis of those outcomes. I think
all of those things are just really good
basic housekeeping for most
organizations. And if you can't do that,
if you don't have the talent to do that,
then I would urge you to find the talent
before you start using these tools.
Right? I just think it's really
important. And at Mastercard, the other
piece that we have is we also want to
have positive social impact for our
products and solutions. Um we we believe
that um you know, all boats rise in the
world if everyone gets to participate.
So that's just one of our other
principles that I think we'd be in a
better place if more organizations
thought that way as well.
So that's uh those are some important
points you said explanability,
transparency, security obviously uh
being the table stakes. Um in terms of
uh you know the reviews and things of
that nature. Uh Joanne uh does
Mastercard have an ethics or an audit
board for AI and what do you suggest
that for uh other organizations?
So we have a governance process. We have
an AI governance process. It grew out of
our data governance process and our
privacy process before that. So um we've
matured as an organization as I'm I'm
sure most companies have over time as
they've become more and more digitally
um sophisticated. Um and I would urge
every organization to look at their
governance practices. Um what we have
found over time is that you know in the
beginning our processes were a little
bit fragmented. We had a privacy review
board and we had a security review
board. What we did is we embedded all of
it into two things. First, we embed all
of these disciplines in our product
review cycle. We um make sure that our
product development process embeds these
reviews in it. So, that's the first
thing. Second, we do have an AI review
board that looks at AIdriven products
and solutions and it includes a whole
host of disciplines including the data
scientists, but it also includes data
quality experts. It includes data
designers. It includes um um we do AB
testing of course, but it also includes
folks from our technology teams, our
privacy teams, our legal teams, our
communications teams and security teams
so that we can make sure that we can
explain these products and solutions and
our sales teams as well so that
everybody along the way has a point of
view on the product or solution. and
things have gotten killed just be not
because we can't do something but the
question is should we do it and does it
align with our brand values. Now there's
different levels of review. We also kind
of put our products and solutions on a
scale and so the more risky they are
depending upon the data they're using
what the purpose of the product is and
who it will impact the higher the
requirement for review. All of our
reviews are pretty much done though by
internal Mastercard employees. We do not
have an external ethics board or review
board as some companies do although we
do talk about it from time to time. I do
think that um organizations in like
health care have deployed external
review boards very very effectively
given the issues of like medicine and
health care and risks related to adverse
effects. Um, I do think that in those
industries it makes a lot of sense. I've
seen in external review boards and
technical companies put to some good
use, but I think sometimes it's harder
for external experts who don't
understand the products and solutions
sometimes to fly into an organization
and so at this point we all of our
expertise is in house. All of it is in
house. Uh a related point to that uh
Joanne is you've been a chief privacy
officer uh and you know obviously now
there are chief data officers with the
advent of AI especially large language
models. Has that role uh changed or
should it change? Uh this is a feedback
that you can provide to uh CEOs of
companies who are listening uh who have
this kind of a role.
So these roles, you know, in in the
grand scheme of things, these roles are
relatively new in the grand scheme of
business, but um we've seen so um the
IAP, the International Association of
Privacy Professionals, um just published
an article in the Wall Street Journal. u
my friend Trevor Hughes um he pinged me
just yesterday so I think it it may be
in today's paper yesterday's paper but
um about the changing role of the
privacy officer and what we've seen that
role morph into and it's not a huge
surprise is being uh responsible for the
laws and regulations around AI partly
because um many privacy officers not all
many privacy officers are lawyers and
they have kept up with data regulations
and they are well placed to do the
thinking behind the the tradeoffs of how
do you apply all of these emerging laws
to your operations. And so I do think
that that is the change in that job that
if the privacy officer has uh legal
training and not all do, but if the
privacy officer is a lawyer and is in
the law department, then having that
lawyer also take on those
responsibilities with the right team, it
makes a lot of sense. Um because for
quite a long time, the privacy officer
has also played a role in data ethics.
Um now if you have the ability to also
have a chief data officer and not all
companies have both but maybe they do or
you just have a chief data officer. Data
officers have also played that role if
they have been in charge of data
governance. Now many data officers have
played a very technical role. Data
officers are not all
one-sizefits-all. We can have that
conversation too. But the mandate of a
data officer often does include data
governance, data management, data
quality. We can talk about the issues
around data quality in a minute um as
well as data science as well as data
architecture and those roles I think can
mature into the AI role if the
individual has also done a fair amount
of data science and is connected to the
strategy of the company. Right? It all
depends on how that position has been um
created and what it's been asked to
focus on over time. I've also seen some
organizations where the privacy officer
and the data officer are now reporting
into a new position called the chief AI
officer. So, we're seeing some shifts. I
teach at Carnegie Melon in both their
chief data officer and now their new
chief AI officer uh certification
program. So, we're seeing shifts in all
of these positions, but I think the
skill sets of these individuals are all
super important. The privacy officer
usually from a legal and compliance and
risk perspective. The data officer from
the data operations which is the food
for AI and the architecture and then AI
really the science the data science the
execution the algorithmic thinking and
also the infrastructure that's needed
connected all of this connected to the
business strategy of what an
organization wants to achieve. So that
that's how to think of the landscape.
Well, that's uh very helpful. You've
kind of laid out uh three different
roles. The chief privacy officer, chief
data officer, and maybe now the chief
officer. You can combine them. That's
But I know that many organizations are
struggling to find the right um skill
sets in one person, right? So,
absolutely. Um Joan, a question that
came in from our uh you know a listener
was that is it uh possible to create
unbiased data sets and truly fair AI
systems and what strategies can help
mitigate bias in AI systems? Okay, so
today our data is biased. Okay, I mean
it just is. I so the reality is is that
our data it it depends on who you are
and what your organization does. All
right. And I think that um so I'm going
to start first with data quality because
I think that many organizations mine
included Mastercard. We have processed
credit card and debit card transactions
right prepaid card transactions for a
very long time. The data itself is not
biased. Okay. and how you want to use it
and for what purpose you want to use it,
right? And so, but our data, right,
reflects where we process and where we
do business. And if there now, we have
the good fortune of being pretty much
truly a global company, right? 210
markets around the globe. We're a global
company. But even for us, not all
countries have the same volume of
electronic payments as others, right? So
some countries are more still use cash
for example or maybe we are have a
lesser volume than of our our
competition right. So even our data as
good as it is and as global as it is may
not be completely
even and for payment information around
the globe. We know this and we will true
it up because we recognize that it's
flat in some locations. That's what
organizations need to do in looking at
the data set that they're going to use
for the query, the purpose, the problem
they're going to solve. It's not that we
can fix all the data. We can't. We're
not going to get every last piece of
data on the planet that we need. But the
data scientist, the data quality expert
should be able to work together to
figure out, okay, what's the problem?
What's the purpose we're trying to solve
for? And then what data do we have? What
are the characteristics of this data?
What does it tell us? What do we know
are its problems? And then together from
a data driven perspective, figure out,
okay, we should know that in these
markets, well, cash is king. We need to
true up volume of if it's an economic
exercise, we need to true up the
activity for cash payments or on our
competition or whatever it is. That's
how you remove bias in the answer. We're
not going to remove the bias in the
data, but we want to remove the bias in
the outcome so that we get an equitable
outcome for whatever the problem is
we're solving. That's how to think about
it. That's great insight. Uh remove the
bias in the outcome. Uh because
everybody's focused on the bias in the
data and how do we cleanse it and the
data is going to get better and data
quality experts, my team, you know, my
former team at Mastercard that we work
on this. So the data is going to get
better and better the more we're aware,
but also more and more data is going to
get created in general, right? So it's
really focusing on the purpose and then
looking at the data you're going to use.
That's how we're going to get to the
better answer, right? And that's how
bias will get minimized in the
algorithmic outcomes. That's what what
excites me is like that kind of
thinking. And yes, please don't get me
wrong, data quality is super important,
but it's not going to be the only way to
do this. Yeah. Uh Joan, there is a big
human cry and I don't know just briefly
you can tell our listeners if it's true
or not. Everybody is screaming that we
are running out of data for these
foundation large language models. Are we
running out of data? You are you're a
data expert. Tell our listeners are we
running out of data? It's like are we
running out of oil?
So are we running out of data? I don't
know that we're running out of data but
there is only so much data that we're
generating that's going to be of
interest, right? And then we're going to
continue to generate data. So the
question is, do we have the right data
for the purposes for what we're solving
for? And at what point does data get
stale and that we're waiting for the
next wave of information? And I think
that's why people are looking at things
like synthetic data to supplement that
information that we're missing. And I
don't know yet. Okay. I I understand the
the the theory that we are going to not
have enough
information, but I also think we
generate information every day. So, it's
there will be a tension between what the
models need to consume, but we're also
going to be generating new products and
new solutions, right, and new
information. The question is how much of
that will be generated from artificial
intelligence and be of good quality,
right? So that we act remember I said
natural data versus synthetic data. Well
then there's artificially in generated
data where that's the tension for me
because I want good quality data like
good food like a good diet rather than
junk food or synthetic food. Okay I
that's the real question. We will not
have enough natural data for the
appetite of these machines. But what is
it that we're trying to solve for? And
is artificially generated data going to
get in our way? I think that's a
question for the foundation model
companies to solve. Yes, I do. Okay,
that's great. Uh Joanne, lots more to
discuss, but obviously you have uh uh
you know uh we need to get you back
here. Generally towards the end, we do a
quick uh lightning round of questions
that uh one-word answer. It's a little
bit of fun, but also gives our listeners
a view into how you think, what you
think. So, you ready? Yep, I'm ready.
Okay. Joanne, AI ethics or AI
efficiency? Oh, I'm going to land on the
ethics side, but I get the efficiency
desire. I sure do. Okay. Uh,
transparency or privacy?
Oh, this is a hard one for me. Uh,
transparency. Okay, you heard it.
Transparency or privacy. Automation or
human oversight? Human oversight.
Algorithmic bias or data bias?
I can't have both of those. Ah, you can
have both. We'll make an exception for
you. Okay, we'll go with both. Um, open
source or proprietary? Oh, man. So I
think we have to have open source for
the foundation models, but I still think
we're going to have proprietary too for
nobody wants master your Mastercard
charges out on on the foundation models,
right? So um it's so it's a hybrid world
we're living in. But uh absolutely but I
do think that to get the kind of
innovation that we want um that
foundation model companies are going to
want to do more open source work. So um
there's that. Okay. uh global AI
standards achievable or unrealistic.
Um for harms achievable but for
specifics not not necessary. Okay. Uh
final one um biggest AI risk bias or
privacy?
Oh.
Um, it might be privacy because of the
big desire for each one of us to have
our own bot.
Um, and I think that it's that when that
begins to happen when we all have our
own personal agent. Um, I don't know
what the I don't know how that's going
to intersect with privacy. Um, so okay,
I'll leave you there with that thought.
Well, that's fantastic. Uh thank you
Joanne really for sharing your
invaluable insights on AI regulation,
data and data governance. You know your
expertise which spans data privacy and
AI has you know provided our listeners
with a comprehensive view of the
challenges and opportunities in this
rapidly evolving field. And as we
navigate the complex landscape of AI
regulation, you know, voices like yours
are crucial in shaping policies that
foster innovation while protecting
individual rights. So to our listeners,
thank you for tuning in to Regulating AI
podcast. Remember the conversation about
fair and responsible AI regulation is
ongoing and your engagement is vital. So
until next time, keep exploring and
contributing to the future of AI
governance. Thank you so much, Joan.
This was fabulous. really really enjoyed
it and I'm sure our listeners really
appreciate your time man. Thank you for
having me. This was fun. Sjin, thank
you.
[Music]
