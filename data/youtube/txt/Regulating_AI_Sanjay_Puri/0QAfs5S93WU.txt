I would say, you know, one of the best
things about a country like Australia is
that we we tend not to be um overly
constricted by ideology. We're really
practical people. We we we've always had
to be. You know, this is a massive
continent, small population. It can have
some of the worst natural disasters in
the world. We later found out that 55%
of those young people on this list were
Aboriginal or toristraate islander. Um
and and here in in this state in
Australia, less than 3% of the
population is Aboriginal tourist. I
think historically what we've seen
through waves of technological
innovation is that you can have
regulation that doesn't just um allow
but actively fosters and supports good
innovation. I'm underlining the word
good innovation.
>> Welcome to the regulating AI podcast.
Join host Sanjay Puri as he explores the
dynamic and developing world of
artificial intelligence governance. Each
episode features deep dives with global
leaders at the forefront of regulating
AI responsibly, tackling the challenges
using AI can bring about head-on and
enabling balance without hindering
innovation.
Welcome to the regulating AI podcast.
I'm your host Sanjay Puri and today we
are joined by one of the most
influential voices at the intersection
of human rights and emerging technology
professor Ed Santo. Ed is the
co-director of the human technology
institute HGI at the university of
technology in Sydney where he leads
policy and governance program shaping
Australia's and increasingly the
Asia-Pacific's AI future. He was also
the Australia's human rights
commissioner where he championed
technology that advances rather than
erodess fundamental rights. With that,
let's dive right in. Ed, welcome to the
Regulating AI podcast. It's great to
join you.
>> Awesome. Uh Ed, we have a global
audience. Uh I'm sure we have people in
Australia, New Zealand, and other parts
of Asia Pacific listening in. Uh but we
have people around the world. These are
policy experts, policy leaders,
uh you know, civil society advocates. So
for them who might not be as aware of
your amazing work, can you just walk
through a little bit about your
background? You began as a human rights
lawyer, then a commissioner, and now
lead an AI policy think tank. Just walk
us through what led you to focus on
technology governance as your life's
work.
>> Yeah, so I was a human rights lawyer uh
working here in Sydney and um I didn't
have a particular background in
technology and certainly not in
artificial intelligence. Um, we ran an
organization which is I guess a little
bit like the ACLU in the United States.
And what became apparent was we were
seeing this one specific problem over
and over again. And I'm one of the
curious people who tends to ask why. I'm
sure that was annoying for my parents
and my teachers. Um, but but the
specific problem was this. Uh we we we
offered this free legal service for
young people who were coming into
contact with the police and we were
inundated with young people who had a
very very similar message. They were
saying, you know, I was doing something
that was pretty innocuous. Maybe I was,
you know, riding on the train without a
a train ticket. Uh and then suddenly I
was being um approached by the police,
you know, multiple times a day. Um but
but certainly multiple times a week and
they were quote unquote checking on me,
checking to make sure that I wasn't
doing the wrong thing. Uh and they
experienced this as as harassment. Um so
that was the first thing we noticed. The
second thing we noticed, and there's no
easy way to put this, I'll just be
really direct, was that all of these
young people had dark skin. Not just
some of them, not most of them, all of
them had dark skin.
>> And we were kind of asking ourselves the
question, what on earth is going on? And
so what we learned um and this is more
than a decade ago now was that there was
a program that the largest police force
in Australia was running called the
suspect target management plan that
essentially used an algorithm to
identify who were the young people in um
the the largest state in Australia in
New South Wales who are most likely to
go from committing minor crimes to
committing more serious crimes. And then
they were targeting those um you know
those young people bit like those broken
windows um kind of policy in New York.
And uh what we found was that that that
you know that system was trained on
criminal justice data sets that went
back many many decades and it learned uh
understandably but wrongly that that
people with dark skin were more likely
to commit crime because here in
Australia as in um some other parts of
the world those people um have
historically suffered a much higher
proportion of injustice. And so what
what that really opened my eyes to was
how a system that was not intended to
create injustice and not intended to do
wrong, in fact was designed to be more
data driven can go terribly wrong and go
terribly wrong at scale. And just to
give you one last point on this, a
statistic, we later found out that 55%
of those young people on this list were
Aboriginal or toristraate islander. Um
and and here in in this state in
Australia less than 3% of the population
is Aboriginal tourist islander. So it's
it it was going off off beam at such a
high rate that it was clearly something
that was very important and I could see
that there was an important human rights
issue at stake.
So this was um this was 10 years ago
obviously uh pre um well pre LLM's
pre-CH GPT or all these other things um
that you're looking at and then what was
that uh aha moment? Yes. Uh it seemed
like the system was profiling certain
kinds of people, dark-kinned people,
aboriginals, indigenous people.
What did you say, hey, this is not
right. I need to do something about it.
What what was kind of motivating you for
uh there?
>> Yeah, exactly that. So, so we, you know,
we're an organization um that had the
capacity to run big litigation and we
did and we we got a result for those
young people who were uh unfairly and
unjustly caught up in this system that
had gone terribly wrong. But what we
also started to ask was well if this
specific system could go wrong uh in
this way um was it a total outlier or
were there other systems that were also
going wrong and uh subsequently I guess
we found out that there were indeed
other systems that were going wrong. And
so I guess what that really led to was a
a journey where where we were trying to
kind of understand how many of these
systems were at risk. Um and and I
should also point out that while we were
doing that, we also had some really
other interesting kind of um research
and and encounters. And this was with
people who were giving us the exact
opposite message. These were people
saying, you know what, our experience of
artificial intelligence is that it's
making our world better. It's making our
world more inclusive. So I've done a lot
of work with people with disability and
there are some extraordinary positive
use cases for AI particularly for people
with disability that can have that
opposite effect. So I sometimes talk
about living in a fever dream you know
you have these conflicting visions
between the kind of true dystopian
uh problem of of AI going off the rails
while simultaneously there's some
wonderful wonderful uses of AI as well.
So I guess the the view that I came to
eventually was we re we really want to
supercharge the the positive uses of AI
that the the AI that can really make our
world better and more inclusive uh while
being cleareyed about what the risks and
threats are and and combat those.
So uh obviously being cleareyed um you
know that uh this innovation or this
transformation that we're seeing has
some tremendous opportunity but being
cleareyed that there could be challenges
that you saw that was 10 years ago now
with you see where the technology is
going or has gone um you know whether it
is in hiring, housing, loans all those
things that are out there. Um
what is your perspective um of where at
this moment where we are and where we
are going because on one hand there is
just an incredible opportunity uh that
exists from healthcare to education and
stuff like that. uh but we also have
this rush to innovate because uh whether
because AI is now a geopolitical issue
too and we'll come to that in a second
where you no country wants to be left
behind you know we want everybody wants
uh to be I don't know whether it's
number one or whatever it is and
everybody wants to have sovereign I when
you look at it from an Australian as
well as a global perspective what are
your thoughts on that?
>> I think you're right. There is this
enormous sense of uh FOMO or fear of
missing out where uh companies but also
entire countries feel like they need to
move at at a faster pace than they feel
comfortable to grasp the opportunities
of of AI. And so I feel like, you know,
10 years on or a bit more than 10 years
on from when I first started getting
involved in AI, we've moved from a
position where you could genuinely have
been completely and understandably
ignorant of the risks. I think people
understand those risks now. uh but
instead we we have sometimes something
that that really frustrates me which is
this sense well it's a bit like having a
balanced investment portfolio you know
where you say some investments will pay
off and some won't um but when we're
talking about uh the the impact of of AI
on people's basic rights it's not okay I
think to say um well yes it's doing all
these wonderful things but it's also
doing these terrible things and so we
should accept the terrible things
because it's also doing wonderful
things. We don't need to make that uh
tradeoff. Instead, what we need to do is
be more competent. We need to make sure
that when we are adopting and developing
AI that we do it in a way that is
effective, that is wellconsidered and
then and that really kind of graphs what
is what we we describe as a
sociotechnical challenge. So there's one
one part of AI which is the technology
but AI has always been integrated into
systems and that integration is often
where things can go wrong.
So things can go wrong and you're saying
that because this is on our show we have
you know policy leaders, senators,
members of Congress, ministers etc etc.
And the the challenge that we always
discuss is this innovation versus
regulation. Uh and what you're saying is
doesn't have to be that kind of a
choice. You can have both uh as long as
there's competence around that. Um and
>> that's right.
>> And I think historically, by the way, I
think that's true, right? Like you know
um this this idea that you are to have
to be a kind of a cheerleader for
regulation or an opponent to regulation
seems to me to be quite childish. Um, I
think historically what we've seen
through waves of technological
innovation is that you can have
regulation that doesn't just um allow
but actively fosters and supports good
innovation. I'm underlining the word
good innovation. Um, provided that uh
you're also cleareyed about the risks
and that's that's precisely what um I
think effective regulation does.
So Ed uh right now we see basically
three major blocks in terms of AI or AI
policy but AI you when you look at there
is the EU AI act then we have the US and
we have China per se and not downplaying
Australia or you know it's got other
great it's got a lot of great things
that are happening.
Uh I sit here in Washington and the the
thought here is that hey
uh the European Union's gone way too far
in terms of regulation. Uh we we're not
going to do that. We have to keep on the
path of innovation going there. China
obviously has a you know it's a top-
down uh centralized uh policy and
they're also heavily going down the path
of uh innovation in their own unique
way.
What is Australia doing and what are
your thoughts in terms of when you look
at these three kind of approaches which
is the right approach I mean EU has this
riskbased framework
US has we got to innovate we just had
today oh yesterday there was an
announcement for the white house about
uh you know the AI frameworks uh
innovation happens to be one of the key
ones infrastructure
uh and security being the other but tell
us uh your thoughts about these three
blocks and what's happening in
Australia.
>> I really like how you outlined those
three approaches. Uh and one of the
things that we're really conscious of in
a middle power like Australia is that
each of those approaches is high highly
ideological. So the the current um kind
of approach to AI in the United States
is very hostile to regulation uh and
sees it as as a block um a block on
positive innovation. Uh the European
Union um by contrast as as you point out
um takes an ideological position which
is much more cautious. Um and the way in
which they've taken that forward is to
engage in what's known as technology
specific regulation. In other words,
it's they're saying if you are using AI,
you have to do these things. And then
China, as you say, um it's it's also
highly regulated. Um and it's it's got
its own incentives.
I would say, you know, one of the best
things about a country like Australia is
that we we tend not to be um overly
constricted by ideology. We're really
practical people. We we we've always had
to be. You know, this is a massive
continent, small population. It can have
some of the worst natural disasters in
the world. Um we we can um it's it's
also got wonderful, you know, um natural
resources as well. But if we get too
high bound uh by ideology, um we get
into a terrible mess. Um and so I think
our opportunity is to be practical and
say you know what none of those
ideological positions on its own is
likely to give us what we need um to be
uh a successful global competitor um in
uh the AI uh market but also to give our
own people what they want. So what does
it mean to be practical? Um I'd say
three key things. The first is to is to
identify where our niche is. we're never
going to be able to compete on scale
with um the massive players particularly
in the United States or China. And so we
should we should find out, you know,
where are our existing strengths and and
really run at that. Um which brings me
to my second point. Um you know, being
strategic about AI is super important.
Um, one country that I've I've um found
to be particularly impressive here that
gets very very little credit for it is
Germany. I remember when Germany a bit
over five years ago set its big national
AI strategy for artificial intelligence.
They effectively said this to the world.
They said, "Look, you know, if you're
buying a product that is made in
Germany, forget AI, but just a general
product that is made in Germany, you're
not going to buy it because it's the
cheapest one available. You're going to
buy it because of Deutsche Technic.
You're going to buy it because we have a
a hard earned reputation for really
strong manufacturing, reliability, high
performance. That's that's why people
buy Germanade cars, for example. they're
not the cheapest cars but they might be
um the you know some of the best
performing cars
>> and then they said think about that when
you think about how we are going to
really um make a difference when it
comes to AI. So how do you apply that to
Australia? So Australia I think has a
hard one reputation for being strong on
our liberal democratic values that that
those are those are things that are um
kind of woven into everything that we
do. Um, coincidentally,
uh, we're seeing in pretty much all of
the kind of major global surveys at the
moment that people when they're using
AI, they want to be really confident
that their personal information, their
privacy is is going to be protected.
They're not going to be treated unfairly
because of something they can't control
like their their skin color or their
race or their um their gender or their
disability. So these are things that we
can really run at and we should u
because they play to our natural
strengths. And then the third thing I
think is um we can um show uh how we do
this well. Um and so for for us um that
means going beyond just looking at AI as
technology but AI as part of a really
effective system. Um Stuart Russell, you
know, one of the true godfathers of of
AI, who I'm sure you know well, S um you
know, in his famous wreath lectures a
few years ago, what he said was think
about AI as something that will always
fail. I mean it won't always fail but if
you think about it as something that is
or that will always fail what you're
going to do is you're going to think
about it as part of a system where you
know there's a technological component
and that technological component
sometimes will go off the rails in which
case you're going to have some
protections underneath to make sure that
it you get it back on the rails. Now, I
think some of the real success stories
coming out of Australia in how we're
using AI in health care, in transport,
logistics, um even in areas like mining
show exactly that they're really
effective systemsbased um approaches to
AI and that those I think are really
really strong and they and to to my mind
they show that practical approach as
opposed to a super ideological one.
So Australia is carving its niche. Um
you know as you said you're not
Australia is not an ideological uh
nation uh based on innovation but
liberal democratic values and they
released uh from what I understand a
voluntary AI safety standard and uh
proposal for mandatory AI rules. How
does uh a you know for our listeners who
are not in Australia or who are
companies looking to do business in
Australia, how does Australia's uh
riskbased approach compared to you know
like the EUA act which is also a risk
based approach and what can other
countries learn from Australia's model?
>> Yeah. So I should disclose we were
honored to be asked to help develop that
voluntary AI safety standard.
just go back in a bit of history, recent
history. Um I think in the early 2020s,
um there was this huge proliferation of
um AI ethics principles and they all
said something pretty similar. They
said, you know, make sure that when
you're developing and deploying AI, you
do no harm. Make sure you protect
people's personal information, all of
that sort of stuff. And in one sense,
they were great. Um they were all great.
Um but in another sense they were a real
failure. Um and we can see this
empirically that the vast majority of
those AI ethics statements or principles
have had no discernable effect on the
way in which AI is developed and
deployed. So that was the kind of
problem statement that that we all
started with here in Australia. And so
what we what we were hearing
particularly from the private sector was
that industry wanted practical guidance.
No no more fluffy principles. tell us,
you know, give us practical guidance of
what we can do to stay on the right side
of what the law requires, but also what
would be good, what what would be
ethical. And so that's where that
voluntary safety standard came in. It
said, okay, here are some very very
practical ways to assess whether your
system, your AI system is going to be
high, medium, or low risk. uh and um not
you know um not shying away from systems
that are that are high risk, but if you
are engaging in something that is high
risk, here are some very practical
things you can do to uh mitigate or
address that risk. And so I think what
what what really marks out that
voluntary safety standard is um that it
is uh it gets it gets hands dirty. it,
you know, when I was human rights
commissioner, I asked a lot of um
engineers and and and people, you know,
working um at the cold face of AI, you
know, do you have AI ethics principles
that apply to you? And a lot said yes.
And then my second question was always,
how do you apply them? And they said,
well, you know, I'm a good person, so I
kind of just have it in the back of my
mind, but it's really hard to apply.
Whereas this um I think is something
that actually takes that risk based
approach which pretty much every major
country around the world accepts but it
makes it much more easy to understand
and apply in reality.
>> So people are taking these standards so
to speak or voluntary uh standards and
applying and you know your institutes
working extensively with business
leaders also. Do you see uh an
acceptance with these uh companies you
know uh and what practical steps are
they or should they take to make sure
that they are you know following these
you know uh rules this human rights and
operating responsibly. Uh yeah,
>> I mean there are three things that uh
kind of are the most common issues that
come up when we working with corporate
leaders here in Australia but also
actually overseas as well. Uh the first
is um you know often what what what
companies want to do is they want to
kind of sit under a virtual bodie tree
and have a corporate retreat for a day
and then come up with you know a few
>> shiny balls or guiding principles that
will kind of apply to them and in one
sense that's kind of use useful in that
at least um engenders a useful
conversation but it doesn't actually
provide the guidance. It's the same
problem as those AI ethics principles.
So, what we say is be much more
specific. Focus in on what your
company's challenges are that you feel
AI might um help you with, which is
really another way of saying is focus on
AI use cases. Um make it so that it's
not an abstract conversation where
you're talking, you know, at 40,000 ft.
Make it super specific. Okay, point one.
Point two is um the question about
governance always comes comes to us. So
you know what is the most effective way
of governing the way an organization
uses uh and and adopts AI and um there's
a there's a really there there's like a
huge suite of different options but I'll
just give two examples of a really bad
but common way and a really good way.
The worst thing you can do is to say,
"Well, AI is technology. I'm just going
to hand it to the kind of technology
specialist, the CTO or or someone like
that in the in the organization, and she
or he will sort it out, will come back
and kind of update us from time to time.
Um, and and and and the reason that's
such a bad idea um is because AI is
general purpose technology. That's the
GPT as we all know in chat GPT. And so
it's being used um in organizations to
fulfill really important functions. And
so if you have the individuals, the
senior people in particular in that in
your company who don't really understand
what AI is, where the opportunities and
risks lie, then you're going to perform
those functions badly. um as we you know
like to say the dirty secret about AI
which should be better understood is
that um AI fails at about 80% of the
time and that's a persistent rate of
failure that that that rate of failure
has been at about 80% persistently for
the last four or five years and I'm not
talking about in the laboratory I'm
talking about in the real world it's a
real world investment um and a big
problem is that failure to engage the
right people in your organizations that
governance piece is so so important. And
now the third issue which is which is
something that we've seen particularly
over the last 12 to 18 months really
start to change and and and we're so
excited about this is the question about
how you can most effectively engage your
workers. Um we did this uh piece of work
last year which we ended up calling
invisible bystanders and what we did was
we worked with three different um uh
employee groups. We worked with nurses
in hospitals, um retail workers and uh
people government employees or public
servants and we were essentially asking
them two questions. We were saying you
know can you reflect on your experience
to date of AI being integrated into your
workplace? What's working well? What's
not working well? And the second
question was, can you reflect on what
you have learned from other or previous
technological transitions?
And the I can't tell you how
fascinating the uh that research was.
And I'll just give you one one example
because I think it's illustrative.
The nurses, a group of the nurses we we
were interviewing, um they had this
really consistent message. They said,
you know, if you go into a hospital, um,
one of the first things you notice as
you walk onto a hospital ward is the
sound, all of the the beeping, all of
the monitors that are that are going off
all the time. And, you know, you you you
notice that at the start and you kind of
go, "Oh." But then after having been
there an hour, you start to tune that
out and you stop it doesn't bother you
anymore. and they say, you know, if
that's happening to you, it's actually
happening to us as nurses as well. And
what it really shows is a very poor
technological uh transformation or or or
project because um those systems right
around the world tend to be um uh you
know, optimized for very high rates of
um of error. In other words, lots of
false positives. Um, and what that means
is that nursing staff tend to go, "Well,
I just can't pay attention to this
because otherwise I wouldn't get my work
done." Um, and so a lot of those systems
just don't perform at nearly the the
rate that they're meant to. And and and
what they what they told us is, you
know, we could have provided some really
useful advice about how um that error
rate should be should be addressed or
what is the rate of false positive that
would be optimal. Um and so we're
actually starting to see we have started
to see over the last um several years uh
you know nursing um unions as well as
nursing groups being more brought into
the tent about how those sorts of
monitors are deployed. And so different
hospitals actually have very very
different experiences now. Um and so I
think we see something very similar with
AI that that employees tend to
understand their own kind of domains.
they they they have that what's known as
domain expertise. They can actually
provide some really useful advice about
how AI can be integrated into those um
domains and and and we should listen
right like companies benefit from from
listening to that and and actually uh
taking on board that advice.
>> No, I think you made some uh very
important points. So let me just follow
on uh to a couple of things that you
touched on. I'll talk about the
corporate um when you deal with
companies
uh do you think governance is a board
level issue or as well as at what
perspective for these companies that you
deal with should uh you know AI
governance happen or should companies
have a governance committee an ethics
committee uh because I talk to chief AI
officers or CEOs Different companies
have different things but from your
perch what what do you suggest or what
do you recommend for these companies?
>> Look I think I think that the answer to
that question will change over time. Um
but in 2025
I I do think that there is real utility
in having AI specific focus on on
governance. um Alandre Nelson when when
she was um very senior at the White
House Office of Science and Technology
Policy, I think she did some some truly
globally leading work on precisely this
question. And so where um federal uh
departments and agencies in the United
States were required to have a a kind of
a central individual who had um
responsibility as a kind of a chief AI
officer. That was a really important I
think change. Um but there's a good way
of doing it and a really bad way of
doing it. So I'll start with the really
bad way of doing it. My my colleague um
Professor Nick Davis talks about the
kind of AI guru model of governance and
it's terrible where you have some single
person who uh purports to have the
answer to every AI question. even if
that individual is truly brilliant, they
end up being the kind of um the the only
person who really takes these issues
seriously within the company and and and
we also know that one individual is
unlikely be to be able to answer every
question that is at the mix of AI as
well as operations. So so so that is a
common model but a truly problematic one
even if the problems don't manifest at
at every moment. So what's a better
model to us? the the best model is is a
chief AI officer um who sees their role
as being kind of almost an ambassador um
within the organization. So um what she
or he can do is to explain to the
various different teams starting at the
very high level at board and senior
executive levels um what AI might mean
for them. So what are the kind of key
things they need to understand about the
particular AI use case? Uh how AI might
be deployed in their particular area of
operations um from a technical sense and
then kind of gives them a chance to
actually give their own views their own
kind of well-informed position on that
AI deployment. Now when you start to do
that then you're getting a level of
engagement across the organization that
is much much more valuable and uh
impactful and so that's the kind of
thing that we we really advocate.
>> That's uh very interesting. So what
you're saying is have a chief AI
officer. We by the way we have another
podcast called the chief officer
connect. I talk to the chief AI officers
of Fortune 500 companies. But you are
saying that person that position is more
of an ambassador kind of a role that
works across
departmentals, cross departmentals
working with the board and others.
Is this technology
that transformative that it requires
someone like that?
>> Well, and I think that's such an
interesting and and subtle question. Um,
yes, I think now and hopefully no in a
few years time. So, uh, I I don't think
we can close our eyes to the fact that
the transition that we're all kind of
experiencing now is a really significant
one. You know, I I in my mid-4s. Um,
I've been I feel like I've been bathing
in the waters of artificial intelligence
now for a very long time, but it's but
it's really only 10 or 15 years. Um and
and that's unusual. Most people are
actually only coming to AI um since
November 2022 when um you know chat GPT
was released publicly. So there is no
question that there's an upskilling that
is necessary and and so I think we need
to see this transition time as a
transition time but we also want to get
to a point as quickly as possible where
instead of thinking about it as AI
governance we're thinking about this
simply as governance and AI has a
component to it and and you know to be
honest you know we've done that in in
other areas of transition as well uh
when we for example if we were having
this conversation 20 years ago about
cyber security. Even that term would
have been um very foreign to a lot of uh
senior executives and um
you know members of company boards. Uh
and um instead what you had to do is say
well this is about how you protect the
integrity and security of the
information that your company holds. um
your obligations uh are the same um but
the technology is changing underneath
you and so what you need to understand
is what what those changes mean and and
we we'll you know create some new terms
cyber security is one of them uh that
will actually help you kind of
understand what is new about it but but
increasingly now when we talk about you
know the information kind of uh
obligations of a company we no longer
talk about them as cyber security only
we talk about them in the broad and that
that I think shows the maturity of the
debate.
>> Uh yeah things have obviously cyber
security AI etc. You uh focus a lot on
the workplace. You just talked about the
nurses and you talk about companies. Uh
at least I can tell you in the US there
are changes that are happening very
quietly in companies where companies are
letting go of employees or not hiring
especially
um fresh graduates coming in the new
hires that are coming in.
Is that a phenomenon in Australia? As
you said the population is small but
just if you just look at it at a broader
base this is also a major transformation
that is happening whether you look at it
from a human rights governance etc. How
should companies and countries because
it's going to impact because the numbers
are just the numbers that have been
predicted by McKenzie and many many
others are pretty staggering uh and
these are white collar so to speak jobs
and some in some cases entire
professions think parallegals think
coding software engineering etc etc so
what are your thoughts on Australia just
globally
>> I a couple of things. The first is
you're 100% right that there is a
massive transformation on foot now and
there is no question that huge swaves of
the working world will experience their
jobs change and sub some jobs will no
longer be uh valued. Um
that's point one. Point two is there are
wild differences in some of the
prognostications from particularly
economists about precisely where those
changes are going to happen, when they
will happen and what will be the new
roles that will kind of uh rise up in
their place. Um, I think it was, you
know, President Truman in the United
States who said, you know, every
economist I've ever come across says, on
the one hand, you know, the the answer
could be this. Um, and on the other
hand, it could be that. Um, I want a
one-handed economist who will only tell
me this, right? Um, will only give me
one prognostication. Um the the problem
is that uh the the the economic
predictions are a lot of people just
putting their finger in the air um
having um a bit of a stab at what really
amounts to guesswork. And I think um we
we anyone who claims a kind of a level
of certitude um that is better than that
is uh not telling the truth. Um we just
don't know. Um what we can do is look at
history. Uh so most recently in
Australia where we effectively saw a
period of de-industrialization in the
1980s
um you you saw two things uh happen. One
was you know mass layoffs. It led to a
recession. Um and um there were a lot of
people who were deeply deeply wounded by
that and and quite a lot of people um
didn't get other jobs. They may have
lost their job in in car manufacturing
or in some other manufacturing
um area and that was a defining negative
experience in their life. Um
>> so that that is something we need to
learn from. need to manage those
transitions in a much much more
supportive way. Um the second
observation is that uh after a period of
transition the labor market stabilized
and we're now at consistently very low
unemployment. So you know even today the
unemployment rate in Australia begins
with a three. It's it's it's high 3%
which is effectively pretty close to
full employment. Now my my point is you
have to consider both of those things
simultaneously. So yes, a whole bunch of
new roles and employee types will be
created. Um but it may not be easy for
people who are in jobs now at a certain
stage of their career to transition from
one role to another. Uh and so we need
to be able to create those new roles,
but we also need to be able to provide
true support and better than we have
historically at helping people move
across
Yeah. So I think what you're saying is
transformation is coming. We just don't
know the scale of transformation but we
should be prepared and
you know it's it's difficult and I you
know I as you said most people don't
know how this transformation is going to
work out like many other
transformations. Um at talking about
transformations you interface a lot with
the you know besides Australia the
region Asia-Pacific region per se
u I talked about the US uh EU and the
China block.
Why should the Asia-Pacific Australia
have a voice in AI governance? When you
look at that region,
it is a pretty significant region with
high growth potential. When you look
especially at the Southeast Asian
blocks, when you look at Indonesia,
Malaysia, Thailand, South Korea, etc.,
etc.
or and what is happening in terms of
governance and policy etc. in that
region.
>> This is I'm so glad you asked because
this is an issue that I feel very
passionate about. Um you know our region
um particularly as you say Southeast
Asia but also um parts of the Pacific
have um consistently over decades now
contributed
far more to uh economic growth globally
than um is that would be proportionate
to our population size. um we are early
adopters of technology. We are um much
more effective developers of technology
than um we tend to be given credit for.
And yet um too often uh uh countries and
um civil society organizations and and
even companies in those two regions,
Southeast Asia and the Pacific have um
far fewer opportunities to have their
say in a kind of big policy setting
forums and uh also I think in um
technical bodies like the uh ISO E that
are setting the kind of technical
standards for things like artificial
intelligence. Um so what do we do about
it? Um well, one thing is making sure
those people um are invited and offered
a seat at the table. Uh I I think the
French government did a a truly
excellent job in running the AI action
summit in Paris earlier this year, but
it was impossible to ignore the fact
that other than Australia and New
Zealand, there were very very few
countries who were invited from the
Pacific region to even attend. And
that's true actually for Southeast Asia
as well. And that should be considered
to be totally unacceptable that you can
leave out huge proportions of the global
population from such an important event
as that. I also think there needs to be
a greater level of curiosity of what's
happening you know in countries that are
not the United States, China or the
European Union. Um, I think those those
kind those those jurisdictions and
economies rightly get a lot of
attention. They should, right? They they
they are driving so much of what's
happening in AI. But they're not the
only only places. you know there are far
more PhD graduates coming out of um
countries like India now um in relevant
areas like like data science um than
there are in countries like France or
Germany or UK um even by some measures
uh the United States and so um I think
it's really good that the next um global
AI summit will be held in India I think
that's that's a very very positive thing
and I think it's a positive thing for
the the our region um the Asia-Pacific
region um and and so I think I think we
need to kind of rebalance that focus.
It's not that there's other countries
that matter, it is that other countries
matter too.
>> Other countries matter too. And just to
follow on that point, as you said,
Australia has a 3% of an indigenous uh
population. New Zealand has and the
other countries when you look at AI um
70% of the data it gets trained on is
English and western oriented
how are we how is Australia going to
make sure or New Zealand or many other
countries that have this indigenous
population that the cultures the
tradition the languages
don't get swept aside in this whole
transition to AI because
uh maybe the next generation don't is
not even aware because the training data
is coming in uh from a western English
oriented band.
>> Yeah. So there are there are two
solutions there that both need to be
given equal weight. The first is a
technical solution. So uh simply by um
careful exposure um of indigenous
languages for example um languages that
are not English or Chinese or the major
languages of the world you can develop
um you know large language models in
ways that um are much more attuned to
the cultural context of of of other
cultures. So that that that there is a
technical aspect of that which is which
is really important. It just needs to be
done. Um and and and indeed you know
there's some very interesting and and
positive work that is happening in the
Asia Pacific. You know Japan, South
Korea famously even Switzerland actually
there there's some very important work
that that's happening. So that that that
is the important technical component but
it doesn't end with the technical
component. Yeah. We we've seen
particularly in the United States um
with some of the debate about um more
inclusive
um uh development of AI, you've seen,
for example, some groups that rec um
that represent uh African-American
uh members of the US population go,
we're not participating. we're not going
to help you develop more AI tools um
where those tools are experienced by our
community as tools of repression. So
it's not purely technical. It's also we
would say something where you go well
you you need to have a proper discussion
with those affected communities about
how the technology will be used and
sharing responsibility or authority
about how those um that that those
technological tools are deployed in the
very communities where um you're asking
um for engagement and I don't think
nearly enough is being done on that
second question more needs to be done on
the first question too. The technical,
you know, question is not solved. Um,
but I feel like we're on a pathway
there, but we'll never get to where we
need to unless we address the second
question as well.
>> So, the technical as well as the
societal component is uh very very
critical. Uh, Ed uh when you look at AI
and you talked about this, it needs
massive amounts of investments. you
know, it needs uh chips, it needs
energy, it needs uh talent, etc. Right
now, the way it looks like it's going to
be with a handful of companies, you
know, maybe on one hand, you could
probably name, you know, maybe two
hands, I don't know.
From an Australian perspective,
um or just from a broader governance,
human rights perspective, does that
concern you? because um it could be uh
you know look at what social media is
today it's kind of driven by a few
companies what you probably think see or
feel is kind of in their hands AI maybe
as you said is one of the most major
transformation
what do you think of uh this from an
Australian standpoint or just overall
standpoint
>> look I I I do I think I'm not someone
who goes around saying that the major
tech companies are universally evil. I
don't think that they are. I think that
they're very big. And I think bigness um
has all kinds of positive implications.
It allows um them to do things at a
scale that otherwise only countries can
do. Um but it has negatives as well. Um
it's not uh good for any society where
uh competition is um so constrained by
markets where there are very few players
that um pricing can be determined by
something that is effectively not the
market. Um it's simply by the exertion
of market power. Uh and I do think
that's an area where the European Union
has been particularly strong. I mean the
US government as well. I mean under the
previous Trump administration uh there
was some very strong action taken on
what what the US is called antitrust
where rest of the world talks about it
as as competition. Same idea. Uh and um
it might be too early to see whether the
the current Trump administration will
have a similar focus on on that issue
but they have previously. And so I think
that that is something that um will have
an outsized influence outside of the
United States and China if um there's I
guess a banding together of countries
where there is literally you know no
interest for um you know unconstrained
market power uh to be supported I if
they come together and say no we're
going to push back on this.
>> Yeah. Uh finally u Ed want to talk about
one of your favorite but also is an
important topic is uh facial recognition
uh governance. uh if you marry facial
recognition with AI obviously uh you
know it becomes very very powerful in
terms of analysis for national security
domestic security for analysis and just
across the board disaster management
etc. what are your concerns when you
think about it and what would be a
rights respecting deployment which would
maybe in retail or public safety look
like for so I mean I think the first
thing to acknowledge is that there are
some very good and in fact rights
protecting ways of using facial
recognition a lot of us are using this
all the time to you know unlock our
smartphone um for most of the major uh
smart smartphone providers um that's
done in a very very well-considered way
that actually protects people's rights.
It doesn't uh harm them and so so that
that's the kind of thing that should be
encouraged and as we see different
models of digital identity being rolled
out around the world. Um we are at a
critical moment to really support that.
So I think I think what regulation can
do is can kind of say well look you know
if you have um designed your system in a
way that reduces risk then regulation
shouldn't shouldn't just go okay that's
all right it should it should foster it
should encourage it um and then I think
we should be just as um clear on where
those risks have not been mitigated so
so we know for things like facial
analysis a that the technology is very
very u unreliable in other words where
we're where a facial recognition system
seeks to identify, you know, something
about you, like how responsible you are,
how angry you are, that that sort of
thing. We know that those systems don't
work very well. Uh, and so when they're
starting to be used in ways that have an
impact on people's legal or other
significant uh rights, then that's a
that's a big problem. And I think the
the law should be very clear in in
pushing back on that. And then there's
the third category of companies where um
they've basically operated in a way
where they've really flouted the law to
develop their systems. I guess most
infamously we we've we've seen with
companies like Clear View AI there's
been quite strong regulatory action
taken against them in a range of
different jurisdictions including in the
United States including here in
Australia because uh those those
countries have that a company has not
taken seriously its obligations for data
protection and the right to privacy. So
I think I think we do need you know
effective regulatory action against
those companies because you know if
they're causing harm they should but I
think it also um sends a message I think
as well to the market more generally
there are good uses and they should be
encouraged there are bad uses and
regulators will will really take strong
action against those bad uses.
So as you I think what you're saying is
there are some really potential you know
good uses and we should just be careful
and have safeguards against the bad
users. Um Ed uh I could go on and on. I
have so many questions about the
Australian uh entrepreneur ecosystem. I
have uh questions in terms of uh you
know Australia is such a powerhouse in
terms of energy commodities. It could
become a big player uh because energy is
a key component. But maybe we'll have to
do part two. But before we uh let you
go, we have a lightning round that we do
uh just to make it little fun for our
audience. It's a one-word answer from
you. Are you ready?
>> I'm ready.
Okay. Uh Ed, so pick uh transparency or
privacy?
>> Privacy.
>> Okay. Uh national AI policies or
international frameworks?
>> International frameworks.
>> Okay. Innovation or uh precaution?
>> Innovation.
>> Okay. Uh, voluntary standards or
mandatory rules?
>> Voluntary then mandatory.
>> Make it a little make it a little
tougher and tougher.
>> Uh, risk based or principalbased
regulation?
>> Risk based.
>> Risk based. Okay. In your view, is AI
regulation moving too fast or too slow?
>> Too slow.
>> Too slow.
Uh what is the biggest risk in your
view? Bias, privacy or autonomy?
>> Privacy.
>> Privacy. Okay.
And then
finally, open-source models, they are a
boon or a pain?
>> Boon.
>> Boon. Awesome. Ed, thank you so much.
you know your work really reminds uh and
should remind our listeners that
progress and principle they don't have
to be rivals good governance can itself
be a driver of innovation so thank you
really for sharing your insights on
privacy productivity corporate
governance the AsianPacific
perspective the you know the global AI
ecosystem the indigenous population you
know it's uh really been so helpful and
for our listeners
If your conversation resonated, check
out Ed's book, Machines in Our Image and
the HTI's resources on corporate AI
governance. Stay tuned for more episodes
with global policy makers, industry
leaders, and civil society champions to
forge a fair, democratic, and innovative
AI future. I'm your host, Sanjay Puri,
and this has been the regulating AI
podcast. keep questioning, keep
innovating, and above all, be
responsible for AI. Thank you, Ed. It's
been just a tremendous, tremendous uh
pleasure to have you on the podcast.
>> It's been my pleasure.
[Music]
