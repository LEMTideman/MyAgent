If you don't play around with it, if you
don't get educated on what is what and
what's the difference, terminology is
not going to solve it and bans or bills
are also not going to solve it.
Welcome to the regulating AI podcast
with Sanjay Puri. AI is changing the
world faster than rules can keep [music]
up. So, how do we protect people without
killing progress? Each [music] week,
Sanjay brings you inside conversations
with global leaders, policy makers,
[music] and innovators who are wrestling
with that exact question. So, if you're
curious about the future of technology
and how it's governed, you're in the
right place.
Co-founder of Yearick, a company
reimagining how humans interact with
sound and technology, Karen brings a
unique perspective [music] at the
intersection of ethics, innovation, and
sensory experience in an AI
conversation.
>> Before we jump in with Karen, I want to
find out Karen a little bit about you.
You ran the music factory for 19 years
uh teaching music. Then you pivoted to
becoming a a mental health tech
entrepreneur. Was there a moment where
you thought I've taught enough people
how to play piano now I need to teach AI
how to listen to humans? So we have a
global uh listener base of policy
experts uh AI entrepreneurs and others.
Tell them how did you get to be at the
intersection of mental health and the
end? Music has a direct connection to
almost every people's person's heart in
the world. It's a a universal language,
right? We don't have translation
problems. We have maybe different
tastes, but it's an unique way of
reaching someone beyond their
capability, let's say, to express
themselves or their capability to read.
uh because everyone has a connection to
music and I grew aware of that. Um the
next thing that I grew aware of was that
the majority of my students they were
youth you know kids, teens, they yes
they wanted to connect with music. They
wanted to maybe be empowered to play the
piano. But their underlying need and the
reason why they stuck around from let's
say age six to 20 when they had to leave
for a university was that for some
reason I had created a space a school
where they were heard and this sounds
very trivial but it's not. It's a very
human need. It's an unmet need to this
day to matter, to be heard, and to be
led or guided.
Part of that uh way. You know, a
teenager comes in with a typical teenage
problems. A mother comes in with her
problems, maybe marriage, whatever. And
we don't talk about those problems, but
we do this through music. And when I
realized how big the impact can be when
you reach someone directly in a safe
space with the right technique, you can
change a complete life. And it's no
longer about music or about being
educated. It's about not being alone on
that journey. um to master life, to
master your emotions, to do to interpret
something and to make yourself hurt. And
when I realized that after those many
years, I was like okay, I can change the
lives of let's say a city if I teach
everyone or my employees teach people.
But that's nothing in you know in the
scope of the problem the world world
where we need this kind of connection
and that's when technology became really
really big and important and I saw that
coming and I was like okay how can I be
that companion how can it be that
guidance that listening ear that
interaction that people need on a much
bigger scale it's still the same mission
It's still the same thing but with much
more power to change lives while we can.
Yeah. At that time it was more the the
young people, the kids, the teenagers
who felt like the traditional way of
teaching music was not theirs because it
was full of limitations and you have to
learn first how to do this and you have
to practice first. All this was stifling
them. And the way I taught it, it was
like, hey, this is a language and this
is how you can express yourself and you
can learn the language in a way that it
helps you be yourself, be the best
version of yourself. Now, the same is
with mental health. And there's a lot of
connection between music and mental
health because it's very vulnerable. And
so the the same way of reaching someone
and taking them by their hand but
empowering them to walk by themselves
eventually is the basic thesis
underneath your king. So Karen just uh
you know shifting topics uh you know
something that we want to talk a lot
about is in February 2024 you know
14year-old CEL said sir he took his life
after chatting with a chatbot from what
uh it's been reported that the chatbot
encouraged suicidal thoughts as someone
who designs AI for mental health what
went through your mind when you heard
about that case
>> I have kids. I have a grandchild even
and I'm also helping to raise two
teenagers right now. And I love this age
because they're they're, you know,
becoming grown-ups. They have so much
for them. And the mother of this um
14year-old also said he was a wonderful
person. Um they're full of potential,
right? And it was just devastating for
me because imagine that at a scale and
all the ways that this could have been
done differently. Nobody builds a
technology to kill someone. Nobody
builds that. But sometimes our
incentives, people's incentives are led
astray and they don't think through what
their users on the end other end are and
who they are and what their unmet needs
is where this can go. Uh, and
without pointing fingers, I think if you
don't know humans well that you're
serving, if you don't obsess
with observing them and walking in their
shoes, these things will keep happening.
And every life, this life was one to
many. Every life that is in danger, it
doesn't even have to go to suicide,
is one to many.
And for me it was like many things like
we failed we all failed this kid. Why is
you know why is it possible for him to
be on the phone or on the computer for
10 hours a day?
How is it possible that his environment
didn't get access to him? I'm saying so
don't he has a wonderful mother and
parents I'm sure. How is it possible
that he had such a need and such an urge
and such a desire that was easily
exploited that we're we all fail here
and I want to be part of the solution
and I am part of the solution and I
encourage everyone to not stay stuck in
the drama headlines and all the
assumptions but be very practical. there
isn't a person who has a role in this
and what we can build that is safe.
>> Something that the former surgeon
general had talked about the epidemic of
loneliness that's uh at least in the
United States that's happening. But what
makes teens so uniquely vulnerable to
these systems? Uh Karen,
>> teens by definition are evolving.
They're in their most formative years.
They're trying to create their own or
find their own identity. At the same
time, they are already, you know, kind
of they have to kind of sort of split
from their parents, split from their
famil family's ways to be able to be
their own selves. But at the same time,
their brain hasn't developed in a way
that they could oversee things or make
decisions like a grown-up. They they are
flooded with hormones and everything is
a first. I see this I saw this with my
kids. I'm seeing it with the kids that
I'm raising now and I see that this
everywhere.
After the fact, after something has
happened, then you recognize it. But for
a teenager, everything is at first. They
may have some need to to to feel close
to someone. I'm not even talking about
loneliness. That's, you know, like
that's even further. But they it's not
like they can make a step back and say,
"Oh, I'm feeling this um urge to be
close to someone or uh someone is
interested in me and this gives me such
and such feelings." No, it's they
explore because they are made to explore
their nature and their urge and what
they need to do is go out and explore
and find things, take risks and the
environment that they are given. If that
environment exploits that
and in times of change in these
formative years any let's say bad or
malicious influence is one to many.
>> A shortage of uh mental health
professionals availability freely of
these chat bots. How does one thread
that needle to make sure we provide help
but provide it in a safe way?
>> So I think technology is not going to go
away. AI is not going to go away. We
need to create and keep creating
solutions that are can be embedded
regardless of your financial background,
right? because not everyone has access
to mental health professional law even
if they if there were any that are
embedded in where people spend most of
the time whether it's school or work and
that are easy to handle and I say this
with the ease of hand because too many
people like a majority
may know where the help is may
understand that there are approaches and
reframing techniques
But it's too cumbersome to get there. In
my time, it was the library or you had
to go to a doctor. Today, it's like if
it's too complicated, someone who's
already in a weak place is not going to
reach out. It's not going to read the
book. It's not going to search for it.
Maybe they don't even know how to
search. And that's why we have to have
systems and here's where AI and
technology come in that sense where
someone is at that can make sense of
context and biomarkers and that can
interact
that in a way that is adequate and based
on the data based on the situation of
each individual. This is something in a
perfect world that maybe a human could
do but reality is different. people are
very busy. Parents don't always have
that that headsp space and in school I
mean I don't have to start talking about
how big these classes are and how little
and how time uh each teacher has and and
how they are also you know at the point
of crisis now. So in short there is no
easy way but there is a collaborative
way and in that collaborative way
technology and a will play a very
important role early enough be there
early seamlessly easy to access uh but
not be the only thing and then get our
youth from being children to being
adults going through struggles not
trying to fix the struggles But making
them strong enough that they can last at
this time without getting caught into
all these pitfalls that we see lots of
headlines about in the media. Can the
chat bots play a positive role in terms
of uh reducing the loneliness crisis of
some of these uh let's say teens to
speak about
>> a chatbot or a technology that bridges
certain very rough moments let's say
3:00 in the morning can't sleep if you
know the chatbot helps you do a
breathing exercise and reframes some of
the thoughts that are have kept you from
sleeping and encourages you to go back
and you
sleep rather than showing you things
that keep you engaged forever, then yes.
Or if you have, as a teenager, you've
been, let's say, bullied and you need to
discuss this, but you don't want to go
to your parents. So, technology can
really bridge that moment. But a child
that is abused at home, that doesn't
have a a support net, that is vulnerable
physically and psychologically, and
maybe left alone.
Well, the chatbot's not going to solve
that problem. It's the same for not just
for youth, also the same at work. You
can't yoga, breathe, or chat your way
out of a toxic workplace. What you need
is u support maybe from a chatbot and
the understanding that you need to
change something more than just what you
can do on a personal level. Um, so it's
not going to fix a company's toxic
culture. No.
And I think the devil sits in the detail
as always with tools. They can be
fantastic,
but they can also just be a way to not
look at the real problem.
>> Should its function, not its label,
define its regulatory class? I mean,
where's the line between an AI coach and
an AI therapist? And who's responsible
if something goes wrong? By definition
um a therapist as we understand it is
someone who has a license has passed
certain things etc and it's a human now
we have machines coming and doing part
of that what we call them you know is is
go is is the terminology going to solve
the problem I don't think so because we
used to cause call ourselves AI chatbot
when it was still a niche product when
people really went there for mental
health. Now an AI chatbot or AI
companion, everyone thinks of chat,
right? So that that the meanings have
changed. The terminology is messy,
muddy. Um you can't any you can't call
yourself AI therapist anymore because
you know there are bills now and laws
and that's fine. But on the ground those
millions of people they have their own
tech terminology. For example, they call
an AI therapist whether we want it or
not. If you don't play around with it,
if you don't get educated on what is
what and what's the difference,
terminology is not going to solve it.
And bans or bills are also not going to
solve it. Well, I'm actually very happy
that there is regulation coming our way
because that means we need to talk about
it. And now I would like to add to the
talking the acting and really, you know,
working with the stuff and making your
getting your own opinion on that.
listening to your children. Why do they
use it? How do you use it? What does it
give them and what does it take from
them? Um, talking to the lonely senior
who has nothing but the bot. Talking to
a woman who's just given birth and
nobody wants to listen to her. Talking
to the man who has been left by his wife
and never practiced to talk about his
feelings. Right? So millions of stories
that we need to surface, look at and
then come up with good regulation.
>> Illinois and Nevada have banned AI for
mental health treatment. Uh Utah uh the
state of Utah requires therapy chat bots
to protect health information and
disclose they are not human. And about
two days ago, Senators Josh Holly and uh
Senator Blumenthal just introduced the
Guard Act to ban AI companion chat bots
for minors. And then some time back, the
Kids Online Safety Act, KOSA had passed
the Senate 9123, but it got stuck in the
US House of Representatives over free
speech concerns. according to you since
you're you know working and building in
this space which regulatory approach
would make the most sense to you
>> so the ideal way would to really have um
all stakeholders and that includes youth
by the way have a say in this I I for
example I give you an example I do like
the fact that according to California
the providers of phones will be charged
or not charged but they are They they
are responsible for making sure that
when the parent buys a phone for the
child that the age is registered because
this this takes responsibility for that
out of each you know app uh creator and
gives it to the the store the phone uh
creator. I think that's a good move a
very good a fair move. You buy your
child a phone, you ought to say how old
it is and you ought to acknowledge that
you have to you want your child to have
a phone. Okay. Other things are kind of
difficult because it's so easy to to go
around, right? Um I'm just going to give
you an example. Yes, I think it's
important that the disclosure is clear.
This is a AI um and not a human.
Absolutely. But just put yourself in the
in the shoes of a 16-year-old um is
starting to discuss and and gets to a
point where he opens up or she opens up
and they may say something that sounds
towards suicidality and then bam, you
get a huge banner in your in in your
face that says stop, you know, or oh,
you can no longer talk because uh you've
been here too long or you are, you know,
You may be suicidal. We have we want
nothing to do with this. I do understand
why this makes sense from liability and
safety side. But if I put myself in the
shoes of a 16 year old, this is going to
absolutely frustrate him or her because
they have come to a point where they
wanted to talk about it and now it's
like no no go away with your problems. I
don't want you. Right? Um do I have a
solution? I think it can be built. Yes.
But in a normal conversation, if it was
your child that came and opened up and
you go from talking about what you have
for breakfast over to how school was and
then it gets into the interesting part
where they're opening up to you. You
don't do that. You don't say, "Oh,
stop." Right? U you have reached you
have reached a threshold and now we're
going to call the police. This is how it
feels to someone that is going through
the currently practice um way or journey
being blocked or being kind of called
out as soon as it enters a certain
territory. And mind you on the other
side of those companies it's not about
the children that they're doing this.
It's not about the youth. It's about
liability. It's about saving their own
skin, right? And for one company to say
that there are millions of people who
talk about mental health that the
problem is all solved like within weeks,
that to me is a slap in the face because
it's not that easy. It's much more
nuanced.
>> Many of these mental health apps avoid
regulation by not collecting personal
data. How can regulators enforce uh
safety measures like crisis
interventions while still you know we
want to respect uh users anonymity and
privacy? How how is that possible?
>> At earick for example my company we
don't we don't even get in in that
territory because we don't you don't
have to register there. There is no need
to know your name. There is no need to
know where you live and what you do and
all that to be helpful. it's not needed.
So, first of all, build it into it.
Right? But why do people ask or why do
companies ask? Because they can then
better market. They can sell that data.
They can they can do a lot of things
with that, right? So, I think first of
all, you know, whatever is free needs to
be defined as what is happening so that
it can be free. Privacy is absolutely
possible. It's technically possible. do
something that is safe because otherwise
we wouldn't have these apps uh that that
some parents buy on top of their phone
that can make sure that they are alerted
to content that is not good for their
kids. So technically it's not a problem.
You just need to want to do it. Um and
the other thing is you know again where
is the money where does the money
where's the incentive where who is
profiting from keeping someone in there.
Now let's say we have a platform that is
really for mental health that really
wants to do a good job. Okay. Now you
can build a very nuanced and empathic
and very personal way
that understands the user
accompanies him her and when a certain
threshold is passed engages in a way
that makes it very seamless to see let's
say the right specialist or to be you
know to to to create a really good
handoff escalation that is not so abrupt
and hey no go away I think that is
technically possible and I think this is
going to have to happen very very soon
mind you not everyone who's in a crisis
wants to talk to you not everyone who is
in a is is maybe having suicidal
ideiation um is helped by just pushing
them into some waiting room of a
therapist so what we need should do for
a good solution is understand all these
people to a very very deep degree and
figure out how to best nudge transition
help guide them from where they are now
and how they like to communicate and
like like to accept help to eventually
where the help is that they would
accept. Why? Because if we push help
down their throats at all costs just so
we can do this, we're not going to reach
the level of improvement and real
support that they deserve.
>> How can regulators trace accountability?
I mean, should AI mental health tools be
required to show some kind of a
reasoning trail even if you know today
the large models are largely opaque.
It's very hard to get some kind of an
explanability as to why a chatbot kind
of responds in a certain way. But here
we are talking about you know mental
health advice. What is your uh
perspective on this? Yes, there is art
there are RTC's maybe Therabot did
something but real data from real users
in the wild you know like in in the
world and how this all works uh I think
that is something that can be required
um and that that is something where
regulators need to work towards um I'm
not saying everyone has to you know
publish their proprietary
uh algorithms but there are perfect ways
to show how you build it, how you put in
guard rails, how you're testing, how
you're challenging, what you do, how you
escalate, all these things, they can
perfectly be transparent. Um, and that's
where we can really uh get very
productive and and and constructive in a
positive way.
Again, we're at the incentive. You know,
if you have to say, "Hey, look,
actually, I'm only after the millions of
ad dollars and actually the next thing I
want to do is sell you erotica." Well,
you know that we should be able to see
that and to call it out and and to make
a decision. Yes, I want to be Yeah, I
want to be sold as or yes, I want to be
I don't know what. But then I know what
I'm I'm getting and I know what I'm
paying for for now. It's very opaque.
It's everything that has free on it is
probably not free. And I think there's a
lot that regulation can do there. You
know, AI systems often reduce emotions
to labels like anxious or sad or you
know basic those things. How can we
regulate uh or design systems to ensure
they don't flatten or misread you know
nuanced culturally specific expressions
or of emotions? Uh can
>> the question is a bit who is in charge
of this? You know I if you if you go out
and ask a a child or a youth even young
adults um to explain what they're
feeling, they will probably tell you one
of five emotions.
So if we're talking about meeting people
where they are, that's why you have to
start. So you have to make it easy
first.
I'm not against putting things like
anxious no you know these buckets no
problem with it but what I encourage is
not to stay there. So if that is your
entry door to speak about how you're
really feeling um how your emotions are
playing out can you start there that's
fine but do not stay there. So don't put
labels for the label's sake. Labels are
or buckets are to make a conversation be
easier be started. And hopefully a a
technology, a human, you know, someone
who supports you is going to help you be
more nuanced about how you speak about
your emotions and about your thoughts.
But again, it's not like it's taught
somewhere. maybe at home hopefully, but
it's not something that you address in
kindergarten. It's not something you
address in school.
So, we have we have this
challenge of not it's not just the
technology that has to evolve.
It's everyone, every touch point of a
youth on social media, but also offline
where people throw around labels without
knowing what they say or what they mean.
There's a lot to do with around there.
First of when I was little, um, it was
taboo, right? So, there weren't any
labels that it wasn't talked about. And
now it's always the other side of the
pendle. everything is a trauma and
everything is uh toxic and narcissistic
and all that and it's creating another
problem right but it's not a tech
problem per se it's a societal problem
>> companies historically have experimented
in those regions with things that they
would probably have got get slapped on
the wrist here is there a danger that
you might see some of that uh be set
forth in those parts of the world.
>> Good technology, good bot, a good AI is
perfectly capable of growing with you,
evolving with you and understanding
what you what who you are, what you
need, right?
But and here comes the big thing.
What does it need? It needs memory. And
here we're talking about a topic that
will become huge very soon.
You can only serve someone
when you have memory
and you can learn from that. You can
only connect the dots between someone
talking about
that life doesn't make any sense anymore
and the next time they check in they ask
for a bridge or for bridges. Right? You
can only
address this adequately
if you have memory. A memory is
something that neither the big one I'm
not gonna point to fingers but neither
the big ones really have worked out nor
you know the small ones at ear kick we
very early we worked hard on the memory
because we knew that someone wants to be
heard and remembered also for details um
I'm saying this but it's not trivial
it's something that needs a lot of work
so now to your example if you go to
let's say um population or a part of the
world that is very different from the
others very remote let's say good
technology very good memory with the
right intentions and with good database
and science base is going to have no
problem if you are N one but you have to
want to do This.
>> You ready? Transparency or privacy?
>> Privacy.
>> Okay. Uh AI and mental health, yes or
no? [music]
>> Yes.
>> Uh security or accessibility?
>> Both. But um
>> first first [music] accessibility for
the many reasons that I have seen
working with air.
>> Okay.
>> Uh fairness in AI or accuracy in AI.
>> Oh man.
>> As I said, we're going to make you
think.
>> I don't believe fairness is really
possible. So
let's go for accuracy.
>> Accuracy. Okay. Is AI a greater risk or
an opportunity for society according to
>> opportunity? Absolutely. Opportunity.
>> Opportunity.
Should teen access require parental
consent or would be should they be
available direct?
>> Right now I would say it should require
parental [music] um buying absolutely
and responsibility if we're at that.
>> Okay. Then let me take it a step
further. Should uh companion chat pods
be banned for uh teens or minors? Let's
just put it this way.
>> Ban the phone first.
>> Okay. Ban the phone first.
Okay. So, what is the single most
misunderstood aspect of AI in mental
health? Very briefly.
One of the most misunderstood
parts is
that we immediately jump to it's either
AI or a human
and that's just simply not the
discussion we should have.
>> Okay, that's a good one. It's not either
or. Okay, then let me ask you one thing
that AI can do better than humans in
mental health.
memory.
>> Finally, uh Karen, one insight [music]
about human emotion that AI has taught
you.
>> That's a hard one.
>> Yeah, AI has taught I mean, let's make
it this very personal.
AI has taught me
that I'm mostly wrong about myself
and not wrong in the terms of 100% but I
am I
um
we're full of stories that we tell
ourselves. AI is very good at looking
through that. We can embezzle people
with stories. tell and stuff but AI is
very straightforward and it has shown me
a lot of white space that I and then not
not necessarily as a you know as as a
coach or a therapist just just by
showing me the data. Karen really thank
you so much for joining us and for the
critical important work that you're
doing at your kick you know and uh this
is a very critical topic uh for so many
families and people around the world
policy makers are really busy right now
I can tell you there's so much uh
activity going on in the US and other
countries and for our listeners who want
to learn more about Karen and her work
you can find her at LinkedIn Uh and you
can also go to the Yorkik uh website at
yorkick.com.
And thank you for our listeners. Uh this
is regulating AI podcast. If you like
our podcast, please like, download and
forward. [music] And until next time,
please stay informed and stay engaged.
And let's keep working towards AI that
truly serves humanity. Karen, thank you
so much. This has been a great uh
conversation. There were so many other
questions but we ran out of time but
really you've been a great guest.
>> Thank you so much. You've been a great
interviewer.
>> Thanks for tuning in to the regulating
AI podcast with Sanjay Puri. If you
enjoy today's [music] conversation,
don't forget to leave a comment. We'd
love to hear what you thought. Share it
with someone curious about the future of
EI and join us next time for more
stories and insights from the leaders
shaping what's ahead right here on the
Regulating AI podcast.
