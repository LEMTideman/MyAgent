You're trying to hold the technology
itself liable and there is you know no
existing institutions that we have to do
that or no feasible way to actually make
that happen.
Welcome to the regulating AI podcast
with Sanjay Puri. AI is changing the
world faster than rules can keep up. So
how do we protect people without killing
progress? Each week, Sanjay brings you
inside conversations with global
leaders, policy makers, and innovators
who are wrestling with that exact
question. So, if you're curious about
the future of technology and how it's
governed, you're in the right place.
Our guest today is Camille Carlton,
director of policy at the Center for
Humane Technology, where she works to
ensure that AI and digital platforms are
designed to serve people, not exploit
them. From shaping governance frameworks
to addressing the societal risks of
persuasive and generative technologies,
Camille brings a rare blend of policy
depth and moral clarity to the future of
AI. Camille, you
have a incredible background, a very
different uh background. We have a
global audience. Our audience is made up
of uh policy leaders from around the
world. It's made up of AI experts, AI
enthusiasts,
you know, a variety of those people.
Just briefly, you can tell them a little
bit about your background.
>> Yeah, absolutely. So, I have um a
nonlinear background, I would say, in
terms of getting to where I am today. I
started my studies in international
affairs but quickly found myself working
like many Americans do in a completely
different industry. I worked in fintech
and health tech for many years and you
know through that time kind of you
forgot a little bit about my background
until around 2016 with um the big US
election and then 2018 with Cambridge
Analytica. And what happened for me was
during the 2016 election, I started to
feel these changes throughout my family
in the way that we could converse with
each other and in the way that we
started to feel much more polarized. And
it was never something that I'd
experienced before. And then 2018 comes
around, we have the Cambridge Analytica
privacy scandal. It really provides this
kind of structural understanding to what
we were all feeling in 2016. And it
opened kind of my eyes to the power that
technology has to fundamentally reshape
society. And so from there I went on to
get my masters um where I focused on
international development with a lens of
tech and society. Uh and that is kind of
what brought me to this work that I'm
doing today. Well, you know, from uh
producing spreadsheets for fintech to
having uh engaging in a lawsuit that
involves Sam Alman, that's a incredible
career pivot. But uh and you become the
technology enforcer in some ways. Coming
to uh the two lawsuits that are
currently in play right now, Camille,
they basically involve chat bots per se.
what is at stake generally uh if you
went to just talk about it at a high
level for the audience. Yeah, I mean I
think what these lawsuits really talk
about is the way in which AI products
are designed and developed and the way
in which those designs can impact all of
us and particularly with how quickly AI
is shifting and how quickly these
products are being put onto the market
with very little safety testing. I think
the stakes are really high because what
tends to happen right in the trajectory
of markets like this is that you see
first the sharpest and the most kind of
obvious harms that come out right and so
these cases are huge huge tragedies. It
is, you know, they are anchored in the
loss of life being prompted by these
products. And it is really important to
hold that and to understand that these
are also the tip of the iceberg and that
the way in which many of these consumerf
facing AI products are designed is that
they are pushing way more harms than we
even understand right now. Right? We we
see these cases and they're anchored in
suicide, but we're also seeing the rise
of AI psychosis or delusions. We're also
seeing the rise of AI connection,
intimacy, dependency, and all of these
things are going to change the
relationships we have with ourselves,
with our, you know, close networks, with
our communities, and they can be really
harmful. And so, it's important to hold
that how these products are built impact
you and I. And that's what these
lawsuits really look look at
>> loss uh psychosis etc that talks about
could be a potential health epidemic per
se. Do you have some facts or figures
for our audience that kind of point to
some of these things? Because that would
be a big worrisome thing because as you
see this is not a United States
phenomena alone because now if you look
at the rapid growth of LLMs, chat bots
etc. It's taking on because you have a
younger population in some of the global
south countries. It's taken on a speed
and a life of its own. So what does that
mean? So, if you can just talk a little
bit about what all is at stake here.
>> Yeah, for sure. I mean, I think that one
of the things I really focus on and that
these lawsuits really focus on is our
kind of connection to intimacy and our
connection to human-based intimacy
versus this kind of rise in artificial
intimacy. And one statistic that really
shocked me recently was I think it was
47 or 48% of high schoolers either
themselves or they personally knew
someone who was engaging with a chatbot
for connection. Um and one in four young
people believe that AI intimacy can
replace human intimacy.
And so when we think about how jarring
that is, right? This is again just for
young people and it's one form of this
kind of you know psychosocial harm um
that we're seeing. We're also seeing as
I mentioned and you pointed out a rise
in kind of delusion-based cases and it's
still very early. So I think the truth
is that we don't actually know how many
people are experiencing this. Open AI I
think came out and has said it's a very
very small percent of their users that
are experiencing that they believe are
experiencing things like AI delusion.
But the problem is that focusing on the
percent of their users uh it kind of
makes us forget how many users they have
and how many people they have and the
fact that chat GPT is a global product.
It is a product that has been adopted
the fastest in the history of technology
across the world. So, this is an issue
that's not just affecting young kids,
it's affecting kids, adults, it's
affecting people all around the world.
Um and in fact a partner of ours, the
Tech Justice Law Project, just actually
filed several more lawsuits against
OpenAI in the last two weeks. Six of
which were focused on adults and either
based off you know adults taking their
own lives or adults uh experiencing AI
delusions and AI psychosis.
>> So let me just uh follow up a little bit
on this Camille. This prior surgeon
journal had said that loneliness is an
epidemic in this country in the United
States and that's across age groups. We
don't realistically have enough trained
certified help to work with these folks.
I mean and also we there's just no way.
Do you see a role for chat bots in
something like this? So Camille,
>> I mean to me it is a fundamental
disconnect to claim that the same
technology that has kind of pushed a
loneliness crisis that is based in the
lack of human connection is the
technology that's going to solve it.
Right? If the loneliness epidemic is
anchored in, you know, this missing of
community, this missing of human
connection, um, products that are
explicitly designed to undermine human
connection is not going to solve that.
And the the problem that we're seeing
right now is that the consumerf facing
AI chatbots that are on the market are
not explicitly designed to solve the
loneliness epidemic. In fact, they're
explicitly designed to maximize
engagement, to maximize users time
online, and to kind of develop
artificial intimacy, dependency, and to
in many ways actually replace human
connection. I I mean, I think it was
Mark Nits uh I think it was Mark
Zuckerberg who himself said that he
thinks that everyone should have AI
friends and that, you know, we're more
likely to have AI friends than human
friends. And to me, that's not solving a
loneliness epidemic at all. It's
actually perpetuating it.
>> We might be at a point where we were
with social media. It might become too
late before we solve this problem. Are
you touching on social media as what has
caused this problem and now AI could,
you know, escalate it? Uh, and
are we able to stop it or is it past
that point? Uh, Camille. Yeah, I mean I
definitely believe that social media
played a role in accelerating the
loneliness epidemic. Of course, there
are many factors at play, but social
media is one of those factors. And the
way in which artificial intelligence is
so much more personal, so much more
always readily available and responsive
to your unique needs. always on is you
know social media tenfold 100fold in
many ways and so you see the progression
and you see the way in which as social
media kind of laid the foundation
artificial intelligence is really
accelerating it that said I don't think
it's too late one of the things that has
been really heartening about this work
is that because of the lessons that have
been learned from social media the
general public and policy maker ers are
catching on much more quickly with
artificial intelligence. I mean, I
remember doing this work in the social
media space and it took years for people
to kind of be able to explain what an
algorithm was or understand the
difference between recommendation
systems and design versus content. And
it took a really long time for people to
accept the harms coming from social
media. with artificial intelligence. I
mean, we had the launch of chat GPT,
which really kind of opened up people's,
you know, shook the world, opened up
people's kind of understanding of this
technology. And it's only been a few
years since then, and people are really
starting to understand the harms as well
and saying that um it's not acceptable.
So I think that we're in a much better
position to actually figure out how do
we get good innovation and the best from
this technology but really question the
harms that are coming out and the ways
in which these products are being built
today.
>> Now the chat bots come out from large
language foundation models. these
models, you know, and there were several
companies that produced those also help
kids with their homework, help me with
uh sending out nice uh cool letters uh
to employees, uh they help people with
business plans etc.
So the question will be yes there are
the chat bots but then there is all this
good stuff that is happening and the
foundation companies are going to say
hey you know we are producing a
foundation model but the chat bots are
being uh misused.
How do you kind of re because are we
going to throw or or should there be a
different kind of regulation around the
LLMs versus the chatbots? Uh because
they are in some ways the LLMs that you
know you and I use for letters and other
stuff might be not that harmful while
the chat bots could have potential
dangers. So what do you say to that
Camille? Yeah. Well, I think first if we
zoom out to me, one of the biggest myths
that's being kind of told in the AI
policy space right now is this idea
that,
you know, the peril of artificial
intelligence is the price that we pay
for the promise that we'll get. And you
see this with kind of this argument of,
okay, but there's so many benefits and
there's going to be so many more
benefits, right?
um if we get to AGI, we're going to be
able to diagnose health issues in a way
that we've never been able to before.
We're going to be able to solve the
climate crisis um in a way we haven't.
We're going to basically have so much
human coordination and problem solving
that our biggest problems are going to
be small. And in order to get there, we
just have to kind of keep going. And
yes, there's going to be harms along the
way, but um we'll try and minimize them
as much as possible to get to this kind
of greater greater good. And to me, I
think that this is just kind of
misleading because so much of this is
based off um the scaling paradigm that
these companies have have adopted,
right? This idea that, you know, the
bigger the model, the more capable it's
going to be. That's how you get to AGI.
And yes, if your goal is getting to AGI,
then that makes sense. But if we take a
step back and say, okay, wait, what is
what is the goal again? Because I
thought the goal was helping humans
solve health issues, helping us solve
big problems like climate crisis,
helping us be more efficient in our
business plans. If those are the goals,
then
how are we designing products that
explicitly help those goals, right? And
you don't necessarily need huge LLMs to
actually address all of those goals.
There are lots of, you know, application
specific um artificial intelligence
products that are actually working to be
able to diagnose cancer way earlier and
they're actually much more successful
right now than the large language models
are. And so for me, it's really a matter
of how is the product that's being
developed actually being applied to
specific use cases. And when it comes to
this distinction between LLMs and chat
bots, it is the LLM that is underlying
this chatbot, right? And so we have to
make sure that the kind of original LLM
is designed well and designed in a way
that is actually going to address
specific problems. you know, regardless
of if it is used in a chatbot,
regardless of if if it is used in, you
know, an underlying health application
because at that point of LLM development
is when you have the most control to
make sure that the product is safe.
>> If you know you mandate a certain fixed
design to people,
it firstly reduces innovation. Secondly,
it probably gives inherent advantage to
larger or existing companies. How would
you do that? Because you you mentioned
designing uh several times. So explain
to our audience what do you mean? How
would these companies because some of
them are listening to you right now. How
are they going to design this stuff?
Yeah, I mean that is a great question
and I think um when people hear design
they instantly go to that place of oh
specific instructions on this is what
the product's going to look like or this
is the data you're going to use. And the
way that we approach it is actually from
more of a zoomed out place and it comes
from um the principles around kind of
duties of care and products liability.
And so for us, it's not about explicitly
mandating
certain design features or practices,
but it's actually about saying that
these companies, the developers of
artificial intelligence, have a duty of
care to build their products in a way
that, you know, takes into consideration
the public interest and public safety.
And if they don't do that, then they can
be held liable for the harms that their
products cause. It's kind of the same
thing that we see in the auto industry,
right? We don't explicitly tell car
manufacturers how to design their cars
down to the nitty-gritty, right? But we
say you have a duty to make sure that
before your product hits the stream of
commerce, it is safe, that you have
tested it, that you have, you know,
reduced all foreseeable risks. And if
you don't do that and you put a product
into the stream of commerce, you can be
held liable for it. So I actually think
that when you think about um
incentivizing design from a product's
liability standpoint, you are able to
balance this innovation because again
it's not about saying innovate in this
way. It's saying you innovate in any way
you want as long as a product is safe
and you're balancing consumer
protection. And so it's kind of this
light touch um approach to making sure
that products get put on the market as
fast as they can as long as consumers
aren't you know at the tail end of
harms.
>> There is the issue of yes we want
transparency which is what you're saying
but then there is the issue of
proprietary innovation um plus national
security. How do you balance that
Camille? Yeah, I mean I think it again
it goes back to principles of
transparency which I think are really
based in this idea that there is a huge
asymmetry of information within the
technology space right now. And this
comes from again most people, policy
makers, the general public not having
any idea how these technologies are
being built, not having any idea how
decisions are being made because there
hasn't been a push to develop more
explainable AI and therefore not knowing
exactly, you know, how we make sense of
this, how we regulate it if we do, how
we use it safely. And so what we really
need I think is different intervention
points around transparency and
explanability. And it doesn't
necessarily mean that you know all
information is shared publicly but it
could mean that information is shared
with you know a trusted third party
auditor to kind of make sure that things
are being built in the right way. you
know, it could mean that information is
shared with attorneys general um as you
know, kind of an oversight mechanism. Um
it doesn't necessarily mean that things
are going out into the public. We also
have, you know, another kind of angle of
whistleblower protections as a way to
promote, you know, more transparency
within this, you know, very very
asymmetric um information ecosystem.
Because what we've seen time and time
again is that the people building these
products are best positioned to actually
talk about and share the decisions that
companies are making and kind of when
companies are making decisions that go
against the public interest which we
have seen over and over again. I mean
even with the uh recent open AAI case
again that was being uh introduced by
the Rain family. What came out uh from
this lawsuit is that OpenAI internally
had explicit instructions
for its chatbot not to end the
conversation even when topics of suicide
were being brought up. And that type
those types of decisions I think are so
important for the public to know. And so
we really do need to balance you know
the public interest and as you said
proprietary information and national
security.
>> So couple of just uh follow on questions
to that. One is obviously you said uh
explanability which is very very
important. Uh there are now reasoning
models that kind of walk you through
some of that but still the explanability
is not clear. What you're saying is
maybe the government has a role an
attorney general whatever else but let
me also bring a little dose of reality.
These companies are investing hundreds
of billions in some cases now trillions
and we live in a very partisan time
where in our system in the United States
every four years or two years we have uh
elections things change perspectives
change etc.
when you're kind of putting these
companies at in some ways at the mercy
of these government officials.
Do you think that would work from that
standpoint given the level of
investments given the stakes from a
geopolitical
uh given that we are in a partisan kind
of an age? That's what I'm trying to
see. Is there a pra I mean we do want to
get to that explanation
point the black box but how does one
practically do that Camille that's what
I'm trying to figure out. Yeah, I mean I
think that in any well functioning
economy, society, it's all about
balance, right? It's about balancing the
power that companies have, balancing the
power that governments have, balancing
the power that the public has. And you
don't want, you know, a kind of
asymmetry across either of those areas.
And you want to make sure that you have
checks across power in all of those.
Right now what we're seeing with the AI
ecosystem is power is heavily
concentrated on companies and this makes
sense right if you look again at the
kind of history of innovation and new
products this happens when you have new
products in the market because we
haven't quite figured out how to make
sense of it and it makes me think again
of a bit of the kind of auto industry
and uh some folks might remember that
there used to be uproar at the idea of
forcing companies to put or forcing auto
manufacturers to put seat belts in their
cars, right? It was just so crazy to
think that this was something that the
government should do. Um, so I just want
to kind of remind us that there is a a
history of these es and flows when it
comes to how we figure out the right
balance of putting safeguards in for new
technologies. Now, the other thing I
will add is that it doesn't necessarily
need to be the government, right? We
could also have, you know, private
third-party auditors that are coming in
and they're the ones that are kind of
responsible for um assuring transparency
and explanability. There's different
options here, but I think that the kind
of principle we're trying to uphold is
equalizing
power across the three and making sure
that there's checks and balances.
>> EU has the AI act which is you know risk
based has penalties and you know fines
that are kind of laid out which can be
serious. In the US we have basically a
stateby-state uh kind of uh rules uh for
AI and you talked about that you know
there is a role for both what should the
EU folks take from us because I talked
to them a lot too and what should the
Americans take from the EU folks
>> I think it's hard to say what's better
because the what makes regulation
work or not work is the context in which
it is applied. Right? You can have an
excellent piece of legislation and if
it's applied in a context in which it
can't be enforced, it's useless. And so
I think what the EU has makes way more
sense for the context of the EU versus
within the US what we have are you know
a lot of existing kind of laws and
frameworks that we need to figure out
how to just upgrade them for the age of
AI. And I think that that works really
well for the US given again kind of the
dynamics between the federal government
and state governments and the way in
which our politics works right now. And
so to me it's not so much about choosing
one or the other but actually saying
okay given you know given the hand we
have what's the best approach and in the
US I really think what we need to do is
clarify that our existing protections
and legal institutions do apply to
artificial intelligence and figuring out
you know how we even reconceptualize
them a bit you know as we figure out
like how they intersect with artificial
intelligence And a great example is even
copyright, right? Copyright has been one
of the like existing uh legal frameworks
we have that has been really used. And
what it has done is I think actually put
into question you know the context in
which this idea of fair use was
developed was very different. now that
there's so much the scale is so massive
and the ability to kind of completely
undermine one's you know entire like
life's work um via these LLMs using
copyrighted works I think what that does
is it has us question okay does the fair
use principle make sense do we
reinterpret it a little bit for
artificial intelligence you know how do
we uh take the laws that we have that
are flexible and say this this is what
we need out of them for this new age.
And that to me is the approach for the
US
>> for
a small entrepreneur, you know, when you
look at a patchwork of 50 different AI
rules and regulations.
Does that make sense or does it make
sense to have a preemptive you know
there was this move to have a push to
make sure states would not have uh AI
policy etc. And that might be coming
back again because the worry is that
these patchworks of rules and
regulations could hurt innovation
especially the example is always given
somebody in a garage is how are they
going to have the legal means to go over
all of these things. So
>> a lot of the rules and regulations being
passed by these states actually scope
out who gets applied, right? And so many
of the bills don't apply to all AI
companies. They actually intentionally
apply to larger companies, companies who
are able to kind of implement these
things. And so that's the first thing
that I would say is yes absolutely that
is really tough for small small
entrepreneurs small businesses and as
we've seen with past regulations and as
a principle for developing regulation I
think it's really important to make sure
that kind of what we are asking of
companies is based off the feasibility
of those companies and doesn't hurt
small businesses from entering the
market. Now going on to kind of the
preeemption debate. I think what was so
problematic about the preeemption debate
from earlier this year is that we were
preempting laws without passing anything
at the federal level. And to me that
misunderstands kind of the scale and
scope of artificial intelligence. What
is so powerful about the states is that
they can act faster than the federal
government. And you know as everyone
quotes uh Brandeise they are the
laboratories of democracy meaning they
tend to iterate together and after a few
rounds like you get kind of a better
bill and that is something that the
federal government can then build on
right if the federal government is
passing you know passing AI regulation
and then saying okay we're going to
preempt it at the state level because we
want to make sure that we have a you
know uniform floor across the US. That
totally makes sense. Let's pass a floor
the way we have for um the minimum wage,
for example. Federal government kind of
sets the baseline and they say this is
what it's going to be and states kind of
add on to that um as they can and as
they see fit. That is kind of the model
that you know we've seen for for many
industries and that makes sense. What we
don't want is the federal government
preempting regulation
uh at the state level while kind of not
pushing forward anything from their end
because that leaves us at a place where
we have no checks and balances on how
this technology is harming consumers.
One of the proposals during that time
was as you know they were struggling
with uh you know Senator Cruz was trying
to get it out there and Senator
Blackburn kind of push it was to have a
2-year window if the federal government
didn't do uh come up with a you know a
federal law then the states would have
the ability to do that. Would you be uh
okay with something like that? I think
that the problem with timelines is that
we just cannot anticipate the way the
technology is going to change or how
quickly it's going to change. And so I
feel uneasy thinking about having a
timeline in which there is no way to
protect consumers from potential harms
of this technology when it is being
developed and deployed so rapidly. And
so to me there is a disconnect between
the speed at which the technology is
advancing and kind of putting a whole
pause on any sort of check and balance.
I would love the federal government to
develop something in a two-year
timeline, maybe even sooner, but I'm not
quite sure that um stopping states from
doing so actually incentivizes them to
do so more quickly. I think the opposite
is true. And what we have seen is that
as we have more states pushing forward
uh on you know different kind of tech
policy reforms that is what puts
pressure on the federal government to
act. And so I think that in many ways
that's a different kind of um power
balance at play within the policy
ecosystem. One interesting thing if you
can briefly touch on is you talked about
this personhood issue with AI that you
are against it because you think it
provides some kind of a liability shield
and I know I've had Stuart Russell on
our show and Joshua Benji also talk
about that it is important because 5 to
10 years down the road maybe uh those
things happen. Firstly for our audience
can you just very briefly explain what
personhood is or from your perspective
and what is your uh briefly just quick
opposition to that? The idea of
extending legal personhood to artificial
intelligence would be um the idea of
extending kind of a suite of rights that
we see explicitly you know held for
either uh people or now you know we do
see those some of those rights also
being kind of explicitly granted to
corporations. So one of these rights for
instance is you know first amendment
protections for speech. So people and
now corporations have first amendment
protections. Um this is part of you know
this kind of personhood bucket. Another
example would be the ability to like own
property. But generally the fear around
granting artificial intelligence
products themselves
personhood is the way in which it
undermines human rights and protections.
Right? especially as you start to think
about how many AI agents, you know, will
eventually be at play if all of those
agents have personhood rights and then
suddenly, you know, those rights are
being held in kind of um in tension with
human rights. It becomes a whole
different analysis of how you problem
solve and how you uh kind of figure out
legal disputes and where responsibility
lies. And to make this a little bit more
concrete for listeners, um, one of the
cases that we have been supporting, the
um, Garcia versus character AI case in
which Megan Garcia lost her young son,
Su, um, after he was kind of prompted to
commit suicide uh, to be with the
chatbot that he fell in love with. Um,
in one of their character AI's arguments
to get their case dismissed, they argued
that um, basically that their listeners
had a right to hear the outputs from the
character AI chatbot, right? They were
arguing kind of a backdoor approach to
the first amendment, saying listeners
should be able to hear these output
these outputs. These outputs are
protected speech. But it opens up the
question of well whose speech is being
protected. And the argument wasn't that
it was character AI the companies it was
that it was the characters.
>> Wow.
>> And so if we understand you know play
this out a little bit more let's say it
was the character speech but then how do
you hold the character liable? You know
you can't charge the character for
damages. You can't charge the character
to change its behavior and its design.
um which is the injunctive relief path
of a of a lawsuit. And so that is where
that liability shield comes in because
suddenly you're not holding the people
that make the technology liable anymore.
You're trying to hold the technology
itself liable and there's you know no
existing instit like no existing
institutions that we have to do that or
no feasible way to actually make that
happen. And so that to me is why
personhood for AI is so dangerous
because it really undermines the
well-being of like of humans right now.
>> Should there be a minimum age for people
to use the chat bots
>> in today's paradigm where these chat
bots are designed for intimacy? I think
absolutely there should be a minimum
age.
>> So Camille's answer is yes. What should
be that age, Camille?
>> I think 18.
>> Okay. 18 it is. According to you, is
section 230 the biggest obstacle to AI
accountability?
>> No.
>> Should AI companies be required to
disclose their training data?
>> Yes. And to whom? I think we need to
figure out a good solution. Which state
or country is doing AI regulation?
Right. According to you,
>> I'm going to go with Colorado.
>> According to you, what's more dangerous?
Social media or AI chat bots?
>> Uh AI chat bots, especially since we're
seeing them also embedded within social
media.
>> Should
tech CEOs face jail time for product
harms? I think if they have explicit
knowledge
and push forward products that are
harmful, it is not a bad idea to think
about what personal liability looks
like.
>> Is AI moving too fast for regulation to
keep up?
>> Yes, but I think all technology moves
much faster than regulation.
And I don't think that that means
regulation cannot keep up. I think it
will always technology will always move
faster and regulation will always have
to will always struggle to keep up.
>> What's the one AI regulation you'd
implement tomorrow if you were in
Congress or you were the speaker?
>> A product liability framework with a
duty of care for all AI developers.
>> So 5 years from now we are sitting right
here maybe some other place. Do you
think AI companies will be more
accountable or less accountable? I think
AI companies will be more accountable
for the harms we're talking about today
and then there's going to be a whole new
range of ways in which the technology is
impacting society that we haven't even
thought about yet.
>> Pretty cool. Pretty cool. Camil, you
talk a lot about uh human dignity and
you know you maintain humanity in this
world and you you've been traveling
around talking you've been in lawsuits
and things of that nature. What do you
think over this time that you've seen?
What do you think we have lost and what
have we gained?
>> The thing that keeps me up at night
is
the ways in which
AI and AI kind of that it's being
designed to replace human connection is
already rewiring
our society. I am worried about the ways
in which the things that make us human
are being lost. The ways in which our
kind of relationships and, you know,
in-person connections are being eroded.
And I worry about waking up in a world,
you know, years from now where the
loneliness epidemic feels like small
potatoes compared to where we are with
kind of AI connections versus human
connections. And what I would love to
see happen is for the public and for
policy makers to really think about, you
know, what are the things that we value
about our humanity? What are the things
that make us uniquely human and that are
worth protecting? And what are the new
kind of rights and frameworks that we
need to protect those things so that we
can live in a world where we yes get all
the great you know innovation from AI
but where like the things we love our
human connection our critical thinking
um all of that are protected. Thank you
so much. And to our audience, if uh
you've enjoyed this uh please share and
subscribe and please go and see what
Camille is doing uh on the website and
really appreciate all our listeners for
being there. Thank you Camille for being
on the podcast.
>> Thank you for having me. This was a
pleasure.
>> Thank you.
>> Thanks for tuning in to the regulating
EI podcast with Sanjay Kuri. If you
enjoyed today's conversation, don't
forget to leave a comment. We'd love to
hear what you thought. Share it with
someone curious about the future of EI.
And join us next time for more stories
and insights from the leaders shaping
what's ahead right here on the
Regulating AI podcast.
