when you let another country have access
uh not only critical inf infrastructure
but the information about millions of
your citizens you know you are
definitely taking a risk there and I
think the risk is much lower with the
US. So
>> welcome to the regulating AI [music]
podcast with Sanjay Puri. AI is changing
the world faster than rules can keep up.
So how do [music] we protect people
without killing progress? Each week,
Sanjay brings you inside conversations
with global leaders, [music] policy
makers, and innovators who are wrestling
with that exact question. So, if you're
curious about the future of technology
and how it's governed, you're in the
right place.
From video game developer to voice of
California's high desert, our
conversation today is with Jay Obernalt,
engineer, entrepreneur, pilot, and
member [music] of Congress. With a
master's in AI and a past lifebuilding
games at Farside [music] Studios, he
swapped code and controllers for
committees. Today he represents
California's 23rd [music] district in
the US House fighting to bring fiscal
discipline, techsavvy [music]
leadership and rural community grit to
Washington.
>> Digitization would it be in governance?
What kind of frameworks would you pick
on uh congressman specifically between
the US AEN relationship?
>> Sure. Well, I mean I think the nice
thing about AI is that it has the
potential to be beneficial in so many
different usage contexts. uh and because
it's so general purpose I mean I think
we can help in a lot of different areas
but I think the pitch is uh the US AI
stack I mean the US still uh not not to
denigrate other countries but I think
it's still undeniably true that the US
leads the world in AI development and
deployment uh and also I think that
being our partnersh comes along with a
lot of risk reduction and you know what
we have seen with some of our
competitors is that there are always
strings attached We we've found that
with Belt and Road, not to to pick on
anyone, right? But there's uh there's
always seems to be some strings attached
when you let another country have access
uh not only critical infra
infrastructure but the information about
millions of your citizens, you know, you
are definitely taking a risk there and I
think the risk is much lower with the
US. So I mean I think that's the pitch.
>> A lot of these countries are also, you
know, they're setting up a national AI
framework. They are dealing with the
same usual issue. You know, want to
build a great innovation stack but also
want to manage safety, security, ethics.
Are there any frameworks that you can
suggest to them how to balance this
innovation versus regulation or
innovation versus safety? Congressman,
>> what people don't want to hear is that
we're just not going to do AI because
we're terrified about what the risks
could be and we're going to forego all
the beneficial uh changes that AI could
bring. So we have to strike a balance
and it has to be a very uh cleareyed
balance between at the same time
uh protecting our citizens against the
risks of malicious use of AI and
balancing that against not wanting to
stifle innovation and entrepreneurialism
and the deployment of AI. And that's
what everyone is struggling with. Uh I
think what you what everyone saw in
Europe with the AI act uh is that you
know they they took a very heavy-handed
approach that I think is is already
really cost them. If you look at the
flight of not just investor capital but
uh technical talent out of Europe and
into places like the United States and
the UK uh that have taken different
approaches. We have that discussion
about that achieving that balance. Uh we
have to at the same time bring up what
the risks are. And it's interesting. I
was listening to the tail end of your
last uh forum there and one of the
things that was said that I really uh I
really strongly agree with is that
most of the illegal uses of AI are
already illegal. Not harmful, I should
say. Most of the the harmful uses of AI
are already illegal. I mean, we were
just talking a moment ago about cyber
theft. We don't need new rules and new
regulations to say that you can't use AI
to defraud someone and steal their
money. It's already illegal to do that.
What we do need uh is we need to make
sure our law enforcement agencies are
equipped with the tools and the
technology they need to detect this, to
track it down, and to catch the
perpetrators of that fraud. But we don't
need new laws that say it's illegal to
do that. And that's true in most of the
cases of malicious use of AI. And that
even goes down to uh some of the cases
that you would think are more
borderline. I'll give you one example
because we talk a lot about bias. And uh
I actually think it's uh it's a little
humorous when I hear people talk about
bias and AI because for me as a
technologist, AI is all about bias. You
know, you're asking when you're training
uh a neural network, you are biasing
that neural network and you're doing it
on intentionally and on purpose. So uh
we have to be very cautious when we use
the word bias. But when we use it, we
use it in to describe social phenomenon.
And one of the things that we're afraid
of is that the biases that are used uh
that that are exposed in the data that's
used to train the AI are going to be
reflected in highly consequential
decisions that the AI makes. But nothing
about that conversation should distract
from the fact that most of those biases
are already illegal. And I'll give you a
specific example. A few years ago, there
was a US company that was training an
algorithm to automate the screening of
résumés. And you probably remember this.
So, the idea was that if you have a job
opening and you post it and you get
10,000 résumés and you want to winnow
that down to the hundred that are going
to get call back from a human that are
going to determine then which 25 people
get the interview, right? So, that's
that's very beneficial use if you can do
that in a way that works. uh you know,
you're going to save a lot of human
productivity if you could make that
work. But in retrospect, we would
classify that as a highly consequential
decision-making process. In other words,
something that deserves extra scrutiny
when you're going to automate it because
of the consequences of that
decision-making on the people that are
involved. You know, someone that might
have gotten a job that doesn't get it
because AI filtered out their resume
improperly, right? So, so we we do worry
very much about bias in that
circumstance. But I mean this these were
early days and this wasn't quite
recognized at the time. So they
developed the algorithm. They went to go
test it and it uh exposed some very
troubling biases in the algorithm,
racial biases and those biases were
unintentional but they were baked into
the data that was used to train the
algorithm and the data you know the
algorithm sused out the wrong thing
which was to to filter on the basis of
race. So, uh, people were horrified by
this and rightfully so. But none of that
should distract from the fact that, you
know, number one, it's great that it was
unintentional. It's great that it was
caught in testing. It wasn't that
algorithm was never deployed. It was
corrected. We learned a lot about the
way that those biases can can filter
into algorithms through training data.
But most importantly, it is already
illegal
to discriminate on the basis of race and
hiring in any way, shape, or form. And
it does not matter if it's a human doing
it or if it's an algorithm doing it. And
the if that algorithm had been deployed
the moment those biases had been exposed
the algorithm would have been illegal to
use and people forget about that we
don't need I mean people think that well
we need to step in you know we need to
guard against these uh uh these
malicious these uh these negative
outcomes and the answer is no no we
already have those rules the framework
in place and so dealing like with AI
like it's brand new is not necessary in
most of those cases and when you tell
people this when we articulate It makes
people a lot more comfortable because
people think that AI is largely
unregulated right now and that is
absolutely not the case. We already
talked about the FAA and Nitsa and the
FAA and you know there are hundred
different examples already in US
government where regulatory bodies are
having to deal with the effects of AI.
Uh and so the the the perception that
it's unregulated is completely wrong. We
just need to make sure that we massage
that framework to make sure that the the
the harmful stuff is filtered out and
protected against while still enabling
the beneficial stuff and I think we'll
be fine.
>> You know, countries want to have their
own system that protects their data,
their languages. We we're talking about
obviously exporting the US AI stack. How
can those two things kind of come
together? uh when you look at you know
countries like Singapore or others they
have tremendous capability in AI they
have asan countries have languages that
go back hundreds and thousands of years
etc. So how do we kind of bring those
things together?
>> Right. No, I think it's a a great
question. Uh you know, and the answer is
that if if you look at the scientific
research, the most powerful and capable
AI is the models that have access to the
greatest amount of information, right?
Just just in an unqualified statement.
Uh and so as a worldwide community, we
want our most advanced models to have
access to everything. and we can have a
discussion about you know copyrighted
material and private data all that stuff
and you know those are appropriate
discussions to have
>> as these countries are maturing where do
you see some opportunities as far as
talent is concerned?
>> No that's a a great question u you know
I think that a worldwide meritocracy
where the best talent rises to uh our
distant ancestor you know just a few
generations ago that went through Ellis
Island or somewhere else. So you know we
we were all immigrants and I think that
uh I think that if we can get back to a
system of meritocracy like that the
whole worldwide community will benefit.
>> A little fun question couple of
countries have named uh a minister for
AI. Would you recommend for any of these
ASEAN countries to set up a minister for
AI?
Uh yes, but
uh regulating it in a medical device is
going to be different than regulating it
in a video game, right? So uh the the
danger of appointing a a an AI minister
is that then they will centralize the
regulation of AI within their ministry
and therefore accomplish the opposite of
what you're trying to accomplish.
>> So is it yes or no? for me. I I line
with yes, but I'd be very comfortable.
I'll give you an example. So, uh you saw
the president uh president Biden and
executive order uh creating the AI
safety institute.
>> Uh and President Trump uh uh undid that
because we can't use the word safety
because that's a Biden thing and
therefore it must be bad. Uh and now we
call it Casey, right? The center for AI,
what is it? Stability and advancement or
something like that, whatever the
acronym is. uh both both of those are
under NIST and it's very important to
make the fact the point that NIST is a
standard setting agency. It is a
nonregulatory agency and folks that is
very intentional uh that we're putting
the the we're empowering the agency to
create and standards and uh and manage
international cooperation in an agency
without the power to [music] regulate
because we do not want them to use that
authority to create regulations. We just
want them to create standards and I
think that that's appropriate [music]
and that's that's that's
a a way of solving that problem.
>> Standards not regulation. That's the
last word. Congressman, thank you so
much. Your insight, not just technical
but also from a legislative standpoint
really. Uh as the world is moving
towards AI policy, we need more people
like you. And I hope our audience
understands that there is a huge
opportunity for us and Asia to work
together. not from an economic
standpoint, but I think billions of
people will benefit from this. So,
please give a big round of applause to
Fox. [applause]
Thanks for tuning in to the Regulating
AI podcast [music] with Sanjay Puri. If
you enjoyed today's conversation, don't
forget to leave a comment. We'd love to
hear what you thought. Share it with
someone curious about the future of EI.
And join us next time for more stories
and insights from the leaders shaping
what's ahead right here on the
Regulating AI podcast.
