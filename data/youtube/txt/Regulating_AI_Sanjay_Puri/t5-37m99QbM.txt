so I think we'd be pretty similar to
most of the Anglo world we do have um as
a society Australia is prepared to be a
bit more interventionist than for
example the United States these new
techniques could make it into seconds
and so it just speeds up every little
bit and and and a piece I have made huge
numbers of mistakes I'm beginning to see
how all these patterns fit but then
forcing them to make mistakes with AI
and make and make that cycle of mistakes
I were going to have an AI regulation
mhm I would have a series of
principles that are very high level that
look like a human rights Charter or
whatever welcome to the regulating AI
podcast join host Sanjay pury as he
explores the dynamic and developing
world of artificial intelligence
governance each episode features deep
Dives with global leaders at the
Forefront of regulating AI responsibly
tackling the challenges using AI can
bring about head-on and enabling balance
without hindering
[Music]
Innovation welcome to regulating AI a
podcast that brings many diverse Global
voices to the discussion of creating
Democratic fair and Equitable AI
regulation around the world today we
have the privilege of interviewing
Professor Brian P Smith the winner of
the 2011 Nobel Prize in in physics and a
renowned astrophysicist from the
Australian National University Professor
Smith is most widely known for his
groundbreaking work on supernova gamma
reburst cosmology and the discovery of
cosmic acceleration Professor Smith has
served as a 12th Vice Chancellor and
president of the Australian National
University from 2016 to
2023 with his expertise and passion for
evidence informed policy Ed education
quality and Global sustainability
Professor Smith is an invaluable voice
in the discussion of regulating AI
welcome to the regulating AI podcast
Brian good nice to see you wonderful uh
Brian how do you see the current state
of AI regulation globally and what are
the key challenges we face in this uh
area well I think right now it's the
wild West we have a rapidly changing
technology that we can see doing more
and more
things uh we don't have a regulatory
environment uh tuned for it at all um
indeed people are sitting sort of
watching what
happens and I think that uh the way it's
being regulated is is essentially how
you regulate any company or person
there's a certain level of
responsibility but uh I think it's fair
enough to say it's uh
well out in front of any regulation at
this point and so yeah I think that uh
right now we're in the infase but we're
going to see this beginning to cause
lots of problems it's going to
ultimately I think it'll slow down AI uh
development in but potentially a huge
amount of harm could be done between now
and when you know the legal
ramifications of what's going on right
now actually make it way through courts
and uh becomes very expensive for I
think a lot of companies and individuals
would be my
guess so what you're saying is right now
it's basically self-regulation and it
will all come to a head um just uh
switching to where you are based what is
Australia's approach to AI regulation
Brian can you for our listeners and just
so that to set the perspective we have a
global base of listeners we have policy
makers congressman Senators their staff
policy uh Hawks we have uh tin tanks
industry leaders as our audience so tell
them where does Australia stand in terms
of AI
regulation so I think we'd be pretty
similar to most of the Anglo world we do
have um as a society Australia is
prepared to be a bit more
interventionist than for example the
United States so we have a position that
was
created uh four or five years ago called
the esafety Comm commissioner and this
is a a position that is is a government
position but it has a lot of
independence from the government of the
day and the that um e safety
commissioner is able to go in and
essentially put in injunctions go in and
call things out very publicly so that at
some level is the most sophisticated
tool we have it's a it is a
sophisticated office and they try to
make pretty reason able calls but
they're they're a very reactive group
right they're out there if they
see you know behavior on Facebook which
they think violates various parts of the
Australian rule book they can
intervene we have had you know in a
few places over the last five or seven
years our uh legislature through a
parliament try to to regulate but it's
you know I would describe it as grossly
misaligned with what's going on sort of
a box teching exercise that was out of
date the moment it was uh created so
those have sort of Fallen by the
wayside at present uh I would say there
is the regulation is is is done through
self-regulation and the threat of legal
sanctions where corporations individuals
are essentially forget that it's AI it's
just that they're a human or a
corporation and they have responsibility
on those basis and I think um it's
probably not a stupid way of doing it if
we get more sophisticated to understand
what the rules really are but that e
safety commissioner I think is is
something that's fairly novel here um
and it's it's uh Julian MC Grant's uh
the person who's in that position and
she's very effective um in the areas
that she is able to work but it's a
small group so it's you know big problem
small group so just to follow up on that
um Brian the esafety commissioner is
going to do something like this so two
questions one does that group have ai
level expertise and I'm not talking just
Technical Machine learning but you you
know what I'm talking about that's that
would be question
one so it's a pretty sophisticated group
so I think they I mean they're not going
to go out and do their own large
language model but uh they understand
and have the ability to talk for example
to my experts um at the University and
they come in and they we'll grab our
experts do work so they will get the
technical expertise that they they want
and and they're prepared to go and get
whatever they need or want so it's very
sophisticated that way okay that's
wonderful the other thing I wanted to
ask is like the EU has recently passed
the EU AI act in the US we've got over
100 bills that have worked their way
through Congress obviously we have
elections happening here so there's not
much going to happen but we have a
executive uh you know um order from the
White House is there anything this is
for our audience to know some anything
similar through the Australian
Parliament or anything that has kind of
come out uh uh or any possibility of
that or is it just self-regulation
basically so at this point there's
nothing as extensive as that um we've
had
interventions in media so there are
places where you see it where it's
obliquely so Australia had a a big
intervention in how Google Facebook uh
Etc deal with media and there are
aspects of AI in that and so that was a
very uh well it's a set of regulations
not very keen on uh very
interventionist but at this point I
think we're as a small jurisdiction
waiting and seeing what's going on on
the very big things like the EU and uh
the United States I would not be
surprised that we wouldn't uh
follow some hybrid of the European and
American uh approach uh that would be in
line with what Australia does but at
this point I do not know of any any
activity to do that
here one final point and then we'll
switch is in the US especially given
that we have elections coming up and
there are 60 countries with elections
there is a big concern about deep fakes
and those kinds of things intervening in
elections has anything like that
happened or any of that kind of concern
in Australia at
all there's certainly concerns and
indeed I talked a lot about this at a
national Press Club at the end of here
and uh there are concerns it's talked
about but then the question I would ask
is well and what are we doing about it
the answer is I don't see any activity
being actually done to
contemplate uh a very large amount of
new material coming uh you know
digitally over the next 12 18 months and
so uh you know with respect to our own
elections uh the security agencies are
looking and monitoring and so I think it
is being looked at and monitored I
haven't we haven't been the campaign of
a of a
international
um you know activities as near as I can
tell uh and so right now it's kind of
individuals misbehaving rather than any
Mass thing but would we be ready for a
mass Onslaught I don't think so I don't
see the pref preparation uh put in place
here okay um Brian given your background
in
astrophysics how do you think AI
Technologies could revolutionize
scientific discovery in your
field well so my Nobel Prize uh which
goes back into the 90s I used a a two-
layer neural network even back in like
1995
1996 to do um defect detection in my
pipeline because I had millions of
things to detect and you know doing it
by by hand was hard so that's a good
insight into where it's just a
no-brainer it can go through and lots of
things that we do right now as
scientists where we're just sort of
turning a crank trying to you know be
filters rather than actually
thinkers uh any place that then I think
AI is is already being used a lot and
will be used more and more and more and
more I think uh as it gets more
sophisticated uh and we can see that uh
it'll be interested you know science is
very creative and so the question is how
are we going to use it so for me we're
using AI to do interpolation so imagine
I have a
very computationally expensive model I
want to run but I need to run that
model um a lot of times so I I have
enough supercomputer power to run let's
say 10,000 of those
models but when I'm trying to fit a
model to data those 10,000 are not
nearly enough I need to be able to kind
of fit where the algorithm takes me
turns out AI can
interpolate amongst those
models extraordinarily well kind of
frighteningly well and that's a little
place where we're using it and it speeds
computation up by you know 10 to the six
and you can see it that it's the moral
equivalent if you look at uh in games
where they they do the ray tracing of
Reflections stuff you calculate it and
then you can interpolate with uh AI and
it essentially gets the answer right
good enough all the time so there's
things like that where it's very very
important where I don't think it's going
to be used um is
to you know come up with new Solutions
it will be very good at taking lots of
information and essentially applying the
principles that we already know so I
think the best best case in science the
most uh radical piece of new knowledge
was Alpha fold doing protein folding now
it you know Alpha fold didn't in my
opinion invent anything new there but it
took all the information we had and was
able through AI to come up with
Solutions we had never seen before so
it's not particularly creative but it is
it's a Brute Force way of solving things
that are really hard so those are I
think the most exciting things right now
but people will use it in ways I haven't
even thought about and that's the
creativity of
science just for our audience folks who
are not
astrophysicist would it imply more
discoveries would it mean that you know
let's say it took you a certain amount
of time for your Discovery to win Nobel
Prize would it compress time I'm trying
trying to uh put it for Layman like me
what does that mean uh
Brian so it will I think allow
scientists to make their discoveries
quicker because it just speeds
everything up so I'm gonna write code I
mean I I wrote 150 200,000 lines of code
for my Nobel prize Well now you're going
to have co-pilot or something helping
you write that code and it's going to be
a lot faster when I go through
and I do all these things I had to do
manually it's going to speed that up
when I want
to calculate as I said a a set of models
when I went through and had to calculate
for
example is the universe accelerating or
decelerating back in 1997 when we were
doing the calculations that took days
right and these new techniques could
make it into seconds and so it speeds up
every little bit and and and a piece so
that's the obvious bit the unobvious bit
is
then it allows you to do something you
just simply cannot do a breakthrough
like Alpha fold doing protein folding
those are the creative ones that I I
don't know what it's going to do in
advance I think it's very hard to do
those but it clearly can do it and
that's the place where you might just
simply be able to do new things make new
discoveries that are Beyond human on
their own uh and that's I guess the
exciting part but also a little bit of
the scary part where you know that's
that's the future it's hard to predict
yeah so speeding things up but uh there
are obviously challenges to that Brian
are there any lessons uh from past
regulatory Frameworks in other
scientific domains that you worked on
that could that we could learn uh when
it comes to shaping AI
policy well the probably the closest
thing um that we have is ethics
within um we have ethics committees
mainly to De with human-based research
so I'm an astronomer but I ran a
university so I know about that and it's
quite interesting to see what's on an
Ethics Committee you have ours is is led
by a philosopher and then you have
biologist you have psychologists you
have a range of very diverse Community
looking and understanding the human
effects of things we do including AI
would turn out so we're already
self-regulating AI through an ethical
framework uh now before ethics
committees were big you know people went
out and did experiments that we consider
now to be amoral immoral having
sometimes terrible consequences because
it was unregulated and now there was
self-regulation the individual human had
to make decision ision is this okay or
can I bring 40 undergraduates and you
know do shock tests on them depending on
what they're doing which we now know is
probably not okay uh without a lot of
bit around it so I actually think the
notion of an Ethics a really
sophisticated Ethics Committee and may I
say an Ethics Committee has absolute
Authority within the university that is
I is the president University cannot
overrule it um it and it it's a diverse
committee that do not have conflicts of
interest uh on the ideas and it's and
it's a sophisticated way of taking a set
of higher principles which is not doing
harm uh and so I think those will be
very useful for companies universities
corporations uh how does it then work
within government government well the
government for University says you're
self-regulating well enough through
these
committees that we're prepared for you
to do it this way now if that committee
gets something wrong the government will
come after the United St after the
university uh so I have every reason to
make sure that that that Ethics
Committee will uh support the university
if we were to be
legally um taken on by the government
and th far for us at least you know no
issues it's uh it's pretty it's it's
very sophisticated and uh you know I I
don't feel any threat about it not being
able to uh uh satisfy uh you know
government a court whatever so that
that's one Insight uh but I think uh
that works to a point but it's not going
to work um for Bad actors it's not going
to work for
individuals so you know there's going to
have to be a set of norms I think about
what's okay and what's not but it is I
think quite effective on corporations
and big
organizations so Brian incidentally
right now there is a big conversation
and a movement happening in large uh
Corporation let's say large Fortune 500
to set up ethics committees and ethics
boards in your uh case again for our
listeners is is your Ethics Committee
made up of University employees or are
these independent people from
outside in our case uh it's
largely uh people within the
university uh but it can bring people
from outside if it wants to um other
universities and I think one of the
things that's different about a whole
culture of
university is
a
university you know you have academic
freedom I cannot fire right no
matter that's not true in a Fortune 500
so it's the the setup we have is a
little different so if I were a Fortune
500 company uh I might have some
internal people who are on that but I
would definitely have it loaded with
externals just based on the um the
challenges of a true conflict of
interest because for a corporation this
could be existential making a decision
to do something or not do something
where we know within a university it
almost never is existential so it's a
little bit different so they could have
like they have these uh boards just a
regular board of directors they could
have people eminent people coming in as
you said philosophers Etc in uh their
organizations and they would have the
independence because generally as you
know in corporations if you have
internal staff there is a dotted line
somewhere and there are some pressure
points that could happen so that's a a
good perspective as uh we've had lot of
our guests come in and they talk about
these ethics boards so uh that's a very
there's a topic dour going on right now
in corporations in the US so that
Insight is very helpful um Brian in the
speech you mentioned earlier you also
talked about the potential danger of
learned helplessness and Humanity
becoming over reliant on AI for thinking
how can we firstly could you explain to
our listeners what do you mean by that
and then how do we strike a balance
between leveraging ai's capabilities you
know the things that you said can speed
things up and then preserving human
agency and critical thinking
skills yeah well this is uh I can it's
easy to talk about the problem hard to
actually figure out a problem is the
following um
if I look at how I learn and how
students learn and you know you know
right up from kindergarten on you learn
by doing right and learning to think
make mistakes so let's talk about
physics well I start doing physics
problems and I make mistakes and I learn
from my mistakes and I redo it and I
redo it and I try new things I've never
seen before and I break again and I
learn by essentially continually making
mistakes
mistakes uh and then after 15 years of
this I have made huge numbers of
mistakes I'm beginning to see how all
these patterns fit and I can start
potentially going through and then being
creative and saying well what I've
learned here in condensed matter physics
might well work in
gravitation now the idea uh thus far
I've seen no evidence that
uh the creative
aspect that AI can do at all right now
okay some people may disagree with me
but at this point I have seen no
evidence that AI can kind of have a leap
of thought like Einstein did when he
thought of general relativity that's
probably an extreme example so imagine
you start using chat GPT as many many
students do and instead of so you have
it just kind of help you all the time so
that instead of making mistake after
mistake after mistake it helps you get
the right answer much much faster right
so the problem I see is that you get the
answer faster but you're going to have a
much less as a human developed
understanding of
physics and as after 15 years of this
you are going to be much less
capable than of breaking new ground
when you get to the things that AI
cannot do so the AI can get you all the
way up literally will'll probably be
able to do to The Cutting Edge of
physics that we teach right now but by
its current design it's not going to do
the new
physics and humans are not going to be
trained to do the new physics either and
so we end up in a place where our
knowledge kind of just kind of ASM
tootes to where it is right now
because AI will do everything we know
incredibly well but it's not going to
push the boundaries in the same way that
humans do and so that's that learned
helplessness where we become helpless in
the future to do anything new because
we're so good at doing what we already
know now with AI and that is a real risk
how am I thinking about um I'm teaching
next year and trying to think about how
to get students to use things and and uh
I am what I'm trying to do is to put
people on the frontier of thinking with
AI much earlier but then forcing them to
make mistakes with AI and make and make
that cycle of
mistakes uh but you have to then put
people to the frontier much
faster uh where Ai and isn't going to
help them or get them to do problems
that AI is not yet figured out how to do
uh and so that
Frontier uh I'm learning exactly how I
can do reasonable problems that put
people in that cycle of learning by
doing rather than learning by
regurgitating which is what I think the
danger
is so learning by doing um not
regurgitate uh just uh two quick
questions uh on this point uh um we've
had lot of guests that come in and we've
raised this issue with them and they
always give
us the example of the calculator that
hey we used to count and you know figure
out how to do it and the calculator came
in and you know we didn't need to do all
these things uh whatever critical
thinking was involved in 2 + 2 or 8 * 9
or whatever the division the pi ET Etc
uh we are applying that thing to better
things I don't know your reaction to
that point yeah and so learning math so
let's just take times taes so do I do
everything by a cal the answer is I
still know my times taes really really
well and I still do mental arithmetic
all the time and the reason is is
because I can do things right now at
least I'm much faster in my head as
orders of magnitude in trying to think
through a problem so do I think kids
don't know need to know how to do these
things so the answer is no I actually
think you still need to know the your
your time table even though I have a
calculator no I use my calculator to do
almost all my multiplication but you
know sometimes I still do it just to
make sure I Know It uh but it's also
thinking through like how do you
calculate pi you know well do we so kids
need to know that too how do you
actually calculate a square
root uh I know how to calculate a square
root square root I use called Newton's
method you iterate through that's how my
calculator does it I can go down and I
can write that on the board right now I
learn that in in high school okay so
those things are still useful because
they help me and I use those techniques
to solve other problems at the edge of
astrophysics so you still need to know
you can't you can't just say I'm going
to go through and use my calculator to
do every order of magnitude thing
because it's slow and maybe a you know
some sort of implant that uh Elon Musk
puts into your brain and can speed the
bit up but I still think those things
are part of the wiring we need to
creatively solve new problems that's
that's why I think I'm worried about it
and yeah we're going to use it all the
time like a calculator but it doesn't
mean you don't actually have to slog
through and learn it still so we need to
slog to and learn it still uh final
Point uh you made a very important thing
I talked about the peak uh you know
reaching the peak for AI if we lose our
creative
thinking um that's a very important
Point
uh do you have a let's just say if we
stopped our critical thinking when do
you think the peak would happen is there
some kind of a timeline or something in
your
mind
well it's long so it's uncomfortably you
know it would take I mean people like me
exist I'm
57 actually I have people who are 20
years younger than me that exist who are
37 so it's going to it would take 40 50
60 years probably to come crashing but
it's going to slow down so you would you
would see slowdown very quickly because
the 18 20 year olds are very creative
normally yeah you know most people do a
lot of their best work like it or not
before they're 30 because they're you
know they don't know what they don't
know they're very creative and risky um
and and you would really kill I think
their creativity because they won't have
the tools to do it so you'd see a
Slowdown I think within a decade but you
wouldn't the machine wouldn't come
grinding to a halt think or wouldn't I
say an ASM toote I think you'd see that
in 50 years and I don't know it kind of
feels like a a dystopia of some um
future sci-fi movie where you know well
I think it's already been written right
it's just uh something where um Brave
New World or whatever where everything
is sort of in a status quo run by a
machine and that's kind of what it's
going to feel like is you know it might
be fine maybe it's maybe
maybe we would be better be in some sort
of sustainable equilibrium but uh yeah
it it would certainly be different than
the rest of humani History wow 40 50
years as you said maybe it's like a
Sci-Fi uh movie the reverse of Minority
Report or something like that who knows
uh
that's it's like it's like a Brave New
World or 1984 is is what it right you're
gonna have
you know in the end the machines will be
doing everything um because we will
become more and more helpless over time
that's the
challenge
wow yeah um Brian switching um just uh
getting your thoughts how can we make
sure that uh the AI systems are
developed and deployed where we uphold
ethical principles and protect
fundamental uh human rights by
you know in the development of these
systems so again I think we need to if I
were going to have an AI
regulation I would have a series of
principles that are very high level that
look like a human rights Charter or
whatever so it's a a set of principles
and we tell companies these are the
principles we expect you anything you do
Tech AI whatever you have to abide by
that and if you don't you are breaking
the law and we're going to come after
you and and I would just make it a high
a high level set of principles so if you
deploy things that have you know massive
racial biases in them that's just like
being a racist you are you're held up
that is against a principle and you were
responsible for the tech you deploy not
doing that uh and again you're going to
have ethics committees and you're going
to have to put in some stuff that we're
not used to right now to have confidence
that you're doing that uh but you know
we don't allow people to go out and kill
people on the street
um with you know without consequence
right now and just because you have an
AI driving vehicle you know I I think
everyone understands that if if
someone's responsible someone has
responsibility for that car if it runs a
pedestrian over and what's interesting
is I actually so I think it's going to
be a dilemma is let's take the AI
driving car right now if you can
demonstrate if you're let's say Tesla
that we are 10 times safer than a human
driver
well like it or not I as a human driver
when I hit someone I have
responsibility Tesla is not going to get
a free
pass even if they're 10 times better
than me they're going to be responsible
for every single person they hit and and
hurt or kill and there's no going around
that you don't you don't get through it
just because you're 10 times better and
and I I don't
see I think people get their themselves
into a quadry say well because they're
10 times better no responsibility is
had well that's not true right now if
it's a
genuine accident that's shown to be
beyond their control fine but I think
this is going
to uh start rearing its ugly head and
slow down AI because it turns out that
if you're going to do things
safely uh against these principles you
just can't go as fast as we're going
right now and you know what that doesn't
bother me I think going a little slower
and getting it right is fine I actually
think that's good I I don't think the
world the future of the world depends on
how fast billion dollar corporations
roll out their AI when it's deeply
flawed um I think uh the upside the
downside are are there's there's a fair
bit of symmetry in both directions so I
I think that responsibility is going to
have to be there but it cannot be a you
know 400 page document with specifics
it's going to have to be principles
aligned just like we deal with
humans so uh you rais a couple of
interesting questions if I may follow up
uh you said if you know Tesla cannot get
away if uh the car um kills someone so
the there's a big question and things
are now bubbling up as you know in the
United States we are a litiga society
and uh lawyers uh you know uh are going
to look at some of these things so if AI
does
harm who is going to be responsible the
builder of the systems the implementer
of the systems the user of the systems
all of the above or it
depends so I I think I mean I what's
going to happen in the United States
whoever the money will be blamed and
that's why we need some regulation that
comes in right so I think that like a
handgun so is Colt
45 you know or whatever is is is the
firearm manufacturer responsible for
every death that occurs with firearm no
it's the person who has agency to fire
the guy that's where the responsibility
lies
a class action could be made against the
firearm manufacturer if it's seen that
through recklessness of
design uh it's kind of built in the ease
of creating harms right so I think it's
going to be a very similar situation on
AI is the companies that create AI so
let's say a um a a fake um video
generator that does deep fak of um of
any okay so if you if I use that and I
do a legal activity clearly I'm
responsible but if Google has built that
and enabled lots of people to do illegal
things clearly they have a level of
responsibility as well so I think we're
gon to I think it's going to look very
very similar to what we do with other
bits of
Technology uh and I and I do think
overthinking about
it um isn't right
ultimately humans are going to be
responsible not the AI and when if we
start getting to the point where the AI
is responsible then it probably has
replaced us and at that point I'm
pulling you know I'm pulling the
electric cords out of the wall because I
figure it's over so humans have
responsibility uh and I would as I said
I I've given you the firearm I I think
it's going to be a very very similar
type of thing to
that so humans are responsible I'm just
going to very briefly mention a
situation to you that happened we had uh
in the US we have presidential elections
there were primaries in New Hampshire
and somebody uh did Robo calls with the
voice of President Biden saying you
don't need to um you know do vote for
the primary the general elections what
it counts uh the technology ol used was
by a company called 11 Labs which does
voice uh you know modulation Etc and
that was a company uh in Texas Etc so
now the challenge becomes is and uh
obviously the Federal Trade Commission
uh and the FCC are coming down on it so
it's going to be interesting to see
whether it's going to be 11 Labs the
company that did it the uh person that
uh uh you know did that I think we are
going to start seeing as said some of
these uh things that start coming in uh
Brian yeah and well clearly the persons
who did the fake calling they they
clearly have respons that that is
Criminal and to my mind no doubt about
it the question then is did um the
company in Texas did they create a piece
of
software where it was very clear that uh
it was trivially easy for large amounts
of people to do criminal activity with
it then I think they are culpable if
someone was really clever and took a
piece of software and modified it or
went you know showed real initiative and
cleverness then you know at some point I
I don't think they are responsible but
it it comes down to when is a gun
manufacturer responsible for something
versus the AI uh manufacturer and and uh
you know when things are new companies
tend to have more responsibility because
there's no societal Norm of what you can
do with a gun and then uh as I said I
think uh as we know we have a a voice
modulator well again what's the purpose
of that what is the what is the uh the
use case for that that's not illegal uh
is it a small bit of it is it a large
bit of it you know so I I you're right I
think it's it's going to be a very
complicated uh next couple years but I
would expect that a lot of companies
like that are going to find themselves
uh in deep trouble because they're going
to be seen relative to a sensible set of
principles they have not uh actually
acted uh as society would
expect you also very briefly touched on
a point that's uh deep concern
globally uh that like social media it
seems like a few technology companies
primarily Silicon Valley or others are
going to dominate the world of AI
because it is almost impossible for
smaller companies to create these large
language models it costs about a billion
dollar or at least hundreds of millions
dollars to create that is that a concern
to to you in terms of either Monopoly
like right now our uh global town square
is controlled by five or six companies
in terms of speech whether it's the
Google or Facebook or meta or X or
Twitter Etc does that concern
you well it it absolutely does uh but I
don't have an easy way to fix it so uh
these
are um you know let's let's say large
language mod model which are interesting
because of course they're
so fluent you know they're the fluency
of a large language model is you know
we've gone from what I saw computers
being hopeless at this stuff but
suddenly is like wow this is this this
really can write stuff not always
factually correct but the grammar and
that is wonderful so uh having a high
concentration and the reasons you need a
high concentration as you say is need
data on scale you need computation at
scale and then you need to have those
overheads of ethics and all these other
things that are hard so it's a naturally
highly concentrated thing but it's going
to actually start looking like a
sovereign capability so Singapore which
I know has spent a whole bunch of money
doing their own llm training exercise so
it's not a you know chat gp4 scale thing
but it was a big one where they limited
the
training um after obviously a a a set of
a basic training through the llms and
they concentrated on the
Singaporean uh data language Etc so they
had a sovereign capability here and it's
not that they're going to nail at the
first go but they are ensuring that they
have a sovereign capability in
llms so that if the big four or five or
whatever we want to call it go some
other way um they have some agency and
autonomy about about their future so
that tells me that there may need to be
a Public Square where there is literally
a open and I would say would have to be
open government
sponsored uh
resource that had computation and you
bid on it so you're going to treat it
just like a telescope I have to go and
write a proposal I it's very
competitive uh and I think creating a
capability uh that is in the Public
Square is going to become of national
importance and it's not going to just be
the US that does this it's going to be
Canada it's going to be France it's
going to be Germany it's going to be
Australia because they're too important
and that that's how I would deal with it
I think you need to create you can't
just let big Tech do it and you know
what it will also create creativity
because right now big big Tech does
things that make money right and and you
know just learning how to sell people
things has really really created some
interesting opportunity but it's also
really slowed things down right and uh
so I think I I that's how I would do it
I would I would create a national well
Observatory except call it a national
capability where um the the public uh
and researchers and highly skilled
people could go out and use it and
including small companies right so small
companies have the right to bid into
this as
well so uh you raised a couple of
interesting points if I may follow up um
one is uh in the United States for our
listeners there is a scientific research
body called the National Science
Foundation we've had the the director of
the National Science Foundation that has
come in and they are creating this what
you just said the Global I mean the
national research institutions where
academics and others can come in
providing compute power
Etc but the other point that you touched
on Brian is the sovereignty the national
sovereignty but it also has implications
we've talked to people from smaller
countries who have history who have
culture and they want to retain that and
they are saying that if these large llms
who kind of you know work on data on the
internet English language Western
Civilization or American civilization we
are small countries let's say in Europe
uh Etc should every country have their
own as you talked about you know uh
large language model in some shape or
form it doesn't have to be for uh chat
GPD 4 4.5 or five but just to retain
their culture values at
ET I think they're going to have to and
we're going to get very
sophisticated about you know
where there's commonality in these you
know so when you your your listeners a
lot of them will know that when you
start with an llm you don't just start
with a a a blank piece of paper these
days you you have one that's been
trained on lots of data that kind
of figures out the basic way that the
large language model and it turns out
that is quite Universal across languages
right so you can use that on Aboriginal
languages which are 65,000 years here
but obviously there's more distance so
they don't work quite as well as they do
between French and Italian which of
course are very similar so I I do think
like Singapore we're going to have to
those countries are going to have to
invest in a cap capability that
retains um their culture and history
because if they don't it will be lost
and I feel particularly bad for a
country like
Estonia that is Tiny and has a language
which is probably the
oddest of a small you know country in
the world sorry estonians but estonians
have a very funny language that doesn't
really Connect into too much else so you
know that's going to be hard work for
them to keep their language going
because it just is quite different than
uh than other things so uh but you know
what
researchers will figure out how to do
this because clearly humans can do it
and you know right now you know an llm
trains on a lot more data than you and I
have ever ever ingested in our lives and
that's what they need to do to get as
good as up but people will figure out
how to make that easier better and learn
contextually and stuff so but I think it
will be
important Brian you obviously are
teaching uh students you at the
University uh there have been all kinds
of reports whether it's IMF McKenzie Etc
in terms of massive job transformation
disruptions that are going to come 30%
60%
Etc you are now teaching this students
and obviously the one of the purposes is
that they go out and find employment
what are your thoughts firstly do you
think these transformation is going to
happen and
secondly what are your Solutions because
as someone who's teaching these students
you have I presume some sense of
responsibility to
them yeah so uh I do think a big
transformation is coming and so what I
always tell students
tell your listeners is if you can always
have the machine be subservient to you
then you become a
superhuman soon as you are subservient
to the machine and it takes your job
you're in trouble so I when I teach
students and our University teaches
students we are very much looking at
giving them capability of Technology
augmenting their capabilities not being
replaced now you know a&u is one of the
top 50 universities in the world however
you want to look at it the re the
reality
is you know and and this worries me is
that the most academically gifted and
privileged people come to my
university right and I am training them
to be
superhumans who can do 10 you know the
jobs of 10 other people so I'm not
really worried about my students because
we are training them to use this
technology I am
worried that not everyone gets to come
to my university and indeed 99% of
people don't and there will be Mass
job uh loss and you know it will
probably reequilibrate over the fullness
of time but you when we go through and
we think of the leites that was real the
leites a generation many generations of
people were really really knocked back
by that and they eventually got jobs but
you know that didn't mind you know if
you were the person born in
1835 that didn't matter because you
didn't get to learn how to use the new
types of machines you just you know had
a terrible miserable life so there's
going to have to be some sort of
societal intervention
uh what we do know economically is that
if you look the return on Capital has
just been going up and up and up so the
more Capital you have the more your
returns were the return on labor has
been going down down down so humans are
becoming less and less valuable and and
I I think that's going to continue and
we will break right we will break
because too many people are going to
feel
disenfranchised and going to have to be
some sort of
reallocation social
intervention and you know that's
something's going to have to happen I
don't know what it's going to be and
it's going to be very different between
the United States which is very much
you're on your own you know if you if
you haven't made it it's your own damn
fault and good luck you know get on with
it and Australia and Europe which are
very much we have a social safety net
let's think about how we can get people
on into a reasonable life uh but both of
us are going to have troubles in this
space and it's going to require Mass
societal intervention probably akin to
what happened in the 30s in the United
States so transformation listeners
according to Brian transformation is
coming and some kind of societal
intervention is going to be needed maybe
a hybrid between the US be on your own
take care of yourself versus Australia
Europe that you know provides a societal
safety net um Brian uh I could go on and
on there are so many other questions I
still have but I also value your time um
as a respected scientist and thought
leader what advice would you give to
policy makers and Regulators who are you
know struggling to come up with uh you
know framework for responsible
governance of AI especially Brian this
is a technology that is evolving so fast
I mean you know our lawmakers around the
world are not AI scientists but they
have to deal with chat GPT then
multimodal then Sora then agents super
agents uh what advice uh in conclusion
from someone uh so prestigious and
esteem would give to
them well I would put together a uh a a
group a a board of experts
from a quite a broad ranging things not
dis similar to an Ethics Committee but
you need to have industry players in
there you need to have velers you need
to have researchers you need to have
bureaucrats uh and then quite
frankly um you need to be taking advice
now politicians don't always follow
advice but they at least need to have a
core really good set of advice uh and
then I think another way to look and do
things things which is overlooked are
effectively the citizen jury way where
you can go through with this group bring
in members of the public explain to them
what they're doing and listen to the
average person on the street through
these citizen juries about what works
for them so you know experts and people
like me are great but it's got to
connect to the average person in the
street and so I think you need that
expertise and then a citizen jury type
thing that's what I would do uh and then
there's some hard decisions and as I
said I think you're going to end up
going against a set of
principles and then holding companies
and individuals against those set of
principles and the faster we can get to
a system like that I think the better
and those principles are beyond the
actual individual Technologies I think
most Technologies right now you can work
through that um but um as you said the
details do matter
so uh for our lawmakers who are
listening have uh group of experts have
citizen uh juries uh have a certain set
of principles that you can operate on uh
that's uh wonderful advice Brian uh
really want to express our gratitude for
you know sharing your invaluable
insights and perspectives on this
crucial issue of how to regulate AI um
you know as we deal with the challenges
your uh insights have really helped uh
you know give us some food for thought
and as we continue to navigate the
complex landscape of AI development and
deployment we want to have as many
divorced voices and that can foster
International cooperation and prioritize
ethical
principles uh we encourage our listeners
to continue exploring this crucial topic
and to AC actively participate in
shaping the future of AI regulation so
thank you for joining us on regulating
Ai and we look forward to continuing
this important conversation thank you so
much Brian for joining us from across
the globe it's really really been a
pleasure and like I said I have so many
other questions I wanted to ask you from
some of your comments some of your
papers but we'll have to have you again
thank you so much thank you so much and
I really appreciate the initiative
you're taking in this very very
important area
[Music]
[Music]
