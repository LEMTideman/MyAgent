I think that we need to make sure that
the control of AI doesn't sit within you
know two or three companies right
because if they put the regulations
inside that nobody else can do the
development of these models then they're
essentially going to be controlling the
tech
itself welcome to the regulating AI
podcast join host Sanjay pury as he
explores the dynamic and developing
world of artificial intelligence
governance each episode features deep
Dives with global leaders is at the
Forefront of regulating AI responsibly
tackling the challenges using AI can
bring about head-on and enabling balance
without hindering
Innovation welcome back to regulating AI
artificial intelligence AI stands at the
Forefront of technological Revolution so
how do we regulate it without stifling
Innovation our podcast aims to bring
diverse voices to the Forefront of AI
conversation by featuring insights from
various perspectives we've had industry
leaders to government officials to
advocacy group leaders today we are
thrilled to be joined by Professor Russ
salur now with us today he's the UPMC
professor of computer science in the
machine learning Department of the
school of computer science at carnegi
melon University he also got his PhD at
the University of Toronto and worked
with Jeffrey Hinton the touring Award
winner welcome Russ it's an absolute
pleasure to have you on the regulating
AI podcast thank you thank you for
having me Russ can you give our
listeners who are generally members of
congress senate think tanks their staff
industry leaders around the world just a
very uh brief idea of what do you what
are you doing at carneg especially
relating to AI yeah so I'm uh I'm a
professor at kelan University my
research I do work in AI specifically
focusing on deep learning algorithms uh
live scale optimization more recently
we've been looking at a topic of PL
language model Foundation models but
also focusing on AI agents you can think
of these as systems that can potentially
make autonomous decisions you know syn
of personal assistance for example so
that sort of the sttion of of of of
different research that I'm doing at at
well that's a pretty wide portfolio you
have all the Hot Topics that are going
on since you talked about the agents the
super agents you know what some people
think that there is a big risk towards
that what in your view are the biggest
risk from AI that need to be regulated
uh yeah so it's definitely very sort of
toic there's a lot of discussion
happening in research community so I you
know I have to say that I'm not really
sort of that much with regulations and
what's happening in that Stace but when
we think about researches and what we do
there's certainly a few risks that can
uh that that come up you know there are
broadly perhaps sort of three different
areas at three different risks that I
can think of one which concerns me the
most is you know the potential spread of
disinformation so the AI technology is
becoming pretty good to the point where
you know we've seen just examples of
Sora just coming up out of open ey a few
days ago the ability to generate
realistic looking videos and so I think
in the near future it's you know spread
of misinformation or you know trying to
do manipulation of your opinion you know
I can have a like an AI assistant or an
AI agent that can convince you of
certain things you know I've seen these
examples where um you know uh one of the
early versions of GPT would actually
give pretty good arguments why recycling
is bad and so if you look at the
arguments you know you could all you
know part of time were fake but they
look very convincing right and so that's
probably one of the bigger risks that I
see sort of in in the shorter term I
mean there's obviously sort of you know
other risks such as uh obviously the
development of AI in military military
applications you know that's probably
inevitable that's going to happen uh it
also sort of Brad potential you know if
we look at 10 20 years from now is AI
going to be replacing some of or some of
the labor they done by humans and what
would it mean for for economic
development what would it mean for the
economy so there's a lot of research
right now happening in a space and not
just AI virtual AI agents like chat GPT
or but also in a space of actually
physical building physical robots robots
that can do you know tasks and so you
know once that sort of takes of then you
know there's probably going to be some
form of job displacement right and
that's you know something that I think
my Economist friends are thinking about
so Russ you talked about uh some very
important topics if I can you know
follow up a little bit one is you talked
about the disinformation and we and then
you talked about obviously the job
displacement and then you know a range
of those things as far as disinformation
is concerned and you particularly talked
about which open I released are we
getting to the point where everything is
fake and only few things that we can say
with Shorty and certainty that it is
real people will have to kind of at some
point say this is real I mean are we
getting to that point uh that's what is
woring now the Biden administrations
talked about watermarking they even
started talking about using you know
blockchain technology Etc so give our
listeners special this is an election
year not just in the United States 60
countries are going to have are are
having election so as somebody who is an
expert in this what should somebody be
believing as far as that information is
concerned so just one one topic and then
I'll come to the employment issue yes so
so uh very good question I think that I
personally think that we still a little
bit far from actually you know having a
technology that's so real that you
cannot you know separate what's real
what's not real you know if you you look
at examples of Sora there're still
missing certain key aspects like object
persistency so you know you have a video
and then one person would just disappear
but again those things eventually will
get fixed and I do think that at some
point in the future I don't know when
you know it going to be very hard for us
to trust what we're seeing online right
and so uh with deep fakes and uh and
everything sort of coming together it's
you know like having a really good
Photoshop but not just the images but
the whole video and somebody in the
future I think you and I would not have
this conversation but our avatars would
have this conversation and nobody would
just tell the difference and so that's
obviously you know there's a lot of work
on you know how do we do Watermark how
do we know that what is generated by I
versus what is actually generated by
humans or what's real so obviously
there's probably going to have to have
some form of Regulation particularly in
in face once it is consumers and if I'm
a consumer start you know generating
videos andu manipulating you know then
obviously that technology will need to
be some form of at consum signment
people are you know using this
technology for something else that that
needs to be probably have some form of
Regulation or at least some way of
saying that that was generated by AI
system and so there's a lot of uh
thinking right now in the research
community of figuring out you know what
what the solution is going to be here so
there are some bills already in Congress
us relating to that and you will have
something thing coming through the
concern obviously is that watermarking
is not in 100% foolproof technology and
then I don't know uh with text to video
or text to image what can you do I mean
you are the expert are there
technological innovations now you're
saying it is still not there but keep in
mind open AI announced Soro mid Journey
immediately said we're going to have
something better and Google's probably
going to have something absolutely
absolutely I mean that technology is
going to be evolving there is no
question about it right and so we're not
there yet but eventually we'll get there
I have no I have no doubt but in terms
of you you've asked about image
generation how we can do it that's
that's a very good question I don't know
I don't know whether there needs to be
some form of system or some form of
Regulation that will basically stay that
would basically say whenever you
generate should generate new if it's a
watermark or specific standard that says
this thing is generated by a just as a
way to saying what is something that
generated by AI versus what something
that's not generated by a and we s to
run into this problem right now there
was a very fun example funny example
where open AI GPT generates text and and
that text shows up on the web you know
it's no longer you know human generated
text it's AI generated text and there
was at one point if you've seen that on
X the platform on Twitter on the X
platform uh they have their own
assistant called Gro and there was one
point in time where GR Brock would say
oh I cannot answer that question because
my open AI license doesn't allow me to
or my open AI sort of guard rails do not
allow me to answer that question and
people started making fun why is one
platform saying that you know I can't
give you the answer because open the ey
why is this agreement doesn't let me and
so what's actually happening is that
text is generated and these other
systems take the text from the web
because they all call the web they all
getting that information and using that
to train their models and so now it's
some point you know differentiating what
was generated by I versus what was not
is going to be you know you basically
can't tell the difference and that's
something that's potentially we need to
think about you know what we need to do
in this case some trial balloons have
been floated even the Administration has
talked about the use of blockchain for
something like that as somebody who is
working diligently is that again just
very briefly uh is that a solution I
don't know we're definitely going to
have some form of solution going forward
because once we move into the next sort
of evolution of these large language
models Foundation models which is going
to be agents we think of them as you
know your personal assistant that will
do tasks for you you know you have an
issue with your phone you just ask your
agent go fix my phone or you try to book
an appointment with your dentist you
just tell the agent hey call my dentist
just book an appointment for me right
and so we're going to have systems that
will make decisions and you know help
you at which point the use of personal
data is going to become very important
they just will know everything about
right just much like Google and other
systems know every time you search on
something online they know exactly what
you're trying to do so and at that point
I think that the technology like you're
saying would it be blockchain or
something else would would become very
important because I would want my
personal data to be protected and so as
we kind of you know moving in that stage
I think that's something to think about
and I don't know what the right answer
is but these systems these AI systems
they very much data hungry right and so
everything that we see today including
Sora and GPT and Gemini everything is
relying on large volumes of data and so
data is sort of probably the most
important part of all of these systems
and obviously when it comes down to
using my personal data that's when
technology like blockchain or something
else will need to be developed and we
need to be ensure that you know my data
stays private Russ you talked about and
I want to still come to the employment
issue but on the agents we've had some
guests who have raised some serious
concerns about some of these agents
super agents as they said that they
could be out of control or they could be
running us where the humans would become
one of our guests who is a biological
ethicist said we would become
subservient to the AI system is that a
fear in your mind at all I actually
don't think so you know it's very hard
to I mean obviously very hard to
extrapolate and see what's going to
happen you know 40 years 50 years but
the way that these systems work today
they still under the control of humans
so the way that the agents are working
today even though they don't work very
well so that's why con active area of
research is where I give my agent a goal
you know particular goal to achieve and
the agent has to figure out what sort of
steps it needs to take to achieve the
goal that these agents don't really come
up with their own goals they still sort
of you know the objectives is something
up we we we set up by by uh objectives
are being set up by humans and so I
don't think it's sort of I don't think
we'll get to that point where these
agents will take over but I think what
will happen in the future is that these
agents are going to be so useful to us
that we're going to be heavily relying
on them to sort of help us with their
daily tasks and then there is sort of a
fine line as to how much you rely on
something versus how much you start
thinking for yourself right so at some
point you know once we hit that that
gray area then we'll have to start
thinking fairly carefully like you know
what do we do in this case much like
today you know when I drive and I try to
go somewhere I don't even think how to
get there I just use Google or Apple to
tell me how to get there and if Google
tells me to go around and make two
circles before I get there I'll do that
that's true there's obviously concern
sort of going forward that as that
technology matures and you know you're
going to start relying on it more and
more and that's sort of one of the
problems and this is again we come to
regulation will need to be addressed is
as these systems become better and
better you know let's say if we take
medical domain doctors and you know a
lot of people are saying well gp4 and I
think the next evolution of these agents
going to know so much about especially
if you give it so much data across
different patients they potentially can
be better than your family doctor and I
don't rule that out the issue is that if
that ever becomes the case then doctors
will start relying on the tech and at
some point what the fine line of me
relying on my own decisions versus me
relying on what AI is telling and this
is where I think that in these domains
we need some very good regulations
because they're very sensitive and very
sort of area where you can't really make
mistakes Russ just one final point on
agents because some of our guests did
bring this up are agents going to
interact with other agents absolutely I
see the future where you know I can give
you example very simple example where I
would like to have an agent my phone
stopped working I was traveling overseas
my phone stopped working I'm calling
AT&T I'm spending an hour with the
customer support and doing things like
rebooting my phone then checking
something I see the future where I can
just ask my agent figure out why my
phone is not working and this agent is
going to call AT&T agent and AT&T's
agent going to talk to my agent figure
out what's the issue how we can resolve
it what's the situation they can't
resolve it that going to go to the human
to resolve but I do see the future where
agents will start communicating with
each other to solve you know specific
objectives that we give them but you
don't see any D you don't see any danger
with that obviously I mean I do I do see
the danger in the sense that we'll still
need to develop systems and that's part
of the research that we are doing in in
Community Systems that can essentially
prevent Agents from taking dangerous
actions or making sort of you know there
is sort of a fun thing which is you know
my phone is not working maybe the agent
will figure out the best way to fix my
phone is to hack an T and fix my phone
and if that obviously that's the sort of
the hypothetical situation but if I give
it the objective my phone is fixed and
so there obviously need to be some guard
rails that we need to put into these
systems and this is activated research
and it's very hard to do it right now in
such a way that when these agents take
actions and make decisions you know
there's certain safety protocols in
place that would basically not allow
these agents to do something malicious
we addressed one of the concerns that
you had the other concern you raised was
about loss of jobs there have been all
kinds of reports r i mean you know you
can take the IMF you take Mackenzie
there will be job losses of 30% up to
60% but a major transformation is
underway that I think everybody agrees
we've had education experts come here
also I mean you're a professor yourself
but people who focus on just this issue
they've talked about reskilling they've
talked about past changes that people
always geared towards more but the
people who will end up losing their job
if we talk to them it's always a very
hard thing and when you're talking about
numbers like 30 to 60% and these are
white collar jobs we talk about customer
service Representatives you're talking
about marketing you're talking about
coding what are your thoughts on this
Russ I have spoke to a few people about
this a few fairly you know smart people
that I respect about you know what the
future might be it's still uncertain but
we have to be prepared for the case that
as thei becomes better and better
there's probably going to be some job
display as the AI physical AI evolves
right we'll have robots in our homes
helping us out you know obviously
there's going to be some job displayed
you know there's always pros and cons
the pros is that you know that
technology is going to be evolving and
will help people will Empower people but
yes at the same time there's going to be
some job displacement and I think that
the main issue that or the main argument
that I see from other people is that
look technology advances are always good
because you know our society evolves but
if it takes a generation or two
generations then we can adapt because
you know we will we'll get education you
know we we you know people can you know
get the skills necessary to to adapt the
the worry is that if that technology
evolves very quickly within 10 to 20
years within a single generation then
that becomes problematic right right
just like exactly like you're saying you
know I we going to be displacing
customer service Representatives there's
a lot of which probably will happen and
that's something that's you know I don't
know how to uh avoid because you know we
we sort of felt about if you look for
example at coding right now today a lot
of my friends use chat GPT for coding
and you know professional coders are
still fine because professional coders
they know exactly what you need to do
they have teams of people when they
build something but sort of you know
other coders they becoming more
efficient and as you're becoming more
efficient then as a company I don't need
10 people I I I might as well just have
five people right they can accomplish
the job that I need to accomplish so I'm
becoming more efficient the company
becomes more efficient but then five
people just lost the job I don't know
what to do in this case I will tell I
don't think we're at the state at this
point where there's a massive job
displacement because of AI but we have
to be prepared if we look at you know 10
20 30 years from now but don't think
Russ it might be sooner than that
because you know law firms I mean you
see small number of you know a company
like dualingo let a bunch of contractors
go I think dingo is in Pittsburgh if I'm
not mistaken yes and they let a bunch of
people go because they said AI the
contractors go because AI could do the
job UPS now I don't know if AI was the
reason but I I can rattle off names of
companies that it seems like the trend
has started
and obviously there's a political
sensitivity of saying certain things but
my concern is as you said these
programmers these customer service
agents it's going to be a challenge now
obviously we'll have to figure this out
and I hope uh that we can figure it out
sooner because I don't think it'll be 10
to 20 years uh that's a fair point that
you know a lot of like companies like
doolingo uh you know they fire or they
let a lot of contractors go because of
translation machine translation
translating from one language to another
and and machine translation systems are
getting to the point where they're
better than humans to do translation and
so there's certain Pockets where we see
that Trend yes I agree I still think
it's you know if you're thinking about
replacing you know Human Resources so if
you think about customer service
Representatives we're still not there to
the point where AI can just take over
the customer service and just completely
replace because AI right now it's sort
of good but it can really it's one of
those things when when I was working at
Apple you know Apple has customer
service they were using AI to help the
customer service Representatives but
they would never replace the human just
because there are certain sort of these
small probability events sort of rare
events where something happens where
only human can address the issue right
and so I agree with you that it's the
trend is starting but I not observing
like a massive displacement at this
point but I agree with you I think
that's something that needs to be you
know especially as AI comp as these big
companies become using more and more Ai
and the job replacement is yeah one of
the topics that we have to figure out I
don't know how to because AI continues
to to be evolved and there's so much
research happening in that space so Russ
you just uh talked about that AI is
evolving one of our guests said AI is
like an amoeba it keeps changing as you
know we had large language morals than
we we had multimodal we have agents now
we have text to video we have people
talking about AGI how do uh lawmakers
who are not AI experts they're trying to
craft a law based on the facts today
saying okay we're going to protect such
and such and such we're going to put
some guard rails but 6 months down your
research comes up with something I don't
know AGI or even Beyond AGI plus how do
you kind of evolve those regulations
that's the concern that is there on
Capital Hill yes yes that's a very good
question the short answer is I don't
know you're right that AI evolves fairly
quickly I mean once so for example when
Sora came out I was very impressed you
know it's like you look at it it's like
okay it's looks very real my estimation
was that it's going to take us maybe a
few years to get to that point and when
I look at the Technic aspects of what
they've done the technology is the same
so the technology hasn't changed what
has changed is the ability to consume a
lot of data and having the
infrastructure to have these systems
right and so with the data and gpus and
you know Computing that we have you know
we'll see how far we can go but in terms
of the lawmakers and trying to craft you
know the regulations and such that's
it's a very interesting topic because on
one hand you don't want to have regul
that can stop the progress stop the
research so you know there's been a few
sort of things where people will say
well let's actually pause the research
or like development and that's that
would be a bad idea because other
countries Russia China they're going to
continue evolving right they they in
assistance and so and so it's important
for research to kind of you know to
continue but but it is important perhaps
for the lawmakers and Regulators to try
to think about how do we regulate and
products so if I'm developing a system
that's going to go and become a medical
AI assistant probably there needs to be
some regulation in terms of the quality
control or if I'm going to be using it
in self-driving car there's obviously
regulations where I can't just take a
model stick in my car and start driving
around because I may you know have
accidents that you know we cannot have
that so but I think that the best way
for the lawmakers in the government is
to perhaps be a little bit more embedded
into AI Community into the research and
see where the field is moving how it's
progressing we at seem you have an
organization that specifically works
with the local governments with the
Pennsylvania in particular to try to you
know they come and they talk to us they
talk to professors who actually
specialize in that field so that we can
anticipate a little bit forward of what
might happen you know in the future of
course it's very hard to predict what's
going to happen in five years but that's
the best strategy for the governments
and for the lawmakers to somehow perhaps
be a little bit more technical and
perhaps be a little bit more people who
work with the government to be a little
bit more involved with industry as well
as with universities yeah I think it's a
very good point that the government
needs to be involved more with Academia
and Industry now just to take that thing
forward the industry at least some of
the industry has been all over Capital
Hill and some of them have said hey we
want regulations are you concerned that
you know that these companies want regul
capture in some ways so that somebody
after them coming in has a real hard
time and also think about it to build a
large language model now obviously
prices have come down Etc but the cost
of gpus I mean nvidia's market cap just
touched 2 trillion because all these big
guys are buying up gpus it's a war to
find gpus right now so my question is
you know somebody in a garage in Pitts
can't just wake up and say okay I want
to build a light language model so are
you concerned a little bit about
Monopoly of the big tech companies yes I
am I am concerned about sort of you know
and this is both on the regulation side
and the open source side so I think that
we need to make sure that the control of
AI doesn't sit within you know two or
three companies because you know if they
put the regulations inside that nobody
else can you know do the development of
these models then they essentially going
to be controlling the tech itself and I
don't think that's very good it's not
very good for society it's not very good
for future development of these systems
right because a lot of times you know
used to be that for example Google and
open AI would publish what they doing in
the research and now it's basically
getting to the point it's becoming very
closed right nobody knows how they're
building these models what data they
using and you know things of that sort I
am worried about this and that's where I
think the role of Academia and the role
of you know open source Community is
going to be fairly important because
part of it is also for us to be able to
study these models the biggest issue
that we have right now is that you have
these powerful models like Gemini and
GPT and it's for academic Community it's
very hard for us to study and understand
when do these models fail how we can
build the guard rails what can we do to
improve these models what kind of data
was used to train these models so that
we can improve both the quality control
and sort of try to build models that are
safer that are more robust and such and
so there is sort of an open source
Community that's trying to replicate
these system and of course the open
source is lagging behind just because it
requires a massive amount of compute and
you know massive resources to to build
these moments but I think that you know
open source will will catch up but yes
but that that is one of my one of my
concerns for sure because I don't want
you know we're doing the work on AI
agents we want to be able to see when
they fail we want to be able to see this
is dangerous we shouldn't do that or
this is you know this is where we need
to study these models and if these
models are seeking closed form closed
box behind the wall in Google or open
the ey or whoever uh these big tech
companies then it's it's not good for I
think for for us as a society so uh fair
to assume that you are for open source
because there's a big debate uh you
close Source you are for open source I
am definitely for open source because I
think that so so the thing you have to
understand is that when we look at these
models like like language model like gp4
or you know Sora or Gemini
or anything else is the technology
itself the algorithms the model
architectures these are pretty well
known we all know what GPT what
architecture is used it's using
transform architecture with you know
specific and and what what's different
between these different models is
probably the data curation of the data
opening I spend a lot of money a lot of
time curating the data labeling the data
and that's sort of that's the
differentiator right and compute the
buil to train these very large models so
it's not like open AI has a model that
nobody knows about and nobody
understands what it is and somehow does
this magic thing we all know what it is
it's just a matter of scaling it up and
compute and so for open source it's not
like you're revealing specific secrets
and and I think that the evolution of of
Open Source again to the point where we
can study these models we can understand
when they fail we can you know you know
we can say well it's probably not safe
to use this model in these applications
because of you know these reasons
whereas if only few companies control it
well they they don't have to be
accountable right they don't have to be
held accountable can just they can just
deploy it and well if it messes up well
you know we'll fix it internally but
nobody knows about it so Russ you talked
about you know that open AI has taken a
lot of uh data in there and that's how
they've been successful so how do we
balance privacy and you
when it comes to data which is used to
train AI models that's a very good
question I don't know what the right
answer is here right you know I can give
an example let's say in the medical
domain just back the medical domain data
privacy is really really important there
right I don't want my records to be
released in somebody else seeing them on
the other hand if we want to build AI
that truly can actually be better than
potentially be better than doctors like
can at least give give suggestions to
the doctors it needs a lot of dat data
and ultimately in ideal scenario AI
would see everybody's data hundreds of
millions of people with all different
conditions and sort of understand the
data understand and you know and that
would help us going forward right but
that requires data and so where's this
boundary of AI consuming the data our
private data versus usefulness for for
society in general I don't know what the
right balances that's something that I
think that we'll have to think about as
a community obviously We There are sort
of you know algorithms some something
called Federated learning or privacy
preserving Lear learning systems that
are trying to address this issue where
you can use my data but it's encoded in
such a way that it's never revealed to
anyone else so the model can still use
the data to train itself to train its AI
but it's sort of not going to be able to
you know identify that that's my data
right so there's some technologies that
people are trying to develop but it's
still it's a very sensitive topic right
and there's a lot of there's a lot of
you know people on both sides because
you know we seen this example of open AI
in New York Times yesterday Reddit had a
60 million doll contract Google she say
Google Reddit data you know and so this
is sort of data is really one of the
most important cornerstones of of of AI
systems today and so how do we do this
balance that's a very good uh question I
think it's also the question for The
Regulators I mean that's something that
economists you know social scientists AI
research will have to think about and
figure out what the right steps uh
should be and I I seeing diverse
opinions from you know saying well if
you're using somebody else's data and
you're not crediting giving it back to
you know the person whose data you've
used that sounds unethical but that's
what a lot of existing systems do
company like open Ai and Google they
just mine all this data and a lot of
data is the data that you've provided
right data that we provided uh we had uh
the head of the copyright Association
here the perspective is that when you
ingest training data attribution is
important to whose data you have and in
some cases compensation also do you
agree with that I think so you know it's
always it's always the case that if I
work really hard and I've created a data
that's useful obviously there has to be
some form of data attribution and
beginning to see that with redit and and
Google and so it's just that you know
two three years ago people didn't really
think that these models would work so
well in open AI show that they do but
even in cases like Google for example
right the way that Google scrapes your
data you know you can potentially put
like something I think it's called
robot. robot. text yeah you can say I
don't want you to scrape my data and it
you know Google will say okay I'm not
going to index it I'm not going to touch
it perhaps we should have something like
that for AI systems as well or or you
know you know if Ani System is using my
data they should be data attribution I I
fully agree with that with that view uh
some people might not agree but in
obviously there is sort of again as I as
I've pointed there is sort of this thing
where if nobody shares the data we're
not going to have beneficial AI that can
benefit to the whole Humanity right so
it's sort of this balance where if I'm
happy to contribute my data knowing that
you know there's going to be an AI
system that will help others maybe I
will contribute my data without
attribution but but I think there should
be some laws in place in some forms of
you're using my data for training these
models obviously they should be a data
attribution you talked uh just now that
you know there should be collaboration
do you see what role do you see for
interdisciplinary collaboration uh in
fields you know whether it's law ethics
social sciences I definitely think there
should be a lot of collaboration between
different fields because that's the
technology that's going to go beyond
just machine learning or optimization is
what we do once it's sort of starts
reaching out to you know if it starts
impacting Society in general through the
jobs or through you know Assistance or
whatever in in whatever way then I think
that we definitely need to have false
collaboration with economists social
scientists with you know people who work
in ethics and and I know it's seemu for
example Even in our machine learning
Department we have a few faculty working
in Fairly different areas we have a few
faculty working social kind of aspects
of AI we have faculty who just work work
on you know Foundation models and
optimization we have faculty who work on
with economists and we have people
working with with health and sort of
Public Health and trying to understand
how that technology is going to be
deployed so I think it's it's it's
important because the Technologies is
goes beyond just sort of algorithms just
machine learn because it's beginning to
impact Society in general so I think
that's you know it's it's going to be
very important we haven't gotten to this
point yet because we still have machine
learning we have our own conferences we
have our own researches we talk to each
other but we don't very frequently go
and talk to social scientists or to
economists but I think that will need to
change going forward and I think AI is
is where it's going right now it's
inevitable for us to start talking to
across these different disciplines and
try to understand the concerns of you
know other disciplines in that in that
PA Russ you work on Foundation models
and agents and things of that nature
some of our guests have talked that
it is very important that we get
cultural and language perspectives of
beyond the Western World you know
English as well as Western world because
there is culture Heritage History
language how important is that in your
view I think that's going to be very
important and I think that's already
happening now to the point where you see
each region is trying to build its own
GPT so we're seeing this in Europe we we
seeing this in in Asia both in you know
in the Middle East and also in Asia you
have China has its own efforts Middle
East has its own efforts Europe has its
own efforts and part of it is because
exactly as you're saying once that
technology starts maturing you obviously
need to ingest the cultural values or
whatever the Norms you have for you know
your country or for your and I think
that's going to be very important it's
definitely you seeing you I'm seeing
that right now just because there's a
few startups and then a few efforts sort
of happens at the region level not just
in the US and part of it is you know was
just for your listeners there was uh at
some point there was this joke that
happened with gbt 4 which sort of
highlights the nature of sort of the
culture where if you ask gbt 4 for you
know a question it would answer but if
you ask the question and then add to the
prompt you know I'll tip you with $100
it gives you much better answer if it if
you tell tip you with 10 do it gives you
much shorter answer and if you don't tip
it it gives you very concise so this is
sort of you
know people well that's just the culture
The Tipping culture in in the US
probably it's what it's doing because
what it's doing is actually looking at
conversations or looking at the data
that people post online and when we
converse online we sort of say well I
over tip this person or you know and
that's something that the model learns
because it's very data data Centric and
so if you if you're training your model
on a different type of data that
reflects the cultural value values of of
your country or your region that's the
AI That's you know going to have that
and so I think I think it is I think
it's GNA be very important absolutely
Russ um you obviously talked about the
cultural Etc do you think Global
collaboration is important because right
now you have ai nationalism Jensen Hong
of Nvidia said every country should have
their AI everybody is now you know if
you it's like the nuclear Club everybody
if you don't have an llm you're not uh
there but how about collaboration you
has done the AI act us is trying to work
there China is doing its own thing what
is what are your thoughts on that I you
know it's it's a very good question I I
don't know as a research scientist as a
researcher I think collaboration is
important because the way that you know
if you look at how this technology
evolved it didn't evolve from Google or
open AI building the technology and here
it is it came out of Decades of research
international research you know when we
develop neural networks when we develop
deep learning algorithms then you know
there's something called attention that
was was developed in in Canada actually
then you know the transform
architectures came up which was
developed in Google at that time and so
it's sort of a combination of different
things that's you know um it's very hard
to develop AI on your your own and so I
think that if we're just going to have
ai in the US and close everybody else
we're not going to go we're not going to
get that far and so I think that
collaboration is going to be very
important and that's how research right
now functions the research right now
functions as um you know we have
international conferences we have people
doing research in Europe in Asia in uh
in Middle East in Africa in Latin
America in the US I mean us is obviously
dominates you know the research happens
Broad uh it's very much International
and I think that when it comes down to
these systems you know we all going to
be better off if the same happens you
know across now of course there are
political constraints to that you know
obviously you know with the US and China
tensions you know obviously there's
going to be some you know saying well
maybe we should not right and so you
know in the perfect world I think we
should but obviously you know there are
other other things in play that may
prevent us from you know from having
having this collabor collaborations but
I do think as the technology evolves and
becomes more mature I think
collaboration is going to be very
important just closing in one ecosystem
is I don't think it's a good idea but
again I'm coming it from the research
perspective because again the technology
that we have right now was not developed
by a single company or by a single
person Decades of work happening across
multiple countries all over the world uh
sort of develop the tech and Publishing
the results and open sourcing and that's
how people say okay well that's
interesting if you look at the example
of Sora Sora at openi was not just
developed by open a it's built on a
series of research papers that were
produced in Academia so it's all kinds
of collaboration is what you're saying
very much collabora and for a
transformative technology like this more
and more collaboration is absolutely
important Russ are there any sectors
that you when you look at it say hey
this is where AI going wrong could be
dangerous is there any spors I don't see
that right now there are certain sectors
that are going to be very we have to be
very careful like medical domain I've
mentioned that one we need to be careful
there obviously it could have huge
benefits right because sometimes you
know you go to your family doctor you
ask the question your family doctor I
don't know and you go po mat and start
searching so you can have a eye that can
really uh or or or reduce the the the
the misdiagnosis so so but but and so I
think that sensitive they're going to be
sensitive or military applications
that's going to be very sensitive in
that space as well obviously right like
how do we you know especially once we
build AI agents agents that can make
decisions that's going to be very the
area where we have to sort of be very
careful how that technology is deployed
I think that as the technology matures
again I think I'm hoping that we're
going to have some regulations on the
deployment side when we deploy that
technology for use users or consumers
will probably need some form of quality
control to ensure that the technology
doesn't just uh you know do do bad
things but I don't see any sort of areas
right now where I could look and say
that's a really bad case of of using AI
I mean I'm I'm an optimist in general
with the with with AI I know some people
don't have the same view but the best
thing we can do is I think is
collaboratively and through open source
as we building that technology on the
understand and understand its risks and
understand when it fails so that we can
build something that's more robust so
Russ you're an optimist and uh so are we
but let's just say if AI goes wrong
somewhere let's say in the medical
situation you built an AI system that's
been emplo deployed by a physician or
something like that who should be liable
for that uh large language model company
the developer the deployer the doctor
that's a very good question I think that
in I see the future where doctors are
still going to stay they're not going to
be completely placed by AI just as
primarily for this purpose that you
asking doctors will make final decisions
and if they make the wrong decisions
they're going to be liable I see the
future where AI is going to be helping
doctors of what they should do or they
should prescribe but not necessarily
make the final decision now of course
there's a Gray Line because the doctor
can say well I was mised by AI but in
general I think that uh I do want to see
the future where people are still in
charge of actually making critical
decisions and the liability for those
decisions is going to lie with humans
not with AI because exceptions to that
you you could argue well s self-driving
car if if there's no person in the car
the car drives itself and gets into the
accident whose fault is this and in this
case obviously the fault is going to be
with the company who developed that AI
because they they they did not make it
safe enough if AI complet replaces the
human then it's the company that
develops AI right they should be held
accountable if it's AI That's helping
humans in sort of these critical
applications then I think it's the
humans who are going to be responsible
because it's it's it's a person or
system that makes the final decision
sort of uh but I do see the future at
least in you know various aspects where
some jobs are going to be automated sort
of routine jobs jobs that require you
know very important like life death type
of a decision is going to be still with
a human decision maker so you think
there'll be a human in the loop um and
that's where the responsibility should
be I think that that's how at least I
see that that's how we will evolve in
the next you know 10 20 years and then
of course there's an argument that at
some at some point when AI becomes much
much better than humans then maybe AI
should just make the decision but that's
something that's you know the regulation
and this is way the government would
really have to come in and say what
because that there are arguments on both
sides I've seen arguments in cases like
there was a case with AI helping judges
make decisions par decisions and AI
would say yes you should give the parole
to this person because based on the
historical data based on all analysis of
the last 50 years this person should get
a parole and the judge would say no this
person should not get the parole and
then so so there is like arguments on
both sides where the judge might be
biased as well the decision might not be
the right decision also and that's very
a top where it's very hard for people
like me researching to say who should be
right or wrong that's when we really
need you know regular sort of at the
government level to to to to to Define
what what that is Russ I could go on and
asking you uh for another 3 hours
because there's a whole range of things
because of your vast background
appreciate you taking the time our
audience as I mentioned is uh Global and
wide variety some final thoughts for our
audience that you would like to leave
behind for them yeah I think that you
know there's a lot of people who are
concerned about risks of AI and you know
displacement of jobs and that's all you
know we need to we need to really think
about this but I also want to end up on
the note that I think there is a lot of
potential for AI as well huge potential
think about education you can transform
education I can have my own personal
tutor that will educate me and think
about the entire population of the world
right I can have a personal teacher that
can you know educate me and I don't have
access to good education health I think
is going to be have transformative
impact I've seen some example some of my
friends actually coming to me and saying
I talk to my family doctor they don't
know then I start interacting with chat
GPT and and it actually gave me much
better you know ideas of of what's going
on and then I can go and de further and
actually understand obviously in these
domains in finances in education in
health we're going to have big big big
transformation so I think we obviously
there's a balance need to take into
account but but I'm very very optimistic
that you know it's uh if we do things
right in a right way the future is going
to be you know we we're just going to
continue improving so it's kind of you
know it's both of them we have to
consider as as the technology evolves
well on that optimistic uh note us thank
you so much for sharing your Insight
your experiences with us today I know
our listeners are going to really get
some real valuable perspectives uh from
this and until next time to our
listeners please stay informed stay
engaged and remember that diversity of
voices is important and the are
regulation and Innovation is important
Russ thank you so much for taking the
time really appreciate it absolutely
thank you so much for having
me thanks for tuning in to the
regulating AI innovate responsibly
podcast you'll find links in the show
notes to any resources mentioned on the
show if you're enjoying our podcast
please subscribe so you'll never miss an
episode and leave us a five star review
