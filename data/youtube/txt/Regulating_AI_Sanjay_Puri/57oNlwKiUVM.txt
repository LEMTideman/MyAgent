ethical issues that are raised by
artificial intelligence whatever they
are right and try to mitigate them try
to solve the issue try to find a way to
make sure that AI is more beneficial
then it might be you know problematic
the biggest ethical issue today is the
narrative the problem that we have today
is that we are surrounded by a narrative
by a discourse about AI ethics uh with
people that do not always know what
they're talking about we are all biased
biases are part of the way we deal with
the world out there so If we remove
biases, we are creating something that
is not real. Each time you see something
or you listen to something and you're
told something, do not take it for
granted. Do not take it as an
information. Take it as a data, right?
And then you put it on your data set and
at some point you can use it to make
inferences and to transform all those
data into useful data.
Welcome to the regulating AI podcast.
Join host Sanjay Puri as he explores the
dynamic and developing world of
artificial intelligence governance. Each
episode features deep dives with global
leaders at the forefront of regulating
AI responsibly, tackling the challenges
using AI can bring about head-on and
enabling balance without hindering
innovation.
Welcome to the regulating AI podcast.
Artificial intelligence AI stands at the
forefront of technological evolution
with experts predicting that it could
add trillions of dollars to our GDP, but
it could also have a major impact on our
workforce, national security, and many
other things. So, how do we regulate it
without stifling innovation? Our podcast
features insights from various
perspectives. Industrial leaders to
government officials to advocacy groups
to civil society leaders, academic
institution leaders. Together they
address pivotal questions that are
needed to create fair
practical legislation. I'm very excited
to have Dr. Emanuel Coffee with us
today. He's an AI ethicist. He's also a
professor and researcher at ISUP which
is the Paris Institute of Digital
Technology. He's also an ethics sharpa.
I invited him on this show as it is very
important to get different perspectives
towards framing AI legislation and I
wanted to get a perspective related to
AI ethics especially a global
perspective. Welcome Dr. Goffrey. It's
an absolute pleasure to have you on the
regulating AI podcast. Thank you. Thank
you for having me. It's a pleasure. Um,
Emmanuel, we have uh a large global
audience which is, you know, very
diverse. Uh, it's made up of policy
makers. It's made up of uh think tank
experts, it's made up of CEOs of AI
companies, practitioners. So for uh all
of them, can you first uh start by
defining what do you mean by ethics?
Because for
non-experts there is some there's a lot
of confusion. We've had a lot of AI
ethics on the show but and there are a
lot of AI ethics now. There's an AI
ethics 100 AI ethics 500. So just to
start the conversation can you frame it
for our listeners please? Yeah sure that
it's it's a really good starting point I
guess to just put things on the table.
Uh lots of people actually are
selflabeling as AI ethicist or doing AI
ethics. The first thing that I want to
say is that I'm not talking about AI
ethics. I prefer talking about ethics
applied to artificial intelligence. Um
so when we're doing that when I'm doing
that actually I'm just trying to to
apply ethical thinking to this technical
object that is artificial intelligence.
Uh so there is obviously a big issue
about how we can define what AI is and
what it is not. So I will not dive into
that. It's too it's too techy. But when
I'm talking about ethics, I I always try
to remind people that ethics is about
philosophy. Lots of people that are
doing ethics today are lawyers and there
are politicians, they're communicants
that they're not really specialized or
not expert in philosophy. The point with
a with ethics applied to artificial
intelligence is not it's not philosophy
for the sake of philosophy. The big
difficulty that we have as AI ethicists
is to take all those philosophical
thought theories and background and to
uh use that for really practical needs
that are that are the needs of big
companies organizations that might be
public and private organization. So the
point is to identify the let's say um
ethical issues that are raised by
artificial intelligence whatever they
are right and try to mitigate them try
to solve the issue try to find a way to
make sure that AI is more beneficial
then it might be you know problematic
well that's uh very very helpful for our
audience so since you have set the table
for defining it in your view what are
the key ethical issues for intelligence
as uce you for our audience. Uh the the
there are two answers to that. The first
one might be just look around and and
read the internet and you find lots of
things about the impact on the
environment, existential risk, uh the
impact on employment, this kind of
things. So these are the major problems
as they are seen or perceived by most
people that are working in the field. My
take would be just to uh withdraw from
all that what I called ethical noise
because if you really dive into all of
these issues uh we can see that they're
not always issues per se. uh my take
would be that uh the biggest ethical
issue today is the narrative the
discourse the discussion that we have
around that and you've asked really
relevantly what does that mean what AI
ethics means and that's the question
that um u not that many people are
asking themselves actually and and the
problem that we have today is that we
are surrounded by a narrative by a
discourse about AI ethics uh with people
that do not always know what they're
talking about people that do not have
the same definition of what is ethics,
what is artificial intelligence, what is
the anthology and these kind of things.
And this is the big issue that we have
now because you know I I like philosophy
obviously and when you go back to
ancient Greece you got the sophist. The
sophist were people that were selling
away or discourse in order to influence
others not to find the truth which was
the the the pal goal of philosophy like
Socrates. But obviously those people the
sophist they were just selling for money
uh some kind of discourse that might
influence others right and and we are
exactly in the same position now with
all those words that people are just
taking they are just reselling without
even thinking about what's behind all
those words what is really the meaning
of all those words uh so we are really I
guess uh we we are now struggling in
this this huge narrative and obviously
lots of people are highly influenced by
this narrative which is I guess a bit
problematic and and this is one the of
the reason that we have this polarized
uh situation where you get people that
are technopiles and people that are
technophobes uh that are fighting each
other just saying okay AI is the uh you
know the solution for all our problems
and on the other hand people that might
say okay it's it's actually an
existential risk uh so so this this is
where we have a big issue because if we
have this polarization if we have this
narrative that is not really clear we if
we have all those actors that are uh
using those words without really trying
to define them we cannot really analyze
correctly the situation we cannot find
solutions the problems that we're
facing so that leads me to two questions
uh Dr. graphy for our audiences. Uh as
you said there are a lot of people who
are talking about AI ethics uh for and I
talk to leaders of companies etc who
have set up AI ethics boards. They have
set up ethics committees. How can uh is
there a way for them to figure out uh
who uh what kind of experts that they
should have? Is there some kind of a
standard uh standardization like you
have for accountants, lawyers, you know,
you pass a bar, you pass an accounting
exam or for medical exam? Is there
something from AI ethics that kind of is
a certification that I we can say okay
this is the real deal? Not yet. And
that's that's the big issue that we
have. We're trying to fix that here in
Europe. We created an association of
ethicist. We're trying to make sure that
those people they have the sufficient
background because it's not only about
doing philosophy. It's also about
mastering some kind of technologies.
It's al also about mastering the
environment, business environment. Uh
it's also about knowing about the laws.
This kind of things. So it's it's really
a wide range of knowledge that are
needed. So so for from now there is no
such thing as a standard to say okay
you're an ethist. My my guess would my
take would would be to say at least try
to make sure that you have someone that
has the sufficient background that have
worked with industries that has that has
the knowledge in philosophy that's key
that's really key and I'm really
insisting on on that because most
ethicists today are lawyers or they are
computer scientists there's a big
international companies that that has
been hiring AI ethicist and uh in in the
position form they were asking for you
know computer scientist background which
is to me totally nonsense sensical,
right? So, at least make sure that you
have people that have the background in
philosophy that are used to work with
companies because the problem when
you're looking for people that have the
background in philosophy is that you
will find philosophers, right? And
philosophers are really good at
teaching. They're not really good at
working with businesses. So, so you also
need to have this ability to bridge the
gap between the philosophical theories
and the practical needs of of
organizations.
So have a philosophy background but
someone who is has that background and
has worked with businesses I think is
what you're saying for our listeners uh
when you're hiring an ethics ethicist
please keep that in mind there is no
certification yet uh Dr. Gofy said
Europe is working on that Europe is
always ahead in terms of setting
standards uh coming to the second
question I have Dr. Goofy is in your
view what core principles should be
embedded in AI systems from an ethics
standpoint are there some core
principles in your view? No, I I don't
think so. I don't think so. Actually the
the question which is totally legitimate
is uh is is at the really high level of
abstraction. So what what I'm doing when
I'm working with companies um the first
thing that I try to make them understand
is that you cannot address this kind of
issue if you do not know your use cases.
So start with your use cases. What are
the use cases in your company and once
you identify that you get the list then
you can identify also the impacts
positive and negative impacts of those
specific use cases in your specific
situation and then you can see what are
the problems that you will have to
address. So for the negative impact you
can mitigate them you can just try to
avoid them for the positive impact you
can try to build on them to make sure
that you are adding value to your
activities. Right? So the point is that
there is no suction gas score values
called principles. It's all about a very
specific context. And that's a really
important point. I would say it's to me
the difference between morals and
ethics. Morals is made of those big
norms that are out there applicable to
everyone in a situ in a big community.
Ethics is a decision that you're making
in a very specific situation. Ethics is
really highly contextual. So you have to
know the context. you you know have to
know the the use cases. You have to know
what kind of application that you're
using and and what are the impacts. So
this is the starting point and then you
can you can build on that. Well that's a
very important point you made. You're
saying that difference between there's a
difference between morals and ethics.
Ethics really depends on the use case uh
of each system each company etc. Uh Dr.
Goofy in my conversations with uh uh
seale leaders at companies who are large
fortune 500 companies who are
implementing AI ethics and I want your
view most of them have ethics boards but
they're all people brought in from
internal
uh you know they brought in somebody
from HR from somebody from legal
somebody from finance etc. What is your
view? Should an ethics board in these
large companies should be internal or
should they have some external uh people
also? I I I think that it's it's always
better in this kind of situation to have
someone that is external. Obviously,
when you're internal, when you're an
employee of the company, you have you
have you have strong pressure and you
cannot tell whatever you want, right? So
you will always have to make compromises
about the risk that you're taking to say
help cloud things that might uh unplease
your your your boss. So, so my my my
advice would be whenever you can try to
have someone that is external to the
company that is able to as I'm doing
myself just to tell the truth right we
call that the parisia it's a
philosophical sense saying okay I will
tell you whatever I have to tell you
whatever it takes right and even if it
does not please you that's not the point
you have to know about the reality of
what is happening I've seen that myself
many many times in companies where
people they have this pressure and they
they know the problems they they
identify the problems but they don't
want to raise the flag and and tell the
boss okay look we are facing the
situation now right they just try to
protect themselves they just try to to
to protect their position which is fair
but that might be counterproductive for
the for the company on the long run no
so I think you're making a very
important point for our listeners who
are setting up ethics boards in their
companies what he's saying is have a mix
of internal and external just to keep
things uh to give you uh real independ
dependent perspective like you have in
your corporate board you don't have you
have your CEO on the board maybe you
have a CFO but you have people outside
on your audit committee on this
committee same thing should be there on
the ethics board uh I think Dr. Goffy
you're making a very important point Dr.
Go shifting uh a little bit in your view
uh what kind of educational initiatives
would best spread awareness of AI ethics
issues you are uh besides being an
ethics share by an ethics leader you're
also an educator so uh talk uh a little
bit you know because there is so much
now about AI ethics on the internet but
how do you kind of use education
initiatives to you know spread awareness
of AI ethics
issues I I really foster discussion and
deliberation and debates that's I think
that it it is it is key as as a
professor I do not tell or I do not
provide any kind of truth uh about
what's what is acceptable what is not
acceptable in terms of ethics applied to
artificial agents I'm just trying to
raise awareness uh that there might be
issue and I help people to to to
identify their own problems and their
own solution it's called I don't know if
if you if you like philosophy it's
called myotic which is how to make
people you know give birth to their own
perspective in ethics so I'm not I'm not
a moralist I'm not here to say what is
right and what is wrong but it's all
about discussion so you can take use
case obviously when I'm working I I'm
facing lots of use cases that are really
interesting so I bring that to the table
and I discuss with you know the students
that might be really young people maybe
older people and just trying to to tell
them okay what would you do in this kind
of situation and then once you start
doing that you can bring slowly concepts
and notions that are ethics concepts or
ethics notions right just to enlight the
discussion or also to raise awareness
about the fact that for example when I'm
working in France in Europe you might
have different perspective coming from
other countries other places other
cultures right and this is something
that is really interesting because here
in the western world I would say Europe
and and and northern America people are
not always aware that in other faces
perspective might be totally different
and and sometimes we think because and
especially us in France we think that we
hold any kind of truth because we are
the heirs of the enlightenment uh but
that's not the case definitely not the
case right uh the biggest part of the
world is in Africa is is in the Indian
continent and in China so it's
definitely not us right so we do not
hold any kind of universal truth and and
sometimes I'm just trying to bring
that both to my to my students but also
when I'm working with companies. I also
I'm I'm doing a mission currently uh
with a real estate company that is
operating worldwide and I'm trying just
to tell them okay look your values or
principles they might be really
legitimate in France in Europe in
northern America but if you go to Africa
if you go to India if you go to China if
you go to Saudi Arabia that might not
work right so you will have to adjust
that and to understand that the cultural
setting will be different.
So you have raised a as uh usual a very
important point that I want to kind of
delve a little bit deeper is uh you
talked about cultural value systems. How
do you think different cultural value
systems would shape uh policies for
governing AI globally? Because um you
know the big uh discussion debate at
least uh that we have uh in many cases
on our podcast from guests around the
world is AI is becoming a Silicon Valley
uh western uh Englishbased uh system and
that's a big fear. It's not just
countries in uh Africa, India, etc. But
there are smaller countries even in
Europe, uh Finland and others who have
uh their languages etc. But let's uh
just talk about how do different culture
value systems shape policies for
governing AI systems globally. Uh Dr.
Gy, I I would say uh if if I'm a bit
cynical, I would say that that that it's
impossible, right? Because Ive I've I've
done international relations, political
science. I I know how it works at the
diplomatic level. Obviously, you get big
powers that want to remain big powers,
right? And they will try to to do
whatever it takes to make sure that
other powers will just follow the rules.
uh the problem that we have today is
that if we really want to have this kind
of global regulations which I which I
don't believe in to to be totally frank
with you uh we should need to learn to
listen to others and stop thinking that
we hold any kind of truths that that's
that's key listening is definitely key
and and to my knowledge which is a
limited knowledge I admit that um I I've
never seen that that much actually we we
are I mean as the western world. We're
lecturing the rest of the world saying
okay this is acceptable this is not this
is desirable this is not desirable
without really uh taking into account
the specificity of the culture of people
that we're talking to right which is uh
which is an ethical issue per se because
it's a denial of the diversity of
cultural perspective in ethics which is
definitely a a problem so unless we
learn to listen to others unless we
accept that we are not at the center of
the world. Unless we accept that other
people might have different perspective
and that we have to respect that we will
not be able to have a global governance
system. At at best I would say we might
have a really superficial governance
system right that will work in some
cases that would not be efficient at all
because there will be lots of gray areas
in which lots of actors will definitely
go. Uh so so the problem is that we will
suddenly have kind of some kind of a
rule of global governance but it will
not work and and and and my guess is
also that lots of of cultures that might
be countries small countries biggest
countries or even cultural areas might
feel like they are u they are suffering
from this kind of cultural tyranny and
we also might have new tensions between
you know cultural groups saying that we
don't want to see your western values
entering into our own you know set of of
values. So so we really have to learn
how to to to listen to others and to
find compromises I mean real compromises
not compromises that are imposed by one
side to other sides. No I think so
you're making an important point is that
we need to listen and we need to accept
the fact um that maybe we are not the
center of the dialogue or the world and
start making compromises. Uh Dr.
you're in France. The EU uh just passed
its comprehensive AI act, the first of
its kind. Do you think they are
addressing some of the the EU AI act is
addressing some of the things that you
are concerned about from whether it's
from an ethics, cultural standpoint?
Not at all. I would say that I'm really
critical against the um uh the EU
legislation. The AI hack per se is not
about ethics at all. It is based on the
trustworthy AI guidelines that are based
on some ethical stances that are really
I would say biased thisification that we
have here that is lacking clear
definition of all the requirements and
ethical principles that are inside the
document. So it's it's really really
superficial and it it will not work.
Plus the problem with any kind of
regulation that is dealing with
artificial intelligence or this kind of
technology is that um it it will not
work really long because technology is
is moving really fast obviously uh and
and also u uh you know social
expectations are are moving really fast.
So uh this this legislation is is is is
better than nothing I would say but it's
definitely not the right solution and I
think that we will uh quite soon see the
limits of of of this and especially when
it comes to balancing innovation and
ethical or normality constraints which
is already the case right and uh the
point is that if we I say we eur the
European Union if we really want to get
our lion's share of artificial
intelligence At some point we'll just
have to to remove those constraints
because obviously lots of other
countries are less constrained. They
will move uh really fast and and and I
feel like in this big competitive world
we will lose.
So you're not as uh optimistic about the
EU AI act. Um Dr. one of the
questions that came up uh for you from
our listeners is that is it uh possible
to create an unbiased and truly fair uh
data set and s AI system.
I I I would I would if you don't mind uh
offer to rephrase the question. Okay. as
it is is it I would ask myself is it
something desirable to remove biases
that's that's the big question right
because when when when we're talking
about biases it's a really complex
question there are more than 180
cognitive biases you also have
algorithmic biases you have scientific
biases you get a lot of biases for
example you get the optimism biases bias
is it something that we have to deal
with certainly not so the point is that
what kind of bias do we want to mitigate
what kind of bias we want to address.
The second point is if we consider that
AI is here to mimic or to duplicate
human behaviors and human minds. I would
say we are all biased whatever whatever
we think about it right we are all
biased. Biases are part of the way we
deal with the world out there. So if we
remove biases we are creating something
that is not real. Right? We are actually
this is really woriented perspective.
We're trying to create something that
looks like the perfect world as we see
it. A world that is freed from any kind
of bias. But the beauty of the world
where we live is where we live in even
if if sometimes it's it might be
problematic is that we are unperfect
beings, right? Is in the imperfection of
the world that make that makes it really
interesting. So my question is is is it
really is it really something desirable?
this is something really acceptable to
say okay we'll remove all the biases
plus and once again biases are really
contextual for example when you're
discriminating between genders or
discriminating between skin colors it
might be totally unacceptable in the US
in Canada and France and other places
but it's totally acceptable and even
desirable in other places right because
the the color of your skin might give
some tips about what kind of tribes you
belong to what kind of population you
you you're in etc and and even you know
discrimination gender if you talk in the
Muslim world it's something that is part
of the culture you you might disagree
with that that's not the point but at
least we have and this is all about
listening we have to accept that some
people might think differently than us
right so the question is how can we
address biases at a really high level of
abstraction knowing that there is behind
that a really wide diversity of
perspective on what is acceptable and
what is
not so as you that it's the question
should really uh be a little different.
Um just to take that ahead a little bit
from what your discussion is um you know
how can historically marginalized
communities equitably participate and
benefit from advances in AI? Um,
Emanuel.
Uh, once again, I would I would say
first, uh, and this is something that
I've told to, uh, mostly your friends in
Africa. I've told them, okay, stop
listening to the European world. Stop
stop listening to the Western world.
Stop to to accepting to submit yourself
to to to the European and Western
perspective. Uh, it's it's easy to say.
it's it's uh it's less easy to do
obviously but I I think that for example
looking at Africa uh but also at the
Latam region in Brazil for example I
would say okay free free yourself from
from the perspective that is brought to
you that that that is sold to you by the
western world try to have your own
perspective with your own values start
with your own values when I when I see
you know once again when I see Muslim
countries that are trying to uh
implement non-discrimination between
genders in their own and this is the
case. I've seen that in some ethics
codes in the in the Emirates. I would
say that that's totally nonsensical.
It's something that you're taking you
copy pasting from the western world. But
that does not fit your expectation that
does not fit your cultural needs. So
start with who you are, what are your
values and based on that you will be
able to establish your own principles.
then you will have your your your own
normative framework and then you will be
able to innovate in in this specific
framework right but I would say okay
free yourself from uh from the discourse
that has been sold to you by by
outsiders I would say mhm so free
yourself from uh the discourse of
outsiders uh Dr. Uh Garfield Umanuel
um what can we learn uh are there other
ethics disasters and other technologies
that we can learn from to prevent AI
harms?
Um I I I I don't think that we can
compare AI to any kind of other
technology. I've had this discussion
with a diplomat uh who was comp
comparing the AI problems that we might
have today with you know nuclear issues.
that's not the same at all. Right? So,
uh comparison is not always reason. So,
I I would say what we can learn is that
we we still learn things like we do not
know about them. I I think that lots of
issues that we're facing today with
technologies like AI are things that
have already been dealt with by
philosophers uh already in in ancient G.
Right? So my my point is that instead of
just try to reinvent the wheel all the
time just try to uh to to see how people
were dealing with the big issue not
making comparison between different
technologies but about the methods that
you have to use in order to deal with
those technology which is basically
deliberation discussion debate. We are
now in a society and we are now in
societies I would say uh where we are
lacking time for discussion. So we are
making our opinion based on what we hear
on in the news, what what we read in the
newspapers but we do not we do not think
by by ourselves and that's the big issue
that we that we are facing today and we
need to move back to the taking some
time to deliberate to discuss with
others to listen to others to make our
own conviction not opinions right and
just to have our own ideas about what is
going on right on on on this issue. So
this is more about the methodology than
about the uh the technology per se. Mhm.
So it's more methodology than
technology. Uh Emanuel, if uh an AI
system causes harm, uh who should be
liable, the developer of the system, the
deployer or the user or should it be all
of them? I mean that's a big uh question
that is looming up and the lawyers are
salivating at all of that. So I wanted
to get uh a perspective from an AI
ethics expert. Yes. Once again I would I
would say that if you if and that's the
the way the question is asked mostly if
you ask the question that way you you
won't have any kind of answer obviously
and this is I've been working in this
field for years now. I started in the
military working on autonomous weapons
and we had exactly the same question
that was raised very beginning of the of
was 2004 2005 something like that in
France. Uh there is no answer to that
because actually the way it uh it works
is that when you do something bad,
right? Uh you will go on a trial, right?
And and you got the people that will try
to see why you did that, what was your
level of knowledge, what were the the
ability that you had to change things,
who was part of this, you know, the the
hierarchy chain, if there is a hierarchy
chain. And actually you do not you do
not um put the responsibility a
priority. You try to evaluate the
responsibility apostroy after something
happens. Right? So you get to you you
get to to really study the case like
like for any kind of you know uh trial
you will just try to study the
situation. You will see who were the
people that were involved what were
their responsibilities and then you will
attribute responsibilities to those
people. You don't do that before
something happened. You did that once it
happened, right? You can it happens in
most companies say okay this is the your
let's say the limits of your
responsibility in terms of what you have
to do and the way you have to behave in
specific situation but that doesn't mean
that you will be responsible for
something specific if something bad
happens. If something bad happens then
and only then you can attribute
responsibilities. If you that the way
around it will not work and you will not
have any kind of answer. I remember
clearly to give you an illustration of
that working on on little autonomous
weapons and drones specifically lots of
people were saying okay it's the guy who
is you know pulling the trigger that is
responsible and I said no no because the
guy was in this cubicle in in in Nevada
just uh you know operating a drone that
is flying over the tribal zone in
Afghanistan he has a hierarchal
hierarchal chain right he has a boss who
made a decision about what is acceptable
what is not he has legal adviser who
told him yes you can or not you cannot
try. So you cannot just put all the
responsibility on the shoulder of one
person. That's not the way it works. So
if something bad happens then just take
the big picture and try to see who was
responsible for what. So if something
bad happens just see who is responsible
and after it happens um yeah so uh
taking that a little forward Emanuel
should certain AI applications be uh
banned based on ethical cons uh concerns
I I think in in specific context yes you
you have depending on your values and
and and what you believe in uh they
might they might have some uh you know
for example I would say that in France u
facial recognition is banned and should
remain banned. But at the same time
because we you know that we are uh we
are entering slowly in the Olympics
we've seen that even when you try to ban
this kind of you know facial recognition
at some point you need it for security
reason. So uh so yes but you always have
to to accept that you will have to make
compromises about your values. Maybe
something that is really strongly
important for you today might be less
important tomorrow because the situation
will change. And obviously if you're
threatened by an actor, you would
consider maybe having this facial
recognition system that might help you
to um to make sure that people are safe
when they're just, you know, wandering
in in Paris. Uh but but I I guess yes,
depending on your values, you have to to
be able to say, okay, this is not
something that is aligned with what I
believe in, so I will reject it. But
it's not that easy uh because technology
is is really pervading all around the
world. Uh and there is no no limit to
technology. Sometimes technology that
have been developed in another country
will enter in your country and maybe
that will you know uh that will hurt
your values. So it's it's not that easy
to to to do. But since you brought up uh
the Olympics, there's also an election
coming up in France. We have an election
coming up in the US and there's one in
UK. We've just had some big elections
that happened recently in India and
others. There's a concern about deep
fakes and the use of uh deep fakes in
AI. Maybe uh it might happen in France
etc. any thoughts from you on uh this
issue? uh when I'm I'm pretty concerned
about that because definitely you know
and it's it's new um deep fix will will
be u will be even more important because
of generative AI system that have been
developed right and and because those
system they are blurring uh the you know
the frontier between reality and
virtuality and and and now we can see
lots of pictures and we can uh we can we
can read lots of things that are not
really real that are fake without
knowing they are fake. So the problem is
that most of us we have to learn to
consider that what we consider as
information is only data. My my my
thought is just just go back to
artificial intelligent and think about
your brain as a data set and and each
time you see something or you listen to
something and you're told something do
not take it for granted. Do not take it
as an information. Take it as a data.
Right? And then you put it on your data
set. And at some point you can use it to
make inferences and to transform all
those data into uh useful data which is
information right. But most of us what
we're doing when we're watching the news
for
example listening to others we think
that people are saying the truth. So we
take it as a piece of information and
then we repeat it as a piece of
information. So the problem is that we
do no longer analyze what we are saying
and what we are using uh in our
discourses. So so my my my advice would
be okay just um step back from what you
what you see what you hear and and and
try to
uh to to be as critical as possible not
in the wrong way. Not critical for the
sake of being critical but just critical
thinking is key. Just don't look look at
things and and try to see okay who is
saying that for what reason what is
behind just try to find other
information about the same subject and
make your own
conviction. Uh some of our guests have
uh some members of Congress have come in
and basically said that we should
assume everything is a deep fake unless
we know for sure. Now that's
a you know pretty
uh unique uh viewpoint. Any thoughts on
that? I I would say first it's
impossible and then which would just
turn crazy if we do that right because
we would not believe in anything around
right. So we we we cannot do that. We
cannot we cannot be uh in in a you know
in in a in a situation of doubt uh all
the time. That's not the way it works.
We we need to to trust others. we need
to trust what we are seeing because if
we don't at some point we will have to
face some big u mental issues. So so
just just once again be critical about
things that are important. There are
basic things you don't care if they're
fake or not. But and especially when
you're talking about a subject, when
you're asked about something that is
important, just try to ask yourself, am
I really legitimate um talking about
this subject? Do I have the sufficient
knowledge? Um or am I just repeating
what I've heard uh somewhere else, which
is which might be a problem. Yeah. So
have do critical thinking. Yes.
uh and view from that. Uh Emanuel, uh
towards the end, we have a lightning
round of questions. It's a little fun, a
little bit putting you on the spot, but
uh our listeners uh love it. So, um
we're going to ask you some questions
and uh hopefully ask you to pick. Uh so,
are you ready for it? Sure. Okay. Um so,
uh let's start with the first one.
Innovation or regulation? Innovation.
AI ethics or AI efficiency?
AI ethics. Uh, transparency or privacy?
Transparency.
Okay. Algorithmic bias or data bias? No
bias. Okay. Good. Uh, national AI
policies or international AI frameworks?
All national policies. National
policies. Okay. Okay. Ethical AI boards
or government regulations?
Government regulations.
Okay. Local regulations or global
standards? Local definitely. Uh security
or accessibility?
Security.
Okay. Fairness in AI or accuracy in AI?
Huh? That's a tricky question. I would
say accuracy.
Okay. Uh, and we'll make this the last
one. You've been a great uh I love it.
Uh, futurep proof uh regulations or
adaptable regulations? Adaptable.
Wonderful. Awesome. Uh, you've been a
great great uh you win a lot of awards
uh because some people duck these
questions. Uh really really love it. Uh
Emanuel, I could have I had bunch of
other questions. We could have gone on
forever but really on behalf of the
entire regulating AI team I want to
really express my gratitude to you uh uh
Emanuel for joining us and providing us
such a wealth of knowledge and expertise
your you know committing uh commitment
to ensuring that AI technologies
developed and deployed responsibly is
really commendable and to our listeners
I hope uh you've enjoyed this uh
conversation which has shed light on a
complex challenge as he talked about how
to figure about what is ethics, who are
the right uh persons in ethics, how to
get an international viewpoint, uh the
challenges that we face and to keep
regulations nimble. Um so until next
time, we'll keep exploring, keep
questioning and keep advocating for a
future where AI remains a tool for
progress and prosperity. So thank you to
our listeners and thank you Emmanuel. A
fantastic uh show. Thank you so much.
Thank you for having me. It was a real
pleasure. Thank you. Thank you.
[Music]
