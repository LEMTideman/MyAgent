The reason people are failing is not
because of their understanding of the
underlying technology. They're failing
for the same reasons they've been
failing since the beginning of digital
transformation. A lack of organizational
focus, a lack of leadership, a lack of
engagement, clarity of strategy linking
the technology to the goals and
objectives of the organization. Um, in
some ways it's the same problem.
Welcome to the Regulating AI [music]
podcast with Sanjay Puri. AI is changing
the world faster than rules can keep up.
So, how do we protect people without
killing progress? [music] Each week,
Sanjay brings you inside conversations
with global leaders, policy makers, and
innovators who are wrestling with
[music] that exact question. So, if
you're curious about the future of
technology and how it's governed, you're
in the right place.
Meet Jeff McMillan, head of firmwide AI
at Morgan Stanley, where he's leading
the charge to responsibly [music]
harness artificial intelligence across
one of the world's most influential
financial institutions. [music] With
decades of experience at the
intersection of data, strategy, and
innovation, Jeff brings a rare blend of
technical insight and human perspective
to how AI is reshaping finance [music]
and the world.
Jeff, welcome to regulating AI. We are
truly honored to have you with us.
>> Sanjay, thanks for having me and um
thanks for this podcast. I think this is
a really important topic which candidly
doesn't get uh the airtime that it
deserves. So I appreciate your uh your
focus on uh on the issue. what were some
of the most critical governance
decisions you had to make during this uh
roll out and how would you advise policy
makers who are now really grappling with
how to regulate something at such a
large enterprise scale around the world.
So the first thing is I would say and
and I don't know for folks who who
probably don't know much about Morgan
Stanley and our journey. We uh were
introduced to OpenAI back in March of uh
22 as a reference point. That was 7
months before CHBT was released upon the
world. So we became the first financial
services company in the world uh to be
you know to work with uh work with open
AAI and we were the first financial
services firm to deploy any form of
solution on the large language model
infrastructure and and that goes back
you know almost almost three years now
and you know the first thing that I
would say and I think it was true three
years ago and I think it's true it's
true today is that uh these are
complicated issues. There is no
precedent. So the first piece of advice
is that you need to bring all of your
constituents to the table. It is not a
competition between your business people
and your legal and compliance people. It
is a partnership
and that partnership requires both a
firm understanding of what generative AI
is, how it works, the tremendous
opportunities, but also the risks and it
also requires an understanding of the
business people what the issues and
concerns that your legal and compliance
and risk people have. So that would be
the first thing and I would only
highlight that is because you're going
to have to forge consensus
in a world in which there are no
prescriptive rules from the regulators.
You're essentially solving for a bunch
of regulatory principles which by the
way in many ways are actually are are
pretty helpful but in many cases are not
fully adequate to represent all of the
issues related to gener. So that would
be the first thing is bring everyone
together and make it a joint effort. The
second thing is what we did is in the
absence of clear regulations we
established our own guiding principles
and I think this is really important uh
because you need you need some sorts
some sense of ground truth like what
what are the boundaries that you're
going to put around this technology so
for example literally almost on day one
we set up a group of folks and we said
well what are going to be the parameters
that will govern this technology and
I'll just give you some you know I'll
highlight some of them at the top level.
So number one, we agreed that everything
that we did with generative AI would be
governed and supervised by a senior
group of people at the firm so that
there would be full transparency on what
people were doing, how they were doing
it. We would track and monitor both the
intake of issues as well as the results
of that work. Number two, everything
that we would do is human in the loop.
And I know there's lots of talk about
agentics and autonomous engines and all
that stuff and I think all that's very
interesting and we could certainly talk
about that. But in a regulated
institution that requires very high
degrees of precisions, while the AI is
incredibly helpful and and and enables
people to do a better job and do better
things faster and and sometimes cheaper,
the human is ultimately responsible and
accountable for the quality of the work.
And putting the human at the center of
everything that we do, I think is really
important. Uh the third thing is that we
require a robust and evaluation
framework on everything we do. And I
suspect we'll talk more about this. I
think one of the things that we've
learned is that generative AI is
incredibly easy to build and very very
difficult to deploy with high degrees of
efficacy. And the reason that is is the
non-deterministic nature of this
technology
makes it necessary to put this
technology through very robust
frameworks evaluation both automated and
and manual and that oftentimes takes a
lot of time, effort and expertise that
historically organizations don't have uh
the wherewithal to to do. Um and then
the final piece is that there needs to
be second line oversight. So while my
organization does a lot of interesting
things, we build a lot of technologies,
everything that we do, there is a second
line uh individuals who are responsible
for independently
validating the efficacy and I can't go
live without a second group of people
saying that Jeff McMillan did his job.
And I, you know, to be clear, those
people don't work for me. They don't get
paid for me. And I think this idea of
independence now I don't know if those
are all of the things that ultimately
firms need to do and I suspect some of
them will change but I have to be honest
with you we established those rules
three years ago and they've really
they've stood the test of time um and I
think they've served us well uh and you
know some variant of them I'd recommend
firms adopting because I do think they
really give a good basis as people think
about deploying this technology and
scale and assume that you won't get it
right on the first day, right? I think
you have to be willing to make these
infrastructures open, flexible, and
adaptable. And while you should put a
line in the sand at any point in time,
you know, every 3 to six months, you
need to re-evaluate and ask yourself
what's going well, what needs to be
enhanced, and how you can continue to do
it better. for one because you'll learn
but two the technology moves so quickly
that you may actually need to make
adjustments simply based on the the
speed and and transformation of the
underlying of the underlying tech.
>> Does Morgan have an ethics AI ethics
board or something of that nature?
>> It's called an the governance committee.
It's it's effectively governs ethics.
It's one of their responsibilities. I
mean its specific responsibility is it's
it oversees the responsible deployment
of AI and that includes things such as
model validity, privacy controls, cyber.
So ethics is a component of that. Uh but
it's it's part of a broader set of
features if you will that are governed
under that broader um that broader
mandate. ethics being one of several
things that are obviously important in
the in the responsible deployment of AI.
>> So there are peers of yours who are
listening trying to figure this out too.
Let's say the governance etc. Is it all
internal based or are there external
experts also in there? What should they
what is a good mix according to you for
others?
>> I would first be cautious of anyone that
really c calls themsself an expert in a
in a space that is literally you know
we're talking months not not years or
even decades of experience. And you
know, I I was recently at OpenAI and saw
a bunch of uh unnamed consulting firms
that were being trained on this
technology that were very likely going
to be out in your offices in the next
few weeks selling you on several
million-dollar projects. And no
disrespect to them, you know, they're
learning as well. So the first thing is
I would just and I'm making a general
comment is that there are very few
experts. So be cautious is my first
comment. We uh have generally found
given that we were the first financial
institution much of what we did is we
had to rely on our own internal uh
expertise. I don't want to act like you
know deploying complicated technology in
a regulated environment is something new
for a large scale financial institution
like Morgan Stanley. Um, you know, we
have lots of very, very competent people
in our legal, compliance, and risk
department that are very familiar with
this. And in some ways, one could argue
that has been an advantage to the
financial services industry because this
is not a new problem to us. So, you
know, to answer your question
specifically, I think that number one,
there's a lot of good information out
there in the marketplace around how to
establish policies, how to establish
ethics, and I think that's worth using.
If you have identified a resource that
has actually done this in practice, I
would just make sure and kick the tires
that they have done this in practice in
a similar situation as yours. But I
candidly I would say to most people the
biggest challenge with AI is not the AI,
it's the organizational it's the
organizational constructs. and we can
probably talk about this and nobody
knows your organizational better than
you do. And while I can probably teach
you AI in a few weeks, I can teach you
how to prompt. I can teach you what the
transformer architecture is. I can teach
you how to use this technology.
It's not that hard because if you know
how to speak and read, you too can be a
prompt engineer. The reason people are
failing is not because of their
understanding of the underlying
technology.
They're failing for the same reasons
they've been failing since the beginning
of digital transformation. A lack of
organizational focus, a lack of
leadership, a lack of engagement,
clarity of strategy linking the
technology to the goals and objectives
of the organization. Um, in some ways
it's the same problem. And you know what
I would say to any organization,
don't get too wrapped up in the
specifics of the technology. What I
would say is bring your best people
together. Set clear defined objectives.
Uh learn from what's out in the
marketplace where you can. If you can
bring outside help in, great. If you
can't, then do your best to figure it
out. Start small, right, with with with
little with little pieces and then grow
over time. and you'll learn as you go.
Um, but I can't stress that the most
important thing that will prevent you
from success is not your knowledge of
the tech, it's your own ability to
execute change effectively in your
organization.
>> Does the chief compliance or the chief
legal or the general counsel have a role
in this? Uh, because it's we are talking
about a completely new paradigm. I'm
talking about large Fortune 500
companies.
We have a global head of legal
compliance at Morgan Stanley.
You know, are they involved in our
day-to-day committees? No.
Are they involved in establishing the
infrastructure that we put in place?
100%.
Do we update them on a regular basis on
the progress and do we bring issues and
concerns to them on a regular basis?
Yes. So when we established the the the
solution the infrastructure when we
identified who the committee and
oversight would be all those folks were
involved and are they updated on a very
regular basis 100%. I mean and
particularly you know as you talk about
and I'm sure we'll we'll talk about this
more there is an inherent conflict
between how much controls what what
degree of controls you put in place to
ensure that people make zero errors and
how much flexibility you let people you
know how much flexibility and and
independence you give people to let them
run. And depending on what side of that
trade you are on, you will complain
about the other side of that trade.
Right? So having senior leaders on the
business and on the second line controls
at the table and being able to escalate
not theoretical issues but real issues
such that those issues don't swirl
for months with five or six lawyers in
six or seven countries so that people
can make a pragmatic decision. what
makes sense for the business. Balancing
the risks but balancing the needs of the
business and finding it a proper at
sometimes the only people that can make
those decision is your chief counsel and
being able to take those issues that
bubble up and bring them to the most
senior people at the firm such that
those decisions can get made in a
thoughtful way. uh I have found to be
really really important because in many
cases a simple one-hour meeting where
all of the issues are raised with that
group of people those issues can get
resolved very quickly but it does
require a senior group of people to
ultimately to weigh in think about the
implications to the firm um and in some
cases need to go to the firmwide risk
committee as well right where where we
can memorialize those issues um and and
reflect those decisions on behalf half
of the firm. So I think what I guess my
message is is you need all the right
people that are empowered to make
choices but you need to have a mechanism
to escalate quickly when and if you kind
of hit a roadblock so that the senior
folks can be informed, educated and that
they can make the call.
>> How would you characterize the sweet
spot between ability to move fast and
governance? the innovation versus
how does one balance that's a question
that comes up on our podcasts all the
time
>> too much AI too fast is a risk right and
you know we need to sort of balance that
with the right level controls and by the
way I'm not here to say that there is
not risks associated with AI and we can
talk about them and we do a lot of work
to mitigate them but one of the things
that I think is important to acknowledge
is there is a risk of moving too flow
with AI. And the reason I say that,
putting aside the putting aside the
commercial risks of being left behind,
one of the things that I don't think
people fully appreciate is one of the
most significant beneficiaries of this
technology are the control functions,
the risk and control functions in a
financial services organization. Because
what AI allows you to do is the ability
to monitor, surveil, control and reduce
the standard deviation
on the quality of output is incredibly I
mean that's what AI or generative AI in
particular is quite effective at doing.
So the first point I would make Sanjay
is that we need to acknowledge that AI
is going to help us improve the risk
posture of a financial services firm in
aggregate not decrease and moving slowly
prevents us from ultimately getting to
that point. So that's the first point
I'd make. The second point I'd make is
that it's very hard to argue
hypotheticals,
right? It's very hard to argue about the
future of AI and the future problems of
AI or the future benefits of AI. AI is
just a tool. So the first thing you have
to do when you look at a project is you
have to assess the degree of risk
associated with that technology. So the
first thing is that you have to
understand the risks associated with
each of these these things. And what you
find is a lot of these things do not
require a massive amount of overhead
because the risk is quite low. And then
once you've established the the level of
risk, then you can have a tangible
meaningful conversation about what level
of controls, monitoring, testing is
necessary. And then you can actually
have a a real debate as to whether or
not this is this is too much or too
little. And my point is is that focus on
the actual real issues like put your put
your template together but then actually
do real work and ask yourself what are
the real risks of this technology
what are the mitigants I need to put in
place and what is the what is the
appropriate amount of governance I need
to mitigate those things and sometimes
you will find that's a very low standard
and sometimes that's a very high
standard and And I think if you can get
agreement on what those standards are
and you get both sides of the equation
understanding what those rules are, then
you're able to start to create um a
process that's that's a more efficient
that's well understood on both sides.
The expectations are clear and there's
and and people are educated on what
needs to happen and people stop arguing
and they start doing and executing. And
I think that's really what's the most
important thing.
Jeff, uh, before I get into another
point, does shadow AI concern you, uh,
in an organization like Morgan at all?
>> U, first of all, you have to be very
clear on what your policies are. I mean,
it is no different in some ways
your employees, and by the way, I I we
haven't talked about this before and and
and and maybe we won't, I don't know,
the single most important thing that any
organization can do right now is educate
their employees.
They should educate their employees on
what AI is, how to prompt, and what is
the responsible use of this technology.
I can't stress that enough. And if if
you have $100 to spend right now, the
first thing I would 90 of it I would
spend on education. And I would spend 90
of that before I would even spend it on
on on building any technology. I can't.
And if I had to do it over again, I
would spend more money on education,
right? Because until people both from an
effectiveness and a responsible use of
technology, until they really understand
how to use this technology, they cannot
leverage it fully, nor do they really
understand what the risks are. So just
just to take a big step back. Um so to
that point on shadow, number one, you
really have to you you have to teach
your employees how to use this
technology. You have to teach them what
the risks are. And then two,
you know, one of the things that we're
we're also working towards is having AI
check the AI, right? So we're moving to
a world where where you will be able to
surveil
the behavior of people
using alternative models, right? So
whereas whereas before you had to
completely rely on that training, which
by the way is important and critical and
you need to do, we're at the point now
where I can actually look at every
prompt that goes into the system. I can
look at every prompt that comes out of
the system. I can train the AI to look
for certain types of anomalous behavior.
I can train the AI on our code of
conduct.
So I guess what I again this is another
example where AI is actually going to
improve my risk mitigation controls at a
place like Morgan Stanley not decrease
them actually. Um and I think we have to
be creative. So yes, we want to empower
people in the use of these tools because
they will drive value. But I think we
also at the same time we have to
equivalently
ro raise the bar around the proper level
of controls and monitoring. And I think
where people will get in trouble is when
they democratize
but they don't put the training in
place. They don't put the guard rails in
place. they don't put the monitoring in
place, right? It's when you it's when
one side of the equation moves too
quickly without the other side catching
up. And that's why I think both are so
critical for an effective, you know, AI
ecosystem.
>> Jeeoff, just one quick point on that. Do
you think AI is a bottomup technology or
is it a top- down technology? It's my
job to
create the infrastructure, to provide
the tooling, to provide the education,
to provide the guard rails,
and make sure we've got the whole
ecosystem to both empower people and
ensure that this thing is done
responsibly. That's my job, and that's
the job of my team. and educating and
and then uh obviously learning you know
we were just out in the west coast you
know we probably met with a hundred
companies three weeks ago trying to stay
ahead and you know seeing what's the
latest in the marketplace and the
vendors and the models and whatever
right so that's that's the job of my
team but my job but I'm not the
innovator innovation happens
innovation happens with the people that
have the expertise and I think that's
the thing that I don't I don't think
people fully appre appreciate is that
what what's happening in the marketplace
is that
five years ago, if you and I wanted to
do something in our business, we needed
$300,000
and a team of three developers and three
months. And we wrote requirements,
documents, and then a technical
specification. And then you gave it to
the developers. And then and then, you
know, six weeks later, they'd come back
and you would complain that they didn't
give you what they want. And then the
developers would say that you didn't
write it correctly and then you'd kind
of cycle through until you ran out of
money and then you would claim the
project is done. I mean that's a lot of
how tech works, right? In 2025
much of that much of that work will
happen by you turning to the left and
saying to some young person on your
team, can you think about this? Or
better yet, they may just independently
say,"You know what? I want to do
something to make this client
relationship better, or I want to I want
to improve the way the compliance alerts
come out of the system, or I want to do
some analysis of this data." And I'm
just going to I'm just going to play
around with this for a little bit. And I
think, you know, if you want to call
that shadow AI, I call that
democratization of these tools. And I
think when you're delivering when you're
getting this technology
in the hands of real experts,
that's when you're really starting to
drive value, right? When the person is
able to not only create the
requirements, but build the solution and
evaluate the solution and test the
solution and deploy the solution and
then use the solution to drive value in
their business. It's a completely
different paradigm and we're already
seeing that. I think that's what's
really exciting. And I often say that
the thing that I'm most exciting about
excited about is the thing I can't see
yet. It's the thing that someone's going
to build or the solution that someone's
going to put in place that hasn't been
built yet that they're going to be able
to build because of their knowledge and
expertise of the business and their
creativity and their level of innovation
and their excitement. And more and more
of that is stuff is happening. So that
I'm the top down with the infrastructure
and the controls and the guardrails and
the bottom up is the is the inspiration
and the creativity of of the employees
that have access to these tools and
hopefully you know the partnership of
those two things is what drives value.
>> Jeff Morgan Stanley emphasizes human
centric generative AI. What does that
actually mean operationally? Because
different people have a very different
view from that. what and what would it
take for it not just to become a
corporate value statement but beyond
that.
So you know one of our AI principles I
think I mentioned at the beginning of
the conversation is we say human human
in the loop is actually what we say and
what that means is that every I'll give
you two examples you know if if you are
to use you know one of our intelligent
assistants to answer a question
we give you that question we give you
the answer that question we give you a
link to the document ment we give you
the fragments of the document that
source that information.
If for some reason
um the AI were to
um answer your your question
incompletely
or potentially inaccurately which by the
way happens very infrequently now we we
have very low degrees of hallucination
for a whole bunch of different reasons
which I could share. Um, but let's say
you know these machines make mistakes,
no different than humans do. That is
your responsibility as the as the
employee. There's no the AI made the
mistake, it's the AI's fault. Um,
similarly, if you're if you're own if
you're a process owner
that's processing new account opening
and the system does a bunch of stuff and
you're the the sales assistant and the
new account opening form comes to you
and maybe there's somehow there's a
piece of information at the end of the
process um that got put in incorrectly.
You know, first of all, you you're going
to be the one to submit that. The AI
doesn't like AI doesn't press the
button, right? That's kind of one of our
quotes. The humans press the button.
Now, the AI may prepare all the
documents. It may fill in the form, but
that form
is ultimately submitted by a human.
All the information, first of all, that
human knows that AI was used in the
preparation of that form.
All of the information that went into it
has links to the underlying source
information to include the fragments
associated with it. So we give full full
transparency
and uh you are accountable right and and
I think what's what's interesting is
that the nature of jobs are somewhat
changing in the sense that historically
the junior level job was the person to
capture and type it into a machine.
that job is no longer is moving from
that job to being the person who was
validating the information and the
person who before was validating
information is now the person that's
starting to synthesize the information
and the person that was synthesizing is
now the person that's sort of thinking
about how to presenting that you know I
mean like so what we're seeing is sort
of the lower-end stuff is moving away
and then everybody's job is kind of
moving up but going back to your your
point human centric approach means that
humans are in in the loop that they own
and are accountable for everything they
output. And then from a responsibility
perspective, from an engagement, like we
don't just leave them on their own. They
are given all of the relevant
information to help them make sure that
they're able to audit uh that
information and that they're fully aware
of like what what went into the recipe
that got them the output that they that
they did. And that's that's on that's on
me and my team.
>> Okay. So the human in the loop I think
is a critical aspect. Jeeoff uh with AI
is increasingly shaping lending, wealth
management, investment decisions given
the scope of what all uh you do, how
does one proactively detect and mitigate
bias before it impacts clients? I mean
I've had leaders of foundation companies
tell me that there is going to be some
level of bias because that's just a
fact. But what is your perspective on
this?
>> First of all, there are actual
regulations in the banking sector that
require us to prevent any type of for
example lending. Um so this is not again
a new concept for us and the short
answer is we have to
we have a respon whe whether whether
there is a legal obligation or you know
just a general responsible obligation.
We need to make sure that our models
are operating as independently as they
possibly can. And again to your point,
we live in a world with lots of
different you know influences. Uh these
models
are a function of human training which
has in of itself many biases. Um I would
say two things. Number one, when uh when
we build stuff and if there is bias
associated or concerns around it, we
will do adversarial testing, independent
adversarial testing to essentially root
that out and if and when we identify
issues of that, we will work to mitigate
it. Uh the second thing is I would say
and and I know this is maybe a little
bit of a technical point. We generally
don't rely on the models as as sources
of information
as much as we as rely them as sources of
language. I call it language
computation. And what I mean by that is
we rely on the knowledge and content and
data of Morgate Stanley
that is fed into the large language
models which is then retrieved back to
give us the answers.
And by doing that and by only feeding at
the knowledge of Morgan Stanley, we are
able to substantially reduce
issues around bias.
Traditionally where you have more of an
issue with bias is in what I'm going to
say traditional machine learning
algorithms
where they are trained on a subset of a
population
uh that is a demographic and in those
cases we actually will do testing
uh we will actually formally do testing
against populations age race and whatnot
gender and we will actually test those
models to see if they are distributing a
product or service or a loan
disproportionately to uh that
demographic population. And in those
cases, it'll actually be quantifiable.
But in the case of large language
models, if we're doing something that
has any form of concern around that, and
some stuff does and can't believe some
stuff doesn't, then you actually have to
test because the whole point with these
large language models is you can't go
into the model itself. You can't
actually quantify
the percentage of, you know, this
algorithm and that it's doing this
versus that. So, you have to go in, you
have to give a hundred people, you know,
a chance to ask 20 questions and you'll
say, "Listen, can you try to break it?
Try to break it. Try to get it to say
something you don't want it to say and
see what happens." that we do a lot of
that and sometimes it results in finding
things that you know you don't want it
to find and then you got to go and fix
them and sometimes it you know it
doesn't but either way I think my
message to people is is if you first of
all you should always be asking upfront
what are the risks and one of your risks
is bias and there's many other risks and
if you have that concern then you have
to actively work to try to find it in uh
in the solution that you're building
>> do you have any concerns? I mean, you're
taking your data with these large uh
language models, foundation models in
terms of explanability at all, Jeff.
>> I think the better question is how does
a regulated institution
deal with the fact that there is no
explanability on these models? And I
think the answer is you have to take
what we call as an input and output
approach. So you say I'm going to take a
hundred
questions
And then I'm going to take a hundred
answers that I know are good answers and
I'm going to see how the AI does against
it. Does it get a 100 out of 100 right?
Or does it get 50 out of 100 right? And
if it gets 50 out of 100 right? Why did
it get 50 out of 100? And what do I need
to do? Is it a data quality problem? Is
it a prompting problem? Is it a source
problem? Like what is the problem? And
then what you end up doing is you you
use a combination of prompts, adjustment
of the content, and you you continue to
sort of adjust those things till you
sort of get to the 80s, the 90s, the
99s, right? And then once you get to
sort of that high high degree of
confidence,
then you're in a position to give it to
people. And then you get 20 or 30
experts. And this is really important.
You have to get your best people. You
can't get your sort of low-end, you
know, temps to do this work. You need
your best people. You want your best AI.
You need your best people to engage. And
then you have them input information and
to do output. And then they evaluate.
And we actually have tools that do this.
They literally will say good, bad, and
if it's bad, they have to describe why.
The team collects all the bad answers.
They look at it. They think about it.
They make changes. They make changes to
prompts. They make changes to the
content. And then they'll go ahead on a
Friday afternoon, update the prompts,
update the content, and then they'll do
it again. Now, what happens Sunday
sometimes is you'll fix one problem and
then you'll create a diff you'll create
a different problem, right? Because the
this is not like a linear this is not
linear technology, right? It's very it
can get it's non-deterministic.
So, you know, in some cases, you can
solve this in a couple of weeks. It may
take you a couple of months. It may take
you 12 or 15 turns and going through
this and it's really a function of what
how complicated is the problem and what
is the quality of output that you need.
And so this input output approach is the
approach that you honestly it's the only
way to do it. You can't just have a
people say yeah it looks pretty good to
me. You've got to be very disciplined.
You have to audit the information.
You've got to keep it in a location and
then at the end of that process,
somebody's got to say, you know what,
this is of a level of quality that I'm
comfortable enough to go live with. And
then, by the way, then you've got to put
monitoring in place to make sure that on
an ongoing basis, these systems are
performing um consistent uh you know,
consistently of a high quality. And by
the way, like you've heard me say this,
like this is real work. Like I don't
think people necessarily appreciate like
it's not like I think sometimes people
think like AI is find me the best dim
sum place in Hong Kong, right? Like what
I'm describing you is a very complicated
process that requires a lot of time and
effort. I mean it's not it's not
complicated so much it is just a lot of
work with a lot of smart people and a
lot of time. Uh but you know that's what
it takes to really create high quality
systems. When you say bring your best
people, do they have to be crossf
functional or what what do you mean by
that?
>> I think it really depends. I think what
I would say is if you're building
whatever system you are building, who
are the people that know that system? So
yes, maybe crossf functional. If you're
building um a compliance system, you
probably need compliance people. If
you're building a banking system, maybe
if if there are compliance people that
have specific expertise in that system.
Again, Mike, I don't know anyone in this
on this podcast organization, but if
you're building something, you should
ask the question, who are the people
that know the most about this thing? And
the only point I would just highlight is
that those very same people are the very
same people that are probably doing
really valuable stuff elsewhere. And it
takes it it is hard for leaders to free
up the capacity of those very those very
strong people. And where I'm seeing the
most success is when leaders are willing
to take the hit and actually extract
very senior successful maybe revenue
driving or senior risk people or
whatever that they're able to extract
them from their organization on a
part-time or even full-time basis and
dedicating them to the effective
deployment of these tools. So bring your
best experts, lot of input, output, a
lot of hard work. Jeff, uh just quickly
a final point on that synthetic data.
Any thoughts on that from your
standpoint?
>> You can actually use Genai to create
pretty decent synthetic data. So for
example, I'll give you an example. If
you wanted to let let's say you didn't
have you wanted to get it going and you
didn't have all your call transcripts
you let's say it was going to take you
three months to get all access to the
call transcript because they were you
had to get through security architecture
and all these issues you might be able
to take you know a dozen or so
anonymized call transcripts
use Genai to create 10,000 of them right
adjusting the topics adjusting the tone
adjusting And while they may not be
perfect, right? Um they will create I
mean for something like that they'll
create a representative sample. Another
example is like you could create 500
trust agreements
anonym with with fake with fake names,
right? That would be that would be
pretty good. Uh Jedai does really well
on commonly
produced contracts, trusts,
living will. I'm talking about things in
my space. But if if it if it's content
that is also you know database schemas
you can do stuff if you if you sort of
give it examples. I mean where generally
really does well is say okay I've got a
schema that looks like this. The
distribution looks like this. I want you
to create 10 million
records that looks like this because I
want to do performance testing. Those
types of things it does really well. The
only thing that I would just say is that
for all of the great work um synthetic
data does and it's very helpful. It will
help you get moving,
I would be very hesitant to fully go
live with a system without actually
doing real testing because inevitably
synthetic data is synthetic. And if
you're going to really identify the true
issues before you actually flip the
switch, you would I would strongly
advise that you want to actually do real
stuff. And that may be me running in
what you know we call prod parallel.
That may be actually taking real stuff
through the system but not in in a
production environment in like a test
environment for maybe two or three
months um and sort of running them in
two different systems. See how they well
do against each other.
whatever it is is don't assume that
synthetic data will will flesh out all
the issues but certainly helpful to get
you moving.
>> We have a patchwork around the world of
AI policies. When you look at EU AI act
um you look in the US we don't have a
federal policy. We have now states that
are jumping in.
um yours is a global organization that
has to make sure it's complying with how
difficult does it make it uh from that
standpoint and because if it is
difficult for you think about small
companies etc and this is something that
you know when I have lawmakers policy
makers from around the world on there I
kind of bring it up any because a lot of
them are going to be listening to you
what are your thoughts on that
>> you know when I want to turn on my
system for recording meetings. I had to
do a 23 jurisdiction analysis. It took
me nine months,
right? I had to go through every single
country and understand every single rule
because, by the way, like Morgan
Stanley, we care, right? We're not we're
not going to simply just say, "Well,
we'll let it see how it goes, right? We
we are respectful to the regulations of
every jurisdiction.
Um, you know, the EUA act uh has a whole
set of requirements.
Um, it does not, I want to be clear, it
does not stop us from doing anything
specifically, but it probably adds six
months to every timeline that I have
because of the work that I have to do in
terms of documenting
um, service level agreements between the
legal entities in Europe and the US.
um putting the right controls in place.
Um and I don't, you know, I'm not here
to argue whether any of those laws are
good or or not. I think I'll leave that
for other folks to to to debate, but I
will tell you it is it is really hard.
Um, and you know, we have a lot of folks
on our team whose only job is to keep a
breast of every country's
emerging regulatory AI. By the way, to
include the states, California just came
out with something new. um and really
trying to do our very best to make sure
that we're always
um you know we're always uh in front of
that stuff and and adhering to it. But I
will tell you it is very very hard. Uh
and it definitely costs us a lot of time
and candidly I'm not I'm not necessarily
sure
it's always it's always it's always
helping the intended purpose.
>> How were your experiences in West Point
in the US Army? Did they shape any of
your approaches to leading AI
initiatives at Morgan Stanley?
>> Oh well I wasn't expecting that one
Sanjay. Um well, first of all, um it's
uh we're recording this around Veterans
Day. So, first of all, happy happy
Veterans Day. And I want to thank all
the veterans out there for their
service. And by the way, sorry to do
this unabashed plug, Sanjay, but if
anybody wants to help a veteran, uh help
them transition from the military into
the civilian world. Open up your
network, help them find a job. Uh we do
a lot of work here at Morgan Stanley to
try to help veterans. And I would just
encourage your audience um around the
world uh any country uh to support
support your veteran community. Um that
being said uh I would say three things.
Uh number one I think it was Eisenhower
who had a great quote that um
plans are useless but planning is
everything.
And I I I've always loved that quote
because
you need
you have to be disciplined and
organized.
You have to bring the right people
together. You have to have a standard
operating procedure, guiding principles.
Um you have to have a a a program
management office.
You need structure, accountability.
You absolutely have to do that and you
have to be locked in. And then all that
being said, as soon as you put that
together, you you realize that the
problem that you thought you were
solving for is actually not the problem.
You realize the technology that you
thought you were bringing in is not the
technology. You realize that the risks
that you thought you were solving for
are not the risks. And then you have to
adjust. And I think the act of being
organized and structured allowed you to
come together as a team and then
appropriately pivot and deal with the
unknown as it came up. So I'm not in any
way suggesting just let it fly. I am
saying be organized, be structured, but
I'm also saying be flexible. So that
would be number one. Uh the second thing
I would say
um you know when when I was when I was a
young officer they would do these
training sessions and they would have
eight they would have eight um actual um
battles and one of those eight battles
was was scored at 50% of your total
grade and that battle is the one where
you as the officer were taken out of the
of the battle. So meaning that your
soldiers were operating without you as a
leader. And the point they were making
is that the effectiveness of your unit
without you there was the greatest
indication of how well your organization
was. And that I've often said to my my
team and even my boss that every day
that my team becomes less less relevant
is a is a better day for Morgan Stanley
because what we're trying to do is we're
not trying to create an AI ZAR or this
AI power center. What we're trying to do
is we're trying to empower people to use
this technology in a responsible way.
And that success for me is success for
other people,
right? It's not about Jeff McMillan.
It's not about my team, right? It's
about giving this tool and letting
people do what they need to do in a in a
in a in a proper controlled way. And and
and any success that happens at this
firm, I feel great about, right? Even if
I don't touch it. And some of the most
things I'm most proud about are the
things that just sort of like, wow, that
happened. And I didn't even like I
wasn't anywhere near it, right? And I
think what good leadership about is
about setting up those set it's about
setting up environments where people can
succeed and creating cultures where
people collaborate and work together. Uh
that was true that was true 30 years ago
and I think it's true today. Last thing
I'll say, Sanjay, and I know this may
sound a little trit, but um one of the
things I in the military is if you've
showered in the last seven days, if
you've eaten in the last 24 hours, if
you're not getting rained on um and
you've been able to change your socks in
the last week, that you're having a good
day. And the thing I learned in the
military is that um you know, that's
work, right? And and by the way, nobody
got shot at today here at Morgan
Stanley. Um, and we're pretty safe and
we're pretty secure and we've got a lot
a lot to be thankful for. Um, and uh I
think the probably maybe the greatest
lesson is that you know perspective,
right? And and I think uh if anything
I'm grateful for the the maybe some of
the challenges I had experienced I was
in South Korea and some other places um
you know cold and wet and damp and other
things. And uh I certainly am much more
appreciative of my time not in a foxhole
in the middle of winter um you know
today um than I than I was then. And I
think that that experience has made me
much more um I guess a little bit more
humble, a little more grateful.
>> Uh great messages. Uh first I think a
very important one is as as you said
around the world if you can help a
veteran transition I think that's the
most important in whatever way u jobs
opportunities training etc. And I think
uh Jeff talked about a few things that
he picked up which are very very
valuable. be structured yet flexible.
Empower the team. Uh be thankful and
have perspective. Jeff, I think is what
you're saying. And that's uh stood you
so very well. Um Jeff and thanks uh Jeff
for bringing such clarity, such
pragmatism and wisdom to you know such a
critical questions about AI governance
which are rooted really in managing AI
at such a large scale of a regulated
industry.
And you know you offer a bridge between
the theory and the practical realities
that companies policy makers fake every
day. And the stakes for getting AI
governance right are enormous. uh as I
think you said the decisions we make in
[music] the next 12 24 months will make
really make a difference whether AI
becomes a tool for broadly shared
prosperity and innovation or it becomes
you know a growing sense [music] of
inequality and we need voices like
yours. So thank you so much uh for this
and to our listeners AI policy leaders
entrepreneurs I hope you walked away
with a deeper understanding that
responsible AI isn't a constraint on
innovation it's the foundation for you
know sustainable and trustworthy
innovation and um we really have to
thank uh Jeff for that Jeff thank you
Jeff really
>> thank you Sanji I really enjoyed it
>> thanks for tuning in to the reun ating
AI podcast [music] with Sanjay Puri. If
you enjoy today's conversation, don't
forget to leave a comment. We'd love to
hear what you thought. Share it with
someone curious about the future of EI.
And join us next time for more stories
and insights [music] from the leaders
shaping what's ahead right here on the
Regulating EI podcast.
