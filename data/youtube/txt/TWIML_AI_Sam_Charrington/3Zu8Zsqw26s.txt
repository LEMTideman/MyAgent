all right everyone welcome to another
episode of the twiml AI podcast I am
your host Sam sharington today I'm
joined by Ram Shara ROM is VP of
engineering At pinec Con before we get
going be sure to take a moment to hit
that subscribe button wherever you're
listening to Today's Show Rah welcome to
the
podcast thanks for having me Sam it's
great to be here uh thanks for coming on
the show I'm looking forward to chatting
with you we'll be talking about all
things Vector databases and retrieval
augmented
generation uh before we jump into that
though I'd love to have you share a
little bit about your background and how
you came to work in AI uh great question
so uh in another life I did a PhD in uh
theoretical physics uh from there I
moved uh to different fields so I kind
of spent some time in Goldman uh working
in finance uh from there I moved to uh
the west coast to John Yahoo this was
around 2010 and I stayed there until
about 2014
uh that was my first exposure really to
Big Data Systems uh large scale data
processing and uh machine learning at
Yahoo I worked in different areas around
machine learning eventually ended up in
yahoo research where I was focusing on
scalable machine learning from there I
left to uh you know over time join datab
Brooks where I spent some time working
on spark but also starting new
initiatives like gmic and so on uh from
there I went to uh Splunk to head the
machine learning research group there
that's some point of time in this in the
course of doing all these things I
wanted to start my own uh company and I
was thinking about doing that when I uh
reached out to Ido uh who's the CEO of
pineco and we started talking and we
realized that we kind of trying to solve
very similar uh overlapping problems and
uh that's kind of how I ended up teaming
forces with EO again and joining Bank it
it strikes me that a lot of our audience
probably has no idea how huge Yahoo was
in terms of developing Big Data
infrastructure and some of the early uh
kind of commercial uses of machine
learning in support of search and ads
and so many things absolutely I think uh
you know some of the best uh work in uh
Cloud systems um machine learning online
machine learning and so on had kind of
come out of Yahoo people who worked at
Yahoo who eventually went to uh places
like Google and so on MH so yes it's
been certainly formative for the whole
industry but for me personally as well
it's one of the it's it's it's really
the place where I learned a lot of lot
about the basics that was needed to kind
of work on Big Data Systems machine
learning and so
on and you know we'll be talking of
course about Vector databases and and
why they have suddenly become so
interesting but you mentioned um
connecting with the the founder of the
CEO of pine cone um does this predate
Rag and llms and and all of that has
pinec con been working on Vector
databases uh for a while now yes so Pine
con has been working on Vector databases
for several years now uh I myself joined
Pine con about uh two years and six
months or so back okay now uh while Rag
and llms are kind of uh becoming very
popular now the the research and uh
development on Rag and uh llms have been
happening for a while language models
have been getting bigger and bigger for
a while so uh it's it was only a matter
of course that they would end up here uh
so we were aware of the llms we were
aware of the trends in language models
we were thinking already about how uh
Vector databases could be used in these
sort of flows and so on but uh the
industry as a whole has started taking
off really only in the last year around
these topics yeah yeah yeah so elaborate
on
uh on that from from your perspective
you know when you think about how Vector
databases have you know suddenly come
into the Limelight and uh you know
people who wouldn't have thought about
deploying a vector database previously
are now trying to figure out what this
thing is and how do I use it like tell
us about the evolution of that from from
your perspective and and you know is the
attention being placed on Vector
databases uh you know is it appropriate
is it uh what's the role of the the
vector databases and what folks are
trying to do now that's a great question
uh I think before uh talking about
Vector databases it might help to set
the context a little bit Yeah so uh you
know chat GPT and uh models like that
are what we call large language models
these large language models are really
just sequence to sequence models so they
take a sequence of text and produce a
sequence of text right now you can use
this for a variety of tasks you can use
this for summarization you can use this
for search you can use this for
literally you know new new use cases are
coming up on a daily basis uh what has
made them very powerful is that these
models are really large they in some
sense embed the world's knowledge in
their parameters so they are trained on
a lot of data they embed uh both the
structure of language itself as well as
the ability to reason in their
parameters uh and then they can be used
what makes them really powerful is that
now they can be used across tasks that
they weren't specifically trained for
okay and this is what really made them
very powerful now the best way to think
about large language models these days
is that they are uh the intelligence
layer or the orchestration layer uh for
emerging generative AI applications okay
what I mean by that is you can use them
to actually orchestrate uh intelligent
AI uh flows uh you can also use them to
reason about things okay MH um so they
act as the intelligence and the
orchestration layer but something is
missing if you just have an llm if you
just have a large language model you're
missing what we call the knowledge layer
okay so uh what you're missing is the
actual knowledge required to perform a
lot of knowledge intensive tasks right
that's really where Vector databas is
com in now large language models to some
extent embed their embed the World
Knowledge to some extent in their
parameters right so it's not like they
don't have any knowledge they do have
World Knowledge they do have the ability
to reason based on that knowledge and so
on but uh they what they lack is the
ability to access accurate uh you know
relevant knowledge and that's what
Vector databases provide now to to
discuss how Vector databases provide
this you kind of need to take a little
bit of a step back and talk about the
other piece of this this uh entire
workflow which is retrieval okay so if
you talk about uh retrieving relevant
accurate knowledge now this is something
that people have been doing before this
this used to be something we used to do
even before there were large language
models right if you think about search
engines like Google if you think about
uh uh any sort of a search engine uh
over text documents for example that's
what it does what it does is you take
those documents you put them in this
sort of a search engine then you can ask
questions about it you can you can
search over it what it does what's
called retrieval and the field of
information retrieval is very rich very
well understood in the classic sense of
retrieving text documents based on
keyword matches this is something that's
uh very very well understood but over
the last maybe half a de decade or so
there's been a new type of retrieval
that's already been happening and Vector
databases have already been used for
that which is what's called dense
retrieval or retrieval using vectors so
instead of doing just keyword matches
over whether your Corpus contains
keywords that match your query and how
relevant are those keywords to
the the query itself this used to be the
traditional focus of information
retrieval what's happened recently over
the last five seven years is that people
have realized that if you take those
documents if you embed them using these
uh neural networks what we call
embedding models and you put them into
something like a vector database now you
can search over them much more uh much
better meaning your searches give you
much more relevant context uh much
better retrieval can be achieved uh
doing this and what do Vector databases
really do here they act as the evolution
of search engines right so what they do
is instead of searching over uh text uh
by breaking up text into chunks of
keywords and then creating posting lists
and then somehow searching over them
this is what search engines used to do
in Vector databases retrieval Works
differently what what happens is that
you take this same text you chunk it up
somehow each chunk is embedded with a
neural network to produce an embedding
an embedding can be just thought of as
an array of floats an array of uh
floating Point numbers right and you
take these embeddings and put them in a
database like a put them in a vector
database and Vector databases are
uniquely positioned to answer the
following question which is given a
query you encode the query as well and
now you have a query vector and you can
ask a vector database give me the most
relevant candidate document vectors
corresponding to this quy vector
and this this particular interaction
retrieves documents for you that are
most relevant to the query and does
what's called semantic search semantic
semantic
retrieval uh so modern information
retrieval already does this which is
modern information retrieval uses Vector
databases as the core of retrieving
accurate relevant knowledge it's been my
observation that you know as llms and
interest in chat GPT and the like have
wrapped rapidly expanded the the field
of AI that the connection between rag as
an approach to getting a chatbot to work
and search and information retrieval is
not really fully appreciated by folks um
yes and the folks that I've seen have
the most success in getting you know
beyond a rag demo to something that's
actually useful have a lot of experience
previous experience working on search
and relevance types of problems and all
that knowledge comes into play in making
a chatbot that a user is actually going
to want to you to use because it's it is
you know relevant and timely and and
provides the right information do you
see some of the same things absolutely
so in fact in fact all of the all of
this work that has gone into retrieval
uh relevance ranking and so on is what
is being used along with large language
models to connect to give them knowledge
right so when we talked about it we said
large language models really don't have
knowledge the knowledge comes from
retrieval MH and all that's why you're
finding that the people who are able to
uh use retrial argumented generation rag
workflows the best are people who are
really uh invested in doing the best
retrieval so put that in another way you
can think of you can think of a a chat
bot that's based on rag as doing you
know executing a search on your
documents based on a query retrieving or
pulling out the results that are most
relevant to that query and then the role
of the llm is just to summarize that and
make it you know present it to the user
and so if you don't do a good job on
that retrieval you've got like a garbage
in garbage out type of problem and the
chat bot's not going to be able to
produce useful results absolutely uh
it's even more interesting than that
which is remember I told you that uh
llms have been trained on large amounts
of World Knowledge MH you could take the
same World Knowledge put it into such a
vector database and if you do retrieval
really well you're making the llms
better even on the knowledge that they
were already trained on forget about the
knowledge they had no access to like
your your own documents or some
corporate uh uh legal documentation
things that they would never have access
to right so even on knowledge that they
already have access to retrieval
actually provides better results and so
to maybe put that into to context what
I'm hearing you say there is when you're
using an llm and
retrieval you've got you know you're
you're talking about two algorithms one
algorithm is a kind of sequence to
sequence next token prediction algorithm
and the other is an embedding based
information retrieval algorithm and
you're essentially saying that embedding
based information retrieval is better
than sequence to sequence at knowledge
production is that too strong a
statement or is that yeah I think the
best way to say it is that the two used
together the sequence to sequence models
like llms used together with information
retrieval through a vector database is
strictly better than using llms by
themselves or or even fine-tuning llms
to do something right so purely just
retraining llms on your data doesn't
provide you as much of value as using an
llm under context with a retrieval
engine like vectory dat bases so this is
what's called rag rag is the combination
of the and ands out to the best way to
provide knowledge to these sort of
knowledge intensive
tasks and so I alluded to this earlier
you know you can go online and search
out you know rag demo and you know
independent of your language or
framework or database of choice like
there are tons of um you know code
walkthroughs and blog posts and things
like that that can uh help you you know
create a demo of this like it's very
easy to to demonstrate but that's just
the kind of the beginning and delivering
something that you would want to put in
front of your users is a lot more
complex can you talk about some of those
complexities and some of the challenges
that you see folks running into yeah I
think there's a multitude of
complexities some of them are just on
the infrastructure and kind of the
scalability side which is it's it's one
thing to create a demo with a few 100
documents or a small number of documents
and kind of do retrieval over that doing
retrieval over billions of vectors is an
entirely different problem right that's
kind of why uh some people at you know
uh people like Pine con spend uh most of
their time uh building and perfecting
Vector databases because doing this at
scale is very very hardh the other thing
is that uh outside of uh outside of
simple demos your Corpus evolves so
people are adding documents deleting
documents uh editing them redacting them
and so on and being able to take the
latest information and make it a a
available for this sort of a rag
workflow is again a very hard problem
right uh this is again where this is
also where the database part of vector
databases comes in because in in in this
sort of flow it's acting really like a
database okay and in essence you're
synchronizing your Enterprise data
stores with this embedding store exactly
exactly and sync is historically really
challenging
very challenging but particularly
challenging for Vector databases I think
traditional databases have gotten really
really good at doing this and kind of
keeping their indexes fresh and so on
Vector database indexes are very very
challenging to keep fresh in fact very
few people have cracked that problem why
is that how do you keep these uh I think
the the biggest problem is that it's
easy to keep an index fresh if your
index can be built incrementally right
okay for Vector databases your uh very
few algorithms exist that actually build
indexes incrementally right because in
some sense uh when you put a document
and encode it and put it into this sort
of a database you can't just
incrementally uh you know add
connections to the rest of the documents
in some small region of space because if
you do that uh when a query comes the
query doesn't has no idea that there is
a new document in this region of space
it has to prove right so either the
query has to scan everything for it to
know that or the query is going to miss
the document out and neither of those
are good right if the query has a scan
the Entre
Corpus that's actually computationally
intractable right whereas if a query you
know is only looking at the places that
it already has somehow indexed or
pre-computed it's not going to know that
a new document was added here so that
process of keeping the sort of uh Vector
indexes fresh is fundamentally a hard
problem okay and this happens to be a
hard problem for every known vector
vector search algorithm out there so
also where we spent the last good half
of a year actually close to a year
thinking about and solving and figureing
out okay and are these problems like is
this research Frontier problems like
you're implementing you know Cutting
Edge algorithms search algorithms or um
are they engineering problems this is uh
equal parts both research and
Engineering so uh you know we uh the way
kind of typically it works at least in
Pine but in general I think also is that
we uh we are pretty pretty much at The
Cutting Edge uh researching things and
testing it out on lots of data sets uh
figuring out what works what doesn't
work uh and then when we feel confident
enough that we have something that works
for the majority of the use cases and
provides a lot of value we start
engineering and we start building but at
the same time we are continuing to
research like something like keeping
indexes fresh and so on is a is will
continue to be a research problem there
is a lot to do over the next few years
to build world-class Vector databases
this way but we know enough to be able
to provide a lot of value to today so
it's it's really striking that balance
between research and Engineering mhm
so recapping kind of where we are with
regards to challenges there's these
fundamental infrastructure challenges
associated with scale there are
challenges associated with keeping your
indexes
fresh at scale at scale yeah there's a
couple of other challenges by the way
yeah there is challenges around just
cost right a lot of generative AI
workflows today or if somebody's
thinking about building a Genera a
application MH uh they they have to
worry about cost because a lot of these
applications today are actually very
expensive mhm solely because of there
because of hitting uh an expensive
inference end point or are there other
reasons exactly it's it's part part of
it right for example open a end points
actually open AI has done a lot to
reduce the cost per token but even then
it's still very expensive and then you
have these uh open source models and uh
you know I would call them today at
least weaker models uh which are cheaper
but they're weaker right so for
knowledge intensive tasks people
obviously want to use the best models
out there MH uh here again you can make
weaker models more powerful by adding
more context to them and by actually
leveraging Vector databases so on so we
spent a lot of time trying to figure out
how do we make these uh cost prohibitive
applications today 10 times cheaper so
that you can unlock new use cases in the
way that you couldn't before and the
last part I missed out in terms of the
question you asked was this is
infrastructure and cost but there is an
even bigger question which is quality MH
now historically llms have struggled
with hallucination they've also
struggled with things like attribution
right so faithfulness are they actually
faithful to the documents that they have
knowledge of or are they faithful to the
context that you provided to them and so
on measuring this and making sure that
uh you are building rag applications
that are actually correct and and giving
you uh answers that you can be confident
about I think that's one of the big
challenges as well and even here Vector
databases and Rag and so on can help
significantly so these are all I would
consider equally important challenges so
Ron those are the highlevel challenges
that folks are running into a couple of
uh more in the weeds things that I hear
about a lot are the choice of embedding
model and the chunking strategy and I
guess there are a lot of these little
things that someone has to
these decisions that people have to make
as they uh build and deploy rag
applications but what is your sense of
how important these are there seems to
be some controversy out there as to
whether these are key considerations or
secondary considerations that's a great
question uh so uh first of all there are
key considerations I think what's
happening right now is that people are
kind of reaching out for things that are
easy at hand right which is always the
case when initially you're trying to
build
applications that you're trying to get
off the ground uh for example embedding
models that uh if you're using open AI
for uh for your llm kind of makes sense
to use their embedding models and maybe
the latest embedding models uh that's
that's what we see often which is people
are just using the things that are easy
at hand which makes absolute sense uh
however the choice of the embeding model
is very important uh it's actually
important in multiple ways one uh
smaller cheap per embedding models could
actually do the same job in which case
you're kind of overpaying a lot uh so
it's important in that sense in another
sense embeding models that are
fine-tuned for specific things that you
uh you're trying to do can actually help
a lot so again uh the choice of the
embedding model is pretty important okay
I would say more important than that is
actually chunking strategies so uh how
you how you go from text to vectors is
probably one of the biggest parts of uh
making the rag workflow
uh or improving the quality of the rag
workflow okay uh so I would say there
are two big parts to improving the
quality of rag workflow which is how do
you go from documents to vectors which
is where chunking strategies come in and
there's a many ways to think about it
but fundamentally yes CH checking
strategy is one of the most important
things you could get right in this
workflow the other one is how do you
once you get uh you know relevant
passages or relevant pieces of documents
and so on what do you feed to the
language model itself do you just put it
all together and feed it do you do
something more on top of it do you
rerank how do you rerank uh basically
the second stage after retrieval is
becomes very important as well in this
workflow yeah yeah okay um so with all
that in mind from a challenges
perspective talk to us about what's
happening in the in the vector database
to accommodate you know
these new styles of applications and the
accordant challenges that have emerged
that folks are trying to to overcome
absolutely so first of all we just
released uh what we call Pine con seress
which is uh which basically unlocks an
entire new set of use cases so up until
now Vector databases unlike traditional
databases were very rigid so uh you know
if you go to the Pod based architecture
in pineco today you have to pre-specify
the number of PODS that you use
and you have to pre- understand how much
data you want to put in there every time
you go beyond that you have to kind of
rehard and re in some sense uh reshuffle
the data around all of this is very
expensive so you've got to make these
kind of fundamental
infrastructure clustering type decisions
before you even know like how far you're
going with this app and what you need to
you know what you're going to run into
exactly exactly and if your use case
changes you have a serious problem right
for example if you want to take the same
data and try to use it for an on demand
use case now you're paying 1000x more
because you're only querying on demand
you're not querying all the time so
you're having these things sitting there
and taking up memory and space and cost
that you don't you don't need so
depending on your use cases you would
have a lot of inflexibility in the past
you would have had a lot of
inflexibility in the past in how do you
best leverage Vector databases uh now
traditional databases have solved this
problem over the course of the last 30
years Vector database is just catching
up and pine con took a very big step in
this Direction with pine con servus the
other thing that uh we we are doing here
is remember I told you that uh you can
actually improve llms on their own
knowledge by putting that knowledge into
a base this again unlocks uh 10x cost
reductions for generative AI
applications and so on but this again
only becomes possible if the cost per
query is very cheap so if you have if
you have to pay a lot to run a single
query that uses I'm only putting my most
important data you're no longer going to
think about exactly you're not going to
put a billion vectors in there right so
uh what we have done over the over the
past year is worked on how do we rethink
Vector databases so that you can achieve
10 to 100x cost reductions across a
variety of use cases secondly you can
you can make it very flexible people can
throw in their data and think about use
cases later because we know that use
cases are going to evolve we know that
people are going to find new use cases
for this data and so on you don't want
them to think about all of that up front
right it's it's very inhibiting for uh J
workflows so that's what that's what
we've done and to do this we kind of had
to reimagine the database itself you
know traditional approaches don't work
and so what does what does that
reimagining entail like what how did I'm
making some assumptions about the
architecture but assuming it was a
traditional monolithic architecture like
how does that need to shift to become
now serverless that's a great question
so
uh all Vector databases prior to us
releasing Pine code servus they operate
on this What's called the search engine
architecture right so which is your data
is split up into a bunch of shards these
shards contain a subset of your data
indexed always available which means
it's on these machines that between a
combination of RAM and SSD they're kind
of always up and running this makes
sense when you're running like tens of
thousands of queries per second which
need to touch your entire Corpus right
if you if you're doing that then yes
that architecture kind of makes sense MH
but if you're running queries on demand
or if your queries are not touching the
entire Corpus which for a web scale
Corpus a single query is not going to
touch your entire Corpus right and you
don't want it touching your whole carpus
for scenarios like that now this becomes
an extremely expensive workflow right
and this is what customers of pine con
were also finding because Pine con was
also using the same architecture uh in
in our pod based architecture so the
first thing you have to do to make these
sort of uh workflows the emerging
generative AI workflows as well as many
other other workflows that customers at
Pine con do if you want to make them 10x
cost effective you kind of have to
decouple storage in compute okay you
cannot have all of the storage for the
index sitting close to compute all the
time right now this is something that uh
traditional databases have started doing
really well over the last several years
so if you're familiar with snowflake if
you're familiar with uh you know uh
spanner and systems like this they are
they're great and they've taken Decades
of uh database uh learnings and figured
out how to make to separate storage and
computer so you can drive down costs for
using those databases right what we
needed to do was to rethink how to do
that for Vector search because remember
I told you that Vector search has this
fundamental problem that every time
you're trying to update an index you
kind of have to have in some sense a
global view of the index yeah okay at
least in existing algorithms you have
similarly all existing algorithms that
you're familiar with whether it's hnsw
whether it's F whether it's practically
anything that anyone does today they
hold the entire index between memory and
local SSD okay okay in fact most
algorithms up until two years back
weren't even using dis they were all
memory based and memory is very
expensive on the cloud even now right uh
but even disk based systems are actually
very expensive because there is no way
to page anything from a cheaper storage
they have to you have to maintain
everything on local ssds and on the
cloud you don't have the fungibility of
ssds you know being decoupled from
compute or something when you get a
machine on Amazon or gcp you're getting
uh you know lot of SSD lot of CES lots
of ram that's the whole box that you get
right there is no fungibility even with
the ssds so the best way to drive cost
savings is to not to really decouple
storage and have it in some cheap
storage like blob storage but the moment
you put things in Blob storage you have
to get very efficient at incrementally
indexing that thing incrementally
pulling out only the parts of the index
that queries care about and this
requires fundamental re innovation in in
how Vector search even works that that's
kind of what we did MH so I would say
that the core of the Innovation is
around reimagining Vector search in such
a way that you can actually decouple
storage and compute and Page parts of
the index on demand so that queries
don't have to look at basically the cost
of the query is no longer the cost of
the whole data under management it's
only proportional to the cost of the
parts of the data that the query even
needs to look at presumably you're still
doing this on the cloud and you're but
you're just using different Primitives
than you were before maybe before you
you were using large machines with lots
of SSD now you're using what uh no so
it's not just the machines that are
different it's the algorithms themselves
that are different I get that before
what we used to do was we would take all
of those the entire index preload them
or even keep them fresh and keep them
loaded uh onto these big machines right
today what we can do is we can still
have these big machines okay but now
they are only loading the parts of the
index on demand they're caching the
parts that uh have been retrieved and
found frequently used so that the
frequently used parts of your data are
actually cached and are and so you can
have very low latencies and so on okay
but then you're not paying the cost for
all the data that is not being touched
that's the core Innovation got it got it
yeah when you mentioned you mentioned
blob storage and that made me think that
data was being pushed to S3 now as
opposed to online uh storage okay yes
exactly it is and but the trick is in
figuring out what parts of the data do I
need to keep close to my compute right
when the queries need to be answered and
so is this things like um you know
predictive query optimization and that
kind of thing so I think that what we do
is basically when you create this when
you index or reindex or when you keep
your indexes fresh and so on you want a
way of partitioning them right in
traditional databases you're familiar
with range partitioning you're familiar
with different ways in which they can
slice up uh ranges of of columns so that
you can say that if I'm getting a query
for let's say time stamp greater than
100 I know that the range I need to look
at is everything greater than 100 I
don't have to even look at data that has
time stamp less than 100 right and you
do this by creating these uh breaking up
data into chunks each chunk has some
statistics around it you then use that
to figure out what do what do I even
need to look at and that General
philosophy is called partitioning right
see you're partitioning data MH you want
to have the same idea except now you're
partitioning these vectors so this these
vectors are now in some space and you
want to kind of geometrically break down
that space so that once you have broken
down the space into GE chunks of regions
of space when a query comes you can say
that actually you know what I'm only
interested in these regions I don't care
about data that's in all the other
regions go to go fetch me only the parts
of the index that belong in those
regions and then I'm going to see what
are the closest candidates that uh the
query needs so it is basically applying
the same traditional
partitioning ideas from
databases except for this new way of
partitioning which is geometry based
right interesting so traditional
databases don't understand geometry
right but Vector databases need to
understand geometry right interesting
and so from the user perspective I've
got an existing pine cone database I've
got documents in it um you know I've
gone through the partition
uh you know strategy that you talked
about earlier and I hear about this
announcement I get excited and I want to
take advantage of you know lower cost
all the the things that you mentioned
yeah does anything change for me do I
flip a switch and all of a sudden I'm on
the the new uh the serverless or do I
have to reload my data and even that you
know it's challenging at scale yeah but
maybe worse is do I have to touch my
code do I have to you know re rewrite my
application like what are the things
that have to change from my perspective
that's a great question so first of all
uh so so today we are in what's called
public preview okay so during the public
preview phase you do have to reinvest
your data if you want to use uh SS okay
uh between public preview and general
availability we're going to make it make
it so that it's a flip a bit so
basically sometime between now and when
when we go when we are generally
available you shouldn't have to do
anything to start taking advantage of
ser L but in this initial phase you will
have to reinvest your data there are no
code changes involved for you so uh it
should be seamless okay from a code
perspective to start using CS okay now
uh we also work very closely with some
of our biggest customers who have
billions of vectors and for them
reinvesting is not even an option right
so for some of these customers who are
trying out seress uh our field
engineering team and Engineers work
together to figure out an option for
them to do a seamless migration okay so
there there are things that we are doing
to help some of the biggest use cases
but for uh in in public preview we we uh
expect people to reines their data today
if they want to use cus from the
perspective of a a developer um or uh
you know someone who's architecting one
of these
systems it sounds like
this changes a lot of the fundamental
economics and maybe I'm considering
different use cases than I was
previously or I have you know fewer
decisions that I have to make upfront um
does API changes that go along with this
is there I guess I'm might trying to get
at are there new capabilities
Beyond uh you know the the things that
are transparent to me that uh as a
developer I might get excited about
absolutely I think uh so first of all uh
you're 100% right this this unlocks some
new use cases that people wouldn't be
thinking about before which is if you
have data and you want to run on demand
queries and so on you can now just start
ingesting it because uh ingestion is
cheap storage is cheap and you're only
paying for what you query so this this
really unlocks an entirely new set of
use cases uh alongside we try not to
break any apis right the whole uh we
we're trying to keep apis as compatible
as possible but in seress you do get
some extra information for example when
you're using a seress index you're going
to get information about uh in some
sense how much did this query cost so we
have a proxy for this cost that we call
radio units so every query returns to
you what the cost of that query was and
now that's very useful for you to figure
out you know in some sense to budget and
to understand how much is a use case
pattern going to cost you right and that
that's very important to know the same
surus architecture allows us to uh in
some sense put control over the amount
of your data that you need to actually
look at for the quality of search that
you need uh what I'm trying to say is
that uh it turns out most query sets and
most experiments we've run and most
benchmarks we've we've done tell us that
about 90 95% of your queries can be
answered by looking at a small amount of
data okay okay uh the trick is of of
course figuring out what what is that
small amount of data to look at right
which is also a traditional you know
database optimization type of a
challenge right exactly exactly except
it's it's a harder challenge for Vector
databases because you have to understand
geometry really geometry and it's Global
as opposed to more local exactly uh but
there are always these 5% of the queries
that need to scan more in our old pod
based architecture or in the in in any
other Vector database today every single
query would be paying for the least
common denominator meaning it would be
as expensive for you to run a query that
should have looked at a small amount of
data as it is to run a query that should
be looking at enough data to be able to
answer your question right with seress
you don't you don't have to do that so
one of the things we are we are trying
to do over the next uh you know between
now and general availability is figuring
out how do what is the best API that we
can give that puts this control in the
hands of users
okay uh I think this is going to be very
powerful because now it allows you to be
in control of just how much are you
paying for a certain uh what you
consider good quality MH MH maybe taking
a step back to to where we started and
and kind of returning to to Rag and the
challenges that that folks experience
trying to get to you know a really
quality production uh production quality
experience for you know a user facing
applications you know tie what you've
done with serverless to to that core
challenge and yeah talk a little bit
about what you see coming down the pike
uh that will make it easy because I you
know it's got to get easier it's it's
still very difficult a little there's so
much attention being paid to it it will
get easier you know how do you see it
getting getting easier and uh you know
how will the vector database contribute
to that yeah I think uh you
know a lot of work that we've done over
the last year has been to make the
vector database really simple to use
really economical at scale and something
that you can just throw your embeddings
in and search over and and we we are put
a lot of effort into that Pon serverless
is like a huge step in that direction
and I expect that we continue to double
down on that and spend a lot of time
refining that workflow now that said so
so I'm very confident that we do we'll
do that really well and we do that
really well but there is the other part
of it which is you know we talked about
creating embeddings themselves as being
complicated we talked about chunking
strategies themselves as being very much
of art than a science right now we
talked about very very little being done
in the area of reranking and information
retrieval itself to connect all of these
together really well right I expect that
we'll find ourselves spending a lot of
time making that workflow seamless and
easy for people so in some sense I
expect Vector databases to really get to
start kind of gluing together the areas
around Vector da bases that uh today are
very disparate and bespoke yeah whether
it is what embedding models you're using
what you know what how how you're really
chunking versus what your r breing how
does that can of get into the context of
an llm and so on I think this is where a
lot of engineering and research is going
to happen over the next year whether
it's spine con or anywh else awesome
awesome oron thanks so much for taking
some time out of your busy day to share
with us kind of the latest and greatest
with uh regards to Vector databases and
and rag topics that uh as I mentioned
earlier are getting a lot of
conversation a lot of AirPlay here and
elsewhere um it's a great conversation
thank you so much and thanks for having
me
