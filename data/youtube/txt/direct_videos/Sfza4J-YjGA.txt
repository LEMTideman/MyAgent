So, welcome to another episode of Legal
for Tech, the podcast. Guys, you do
recognize my voice here. It's always
Rosie. And as I've said many, many,
many, many, many times, this is just a
group of students and young
professionals who are very interested in
anything that has to do with technology
and yes, its regulation. So, today I'm
joined by my boss, non-boss, Jakamo. Hi,
Jakamo. It's great to see you.
Great to see you, too.
>> Incredibly, we're calling from the same
country, but we're calling someone
that's not in our country right now.
Hello, Robert Bman. Calling us from, I
guess, the UK.
>> That's right. Hello, Rosalia and Jacob.
Nice to be here.
>> It's a huge pleasure to finally meet
you. We've we've had some issues in in
getting this interview done, but I'm
super excited because meanwhile, a lot
of things have happened and so we can
ask even more questions. Are you not so
excited, Robert?
>> Very much so. Yeah, there's always
plenty happening and lots going on in
the UK as well. Uh so yeah, ask away.
>> Yeah, perfect. So a year after the
beginning of your new government with
Kiosama, we are here talking to senior
partner of privacy partnership, Robert
Baitman. And I would like to know,
Robert, what led you to this path? Why
are you so interested in data privacy,
data protection? Why are you here today?
And yes, what's the future got in hold
for you?
>> So why data protection? Well, I started
out being interested in this subject
while studying law uh which is probably
the the case for many people in the in
the fields. But one thing that I took a
particular interest in was uh a UK law
part of the data protection act here
about what was called the immigration
exemption. This was quite a nasty bit of
UK privacy and data protection law and I
wrote a research project on that during
my law degree. arguing that it was
invalid under human rights law. That
actually proved to be the case in the
end, but it was a different argument
from the one I used, so I can't take any
credit. Uh, so that was around 2017, and
since then, well, I started out writing
about data protection as a journalist.
Got to know lots of interesting people
within the within the sector. I've
interviewed people like Max Trems and uh
Johnny Ryan from uh the Irish Council of
Civil Liberties and managing data
protection related events for a while.
Then got into training and consultancy.
So I train on all matters of data
protection law and also uh increasingly
AI related stuff as companies are so
determined to shove this technology into
every aspect of their operations. and I
am a consultant and senior partner at
Privacy Partnership which is a kind of
boutique consultancy and we offer advice
on all sorts of data protection and AI
related stuff. So yes, I've joined the
field around the same time as lots of
other people during the GDPR era but
have really taken to it and do a lot of
kind of speaking and writing on the
topic still.
>> What can I say? amazing career. But
let's talk about a relatively recent
legislation action, the UK data bill.
What are the main reforms introduced by
the bill and why, if so, were they
necessary?
>> So, this is the UK's Data Use and Access
Act, and I keep calling it a bill, so
please excuse me if I do so. Uh, it's
now, of course, law. King Charles gave
his royal ascent to this legislation
last month. It would have been quite
surprising if he hadn't because that
hasn't occurred for several hundred
years. But it is quite a complicated
piece of legislation actually. Uh it has
I think eight parts.
Five and six are relevant to data
protection and they reform the UK GDPR
which is still for all intents and
purposes identical to how it was before
we left the EU and also the data
protection act 2018. So that's the UK's
kind of legislation supporting the GDPR
and the privacy and electronic
communications regulations 2003 which is
the UK's implementation of the e privacy
directive. So cookies uh electronic
direct marketing those sorts of things.
It's complicated in that it only amends
these existing laws for our purposes.
There's a lot of other stuff in there
about smart data schemes and digital
verification, but I'm only really
concerned with the data protection and
privacy bits for now. And uh it puts in
a lot of fiddly, very specific changes
to wording in the UK GDPR and PEKA as
it's called the E privacy law. It is
very important and very significant that
the UK has done this, but it's not all
that ambitious. The proposals, and we'll
probably talk about this uh shortly,
have been scaled down quite a lot over
the last few years from when they were
first introduced by Boris Johnson's
government. And we've ended up with
quite a modest piece of legislation that
does change things and is very exciting
for us consultants and commentators and
data protection people, but is quite
hard to get other people excited about.
So I did training on this uh law and
the training sessions can be very good.
Lots of people contributing, lots of
discussion about it, but at the end of
the session often people say, well, I'm
glad I know about this, but I don't
think it really changes very much for my
business. That said, I don't want to
downplay it. There are some really
important bits around automated
decision-m, for example. uh cookie rules
are changing in the UK and there is a
kind of new finding regime for the e-
privacy directive which is quite
important and also the regulator is
being restructured quite significantly.
So the changes are quite specific. They
are important but they are not kind of
earthshattering. They don't they're not
as ambitious as previous iterations of
this of this law have been. Um do they
do these changes target um primarily for
example big tech companies um of the
sort of the kind of obligations that are
in the DSA for example or would you say
they're more like widely spread and more
even in terms of which um kind of
controllers which kind of um sites have
to abide to them.
>> It's a good question and no it's not
really a it doesn't really target big
tech. We don't we do have other laws
that will have big implications for big
tech companies like the online safety
act uh which is not really I mean it's
vaguely similar to the DSA in some ways
it has content moderation obligations
and uh you know it it's it goes further
than the DSA in some ways actually the
the data use and access act is more of a
generally applicable law. I think it
will mostly actually impact companies
that primarily operate in the UK because
most of the new rules are
they they they offer less protection and
a lower compliance hurdle than the EU's
GDPR. So if you're operating in both
jurisdictions, then you could probably
you you don't necessarily have to change
much because you you'll be meeting both
laws. It's not like it's kind of
stricter in the UK. Now there are some
areas where
there are some small obligations that go
above the letter of the GDPR uh the EU
version that is uh for example there's
stuff about complaints handling where
organizations have to have a complaints
policy now for data protection
complaints they have to respond within
30 days and uh they might have to report
the number of complaints they get to the
ICO as well the UK's data protection
authority but most of um are are
generally applicable. They're not
targeting big tech with this law. I
think it's fair to say.
>> We did mention the great word, the word
GDPR. And you finally you gave me a
great answer for asking you the next
question, which is about the recent
report that the European Parliament
published about the UK's proposed
strategy for revisiting some innovation
trampling obligations in the said GDPR.
So I would like to know what are your
thoughts on this British attempt and
whether it is worth pursuing because we
know that the GDPR is very important in
protecting the rights of users and data
subjects online. So do do you think it's
worth it for the price of innovation?
Well, I read this report and um it seems
that they the focus is on the various
failed attempts by the UK to reform its
data protection and privacy uh
framework. So as I mentioned this first
came in under the Johnson government and
the earliest iteration of these reforms
were in a 2021 report uh called the
tiger report as an acronym I can't
remember task force for innovation
growth and that went very hard on
proposals to amend the GDPR like
scrapping the automated decision-making
rules entirely for example and it also
was pretty wrong about what the GDPR
actually is and it made some of the
common mistakes. You know, you need
consent for everything, people own their
data, those sorts of fallacies. Um, so
that was not a great start for these
reforms. And then we had a consultation
which was much more legally correct, but
also made some quite bold proposals for
reform. Then we had the first version of
the bill. Boris Johnson said he wanted
to get rid of cookie banners entirely
and um that wasn't quite uh wasn't quite
iterated in the the legislation but it
was more bold than what we ended up with
and gradually some of these more
controversial aspects ended up being
stripped out. I think that the
government is quite worried about
maintaining its EU adequacy status. So
proposals for example to scrap data
protection officers were in the first
and second version of this bill. Um they
would have been replaced by senior
responsible individuals. So a management
level uh person responsible for data
protection. We also had uh bolder ideas
about reforming records of processing
activities, data protection impact
assessments. They didn't make it into
the final bill. So I think this European
Parliament report, they're trying to
say, well, what can we learn from the
UK's failed attempts at going quite hard
on data protection reform? And the
answer is not much, I think, because the
reason we didn't go further was because
we're worried about the EU's adequacy
decision in relation to the UK. Now, the
EU doesn't have to worry about that
because it makes its own rules. you
know, we we they they they can pretty
much do as they wish. They don't have to
worry about a larger neighbor that might
restrict trade somehow. U they also have
the highest standards and it wouldn't
affect trade with the US uh its biggest
trading partner, I think, because their
standards are much lower. So, we don't
really know what impact these reforms
will have yet.
the EU would have to worry a bit more
about human rights law, I think, than
the UK probably does. But it's not quite
a fair comparison because the EU doesn't
have to worry about its own adequacy
decision, which is I think why the UK
decided to strip these reforms back a
bit.
Talking about my lovely GDPR and how to
make things easier or worse, may we ask
your opinion on the topic of proposed
simplification to article 30 of the GDPR
as put forward by the commission.
>> So the commission has proposed a few
things in relation to simplifying the
GDPR
and I think lots of people were quite
disappointed by what they came up with.
Uh basically just to summarize, they
want to allow organizations with up to
750 employees a bit more flexibility
with regard to recordeping and they want
to they want regulators and other bodies
to consider the needs of medium-sized
companies when setting certifications
and codes of conduct. Now this I don't
think will make much difference to
anybody to be honest. There's already a
an exception for a limited exception for
companies with less than 250 employees
when it comes to recordeping.
I've worked with smaller companies and
it doesn't really I don't think I've
ever pushed anyone to take advantage of
that exception because you have to keep
these records in other places in a way
and your privacy notice has to have this
information. It's a good idea generally
to keep these records. I think if a
company really doesn't benefit from
doing it, then I would actually approve
of more flexibility legally saying that
they don't have to do this because it's
kind of an internal recordkeeping
process. I have met people who say this
is a complete waste of our time. You
know, we don't we don't see any benefit
from keeping a roper as it's called
records of processing activities. So, I
don't think that those reforms will make
much difference. Maybe the commission is
being a bit conservative because it's
worried about what the courts would say,
the court of justice about reforms that
remove people's human rights
essentially. But, you know, I I don't
think many companies will see much
benefit from the the proposed reforms.
Would you say this is more like of a
statement of a commission wanting to
signal companies like Europe wants
innovation? We don't want to trample
your access to Europe. We don't want to
block you and that's our step forward.
It's just a statement. It's it's a way
of communicating that they want them to
come.
>> Yes, I think that is what it
constitutes. Um, you know, they're doing
something. It's not nothing.
I don't think I think anyone who
understands what's being proposed and
reads it and has worked in the sector
too um will will realize that this is
not not not going very far at all. But
uh yes I think that's right. I think it
is a more of a statement from the
commission. Whether it's a good
statement or an effective one is is
another question.
There is a question that is particularly
relevant also for me. The topic it's the
way the GDPR operates is related to
training of AI models using personal
data. Where do you stand when it comes
to this basisly complex question?
So it is a very difficult question and
for a long time I was saying well the
GDPR is actually quite well set up to
deal with uh AI. The automated
decision-making rules have been there in
some form since the mid90s under the
previous legislation and the principles
of data processing apply to AI systems
as much as they do to any other systems
that process personal data. Um, the
problem is that I do think there are
real tensions between the
between AI and generative AI and large
language models in particular that with
with with the GDPR and particularly with
regards to rights and and freedoms. Uh,
so the data subject rights, how to
exercise your right to erasure against
AI models that have been trained on your
personal data or to correct it. Uh there
aren't easy answers here and I think
we've seen some pretty flexible
interpretations of the law uh even
coming from the European Data Protection
Board in some cases which has surprised
me. The main problem though is that it's
quite ambiguous and there's no clear
answers for businesses that want to do
this stuff. I think this is where
certifications and codes of conduct
could come in to provide a bit more
clarity on what people are allowed to
do, whether or not consent is required
to process someone's personal data for
machine learning purposes.
um whether for example
uh removing someone's data from your
training data is sufficient to fulfill
their right to erasure even if you've
already trained the model on it. Um a
strict and almost straightforward
reading of the GPR would suggest that
those things are are not satisfactory in
this context.
But we are seeing different
interpretations from different data
protection authorities which I think is
not helpful. We don't have a lot of
court of justice rulings that deal
directly with this stuff. We've got some
stuff from the European Data Protection
Board, but I must say I don't find it
particularly enlightening. Um, I think
it leaves a lot of unanswered questions,
which is understandable because the
answers are not obvious for a lot of
these questions. Ethically speaking, I
mean, I personally am not that bothered
about my personal data being used in
this way, and I'm someone that does care
about this stuff. But other people are
different. I think the noise case
against meta made some very good points
about human rights in the AI context. So
they said that uh for example people
should be allowed to object to the use
of their data in this way but that
objection should be valid
retrospectively. So you should be able
to object to your data having been used
in this way and being processed by the
model on an ongoing basis which is
basically impossible without doing
another huge training run. Uh, I think
that until we get some clarity on how
the GDPR applies, probably from the
Court of Justice to be honest, which is
not best known for its very clear
judgments, uh, in any case, then
we're going to see companies continue to
just go ahead and do stuff um, whether
or not it's it's technically legal,
which is not a great situation to be in.
Um, at this point there's a question
that's kind of burning in all of our
minds. It's about consent which said
like that is already quite interesting.
So we know that some companies like Meta
for example have given in some countries
especially their users the possibility
of opting out from giving consent to the
use of their data for the purpose of AI
training. So, uh, for example, in
Germany or even in Italy, you had until
the end of, uh, May 2025 to opt out and,
uh, state that you didn't want your data
to be used as part of the training for,
uh, lama, so um, measures AI.
>> Um, do you think that that's a feasible
option? Do you think that people users
are well informed enough to give that
level of consent? Do you think the
procedure means that people are actually
expressing their free consent or it's
just like so difficult to follow that
then people are just like not going to
be bothered? And also um since you are
an expert also in when it comes to
cookie policy, we know that cookies
really do work on giving consent or not.
And that's um quite of a I would say um
a gray zone whether that's free consent
or it's just please get off my phone and
let me go on this website. So do you
think that we should learn something
from how cookie policies have been
updated and upgraded throughout time in
terms of how consent is given when it
comes also to its to being given in
relation to um training with personal
data of um AI and and other um similar
technologies.
>> Yeah, great great question. So I'll I'll
break them down a bit.
Firstly, when it comes to what Meta is
doing,
I sometimes
I don't want to say sympathize, but
sometimes I am a little bit less harsh
towards Meta than some of my colleagues.
In this case, I'm not crazy about how
they've gone about obtaining training
data. Um, they aren't even purporting to
rely on consent in this case. they are
offering an opt out and relying on
legitimate interests.
I don't think that's necessarily
illegal. I think in the case of meta
more time and a better explanation would
have been preferable. Ultimately I think
consent would have been better for this
particular processing but then they
would have got less data. So that's why
they don't want to do it that way. I
assume in this case we we we see
people could be, you know, they could be
using photos from a long time ago,
decades ago that people uploaded to
Facebook. And we know that
AI systems, whether or not they contain
personal data, AI models is a debate
that I won't take a position on, but
they do produce it and they can just
reproduce or parrot their training data.
So people might not expect their stuff
to turn up in this context and I think
that would be quite disturbing for some
people if it did. So I'm not keen really
on how Meta has handled this
specifically. As for whether you can
consent to this type of processing given
how complicated it is, um I think yes,
in theory you can. I'm a little bit more
liberal than some privacy
advocates in this regard. I think I I
think people should be allowed to choose
this sort of thing.
I I don't know whether it's possible to
become fully informed about it to the to
the extent you know I think it's
possible to be informed to meet the
GDPR's threshold uh of being informed or
given informed consent
but that's not to say that
people will be given a free choice in
GDPR terms like you say as a comparison
to cookie banners people usually just
agree to those to get rid of them. I
think it would be a problem if that same
model was emerging with regards to
training person with personal data via
AI models. I wouldn't want people to
accept it without at least having a
decent understanding of the
implications.
Now what those implications are
depends on the context. So when it comes
to llama perhaps reproducing parts of a
post I made in 2010 and I'm pretty sure
I made some embarrassing posts over the
years. So I'm not crazy about that idea
that consenting to that I think carries
less risk than consenting to certain
other AI related activities. So I am a
bit worried about scope creep here where
you know if we think down the line what
are people going to be using AI systems
for crude consent to have our data
ingested by those have much more serious
implications.
Um
legitimate interest is emerging as a
basis for some training of AI models.
There are some authorities across Europe
who have basically said yes it's okay
within certain parameters. I think that
is probably the right approach for some
types of AI some models and not others.
So the world of data protection and AI
regulation is not getting any simpler.
These new questions are going to cost a
lot of legal hours and people thinking
this stuff through, I think, and a lot
of pontificating about the various
provisions of of various laws. The AI
regulation, of course, is is impressive
in some respects, but very very
problematic in in others and very
complicated in in some areas.
I I appreciate it's a bit of a nonwans
answer, but I don't think it's always
going to be reasonable to require
consent to everything. And there are
contexts in which I think we could we
could use AI systems without consent
within proper proper boundaries, you
know, in in suitable contexts where we
are asking for consent. I really think
people need a bit of handholding to help
understand what the implications of that
are. Um, and of course there are some
cases where transparency is not enough.
You know, there are some practices that
should be prohibited outright. The AI
act has done a bit of this. I think some
things are implicitly
uh prohibited by the GDPR, but
transparency and and consent is not an
excuse for doing things that are legally
or ethically wrong. Uh I think
>> um I I see your point and yes sometimes
it can be a really non-answer answer but
like really um with the level of
knowledge we have now I think um it's
really difficult to give a defined kind
of I believe this type of answer when
it's not really clear what's happening
really to this data. I mean, a couple of
weeks ago, an article slightly
sensational, published by British
newspaper, The Guardian, was implying
that apparently one contact that had
been saved into someone else's phone had
been kind of conjured up by MetaI on
WhatsApp and then the contact was
actually on the website. So, um, it
wasn't really coming from anyone's
phone. So, it's that kind of like gray
area where you you really don't know
whether your data is coming from the
phone, from your conversations,
from um your social media posts or it's
simply coming from the internet. You
completely forgot that it was even
there. So I I guess that that's um
that's the bigger issue is understanding
what's happening with the data more than
um defining which areas and and which
types of data you can give consent to
because as you said many of these are
already uh regulated under the GDPR.
Um
>> yeah and you know these adversarial
attacks against AI models or using AI
systems
are very real and very worrying. We've
seen it happen and I don't know about
that Guardian case but New York Times
had an interesting piece about Chat GPT
uh being well it's hard not to
anthropomorphize
being tricked into handing out people's
email addresses you know reproducing
part of it training data. I think in
theory the law prohibits
some well it's hard to say isn't it
because the the the scope of the
training process and the scraping means
that it will inevitably
uh end up with personal data in the in
the AI training set. I have seen
arguments that this should not
constitute personal data processing at
all. I don't buy that. I think that we
need to put guard rails on on that
aspect and trying to improve people's
approaches to training and and and
scraping uh and and gathering data for
training purposes is probably better
than prohibiting it outright.
There might be some role for an
organization technologies at that stage.
I'm not sure of the state-of-the-art at
present. I know that the term
anonymization has been abused a lot
throughout the years. So, I wouldn't
want people to, you know, point to that
as a kind of excuse. It happens in
you know many quite worrying context
healthcare and so on reusing uh personal
data. This is something actually under
the UK's law that I think they are
trying to make it bit easier to reuse
personal data in the health sphere for
research purposes for different
different in different contexts probably
it doesn't mention AI training but they
probably had that in mind now having
said that I'm not a kind of purist on
this stuff I think particularly in the
healthcare sphere maybe not so much with
large language models but there are some
fantastic AI use cases. You know, the
kind of old school AI in particular has
been has been making, you know,
improvements in in some specific
healthcare aspects. And I do think large
language models are pretty useful. Uh
they do have their uses. I'm very
skeptical about how useful they are
generally, but they I've found them to
be useful in very specific contexts. I
don't use them to write per se, but you
know they they they do have their uses.
I wouldn't want to completely prohibit
them. So I think we have to think about
whether the regulations we place on AI
developers would have the effect of
prohibiting these systems altogether. If
we want to do that, I think it would be
better to do it outright. uh you know
rather than just cutting off the various
ways that these these systems are built
and trained uh we should probably have a
a more democratic conversation about
whether we want this technology in
society or not because shutting off all
avenues to developing it would have that
effect almost inadvertently
>> to have that democratic conversation. We
think that it's particularly important
that our listeners and anyone that's
actually listening to this podcast right
now um will be given all the possible
instruments to understand what type of
conversation this should be. It must be
an informed conversation and in that
light we're asking you our last um
question but also a suggestion as a
friend and as someone who understands so
much better than the average person what
we're talking about in this case. Do you
have any suggestion of books, podcasts,
TV series, anything that comes up to
your mind that may help listeners
and making a more democratic decision in
the sense of democracy?
>> Well, look, I read a lot of stuff. I
I want to recommend people really on on
social networks to follow rather than
publications. Apart from of course my
own newsletters and uh privacy
partnerships uh newsletters which I do
recommend. I put out videos and stuff as
well. But besides me uh there are other
people that I found really valuable to
follow. There are too many to mention
really, but on the AI act front, there's
a a lawyer called Alexander Tulkenov who
is very very good on understanding the
AI acts. Uh Tia mustache as well uh who
works alongside Peter Hens uh I think
who has been on this show. They are both
very good to follow when it comes to the
AI act specifically and AI regulation.
Peter is particularly cynical about this
stuff and is a litigator. So if any of
my statements have been a bit too
wishy-washy and ambiguous for your
liking, then I recommend following him
because he does not lose words. Um Ria
Alexander Walla is is really good as a
way to keep up to date with developments
at EU level particularly in data
protection and um she has a subscription
as well that I can recommend in the UK
here uh particularly spring to mind John
Baines a lawyer who really gets into a
lot of detail about UK litigation and
legislation and uh regulation and also
Whitaker who's very good with uh kind of
broader data protection issues uh in the
UK. So I mean there's thousands of
people that I have mentioned there or
Kerry Leaning as well uh is very good on
the kind of ethical and
kind of broader stepping back and
looking at how AI has been developing
and the regulatory side too. So there's
a hundred people that I've missed off
that list. So, I apologize if you think
you should have been on it, and you're
probably right, but those are the first
people that's front to mind when when
you asked.
>> Robert, what can I say? You're great at
giving me uh potential kind of asses to
make a great plug because we have talked
indeed to Alexander Tulkenov. We have
talked to Peter Hanza, Tia Mustache, and
of course, Carrie Laning, who is a great
friend of ours. So, guys, this is a
great shout out. go listen to all of
these podcasts and we also have talked
to Ria um Alexander Valet. So everyone
please do listen to these podcasts and
uh do follow the page of course legal
for tech you forgot to mention it but I
know you do for follow us so
that was top of my list actually but I I
I neglected to to read it out so
apologies.
>> Absolutely. No um we do thank you for
your support. we do have um greatly
enjoyed this interview and and thank you
for the great tips. Um so I shall say
thank you Robert Bman for this
incredible interview. It was a great
pleasure to see you.
>> Thanks so much. Pleasure to be here.
>> Um and thank you Jakma for joining me as
well.
>> Thank you Rosie. Thank you Robert.
This was all for Nealagle for Tech the
podcast.
