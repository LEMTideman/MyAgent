Welcome to another episode of Legal for
Tech, the podcast. So guys, you do
recognize my voice here. It's always
Rosie. And as I've said many many many
times, this is just a group of students
and um many more young professionals
right now who are very interested in
technology. And yes, it's regulation. So
guys, incredible. But today I'm actually
having the stage here. Not joking, but
like there are no boys. It's all girls
because I'm finally recording with a
woman, a woman that I really, really
hold in high esteem and she met me today
uh on the podcast on a very special
episode because we are here to discuss
the digital omnibus. So, welcome Andrea
Liz Nevin and I made that name
correctly. Yes,
>> thank you very much for having me. It's
a pleasure uh to to be here and um I'm
very excited to discuss this uh piece of
uh well not legislation but proposal
that we have had for roughly 24 hours.
So this is very early thoughts uh on on
both sides but uh still I'm uh I'm very
excited to um uh discuss it and u see
where we're going to go from here.
Basically,
>> it's incredible that you managed to read
it through in 24 hours. Um, like Andrea
actually made the effort that many of us
potentially have not made to read it
through and actually form an opinion
that it's not just, oh my god,
everything has changed. Um, so I'm I'm
super impressed. I'm really like,
>> okay, let's not oversell it because I
need to I need to mention that it wasn't
maybe the most difficult task only for
the fact that I had read the leaked
version. So last night I looked
comparatively okay what actually changed
from uh the leaked version. So that made
it a little bit easier. But I would not
say that I went through everything or
had time to really digest what is in
this document. But I'm still thinking of
about some of the things I have to
admit.
>> Okay, maybe we'll help you with the
digestion will be your um after after
dinner um spirit uh the graa that
Italians love to drink. Um um I wanted
to um first introduce you to our
listeners because I'm sure that many
know you uh considering also the likes
that we received when we actually um
advertised podcast. But for in case you
haven't heard of Andrea, very very
unlikely. Who is Andrea?
>> All right. Well, Andrea is a privacy
lawyer uh that also deals with the other
uh digital laws uh nowadays just because
this field has uh and is evolving every
single day. And um I nowadays uh run my
uh hub called privacy craft where I
teach and I coach u privacy
professionals that either want to
advance their career or want to enter
into uh data protection and the other
satellites um
which are continuously evolving of
course nowadays and um I work as a data
protection officer and I also lecture at
the masters university in the
Netherlands and yes it's challenging
doing everything together but um you
know I try not to sleep too much so um
without saying um very much into the
past uh I have been around into in this
field uh for longer than um the GDPR was
around uh before it was very very very
trendy I've seen the
uh GDPR bubble as I can call it when
GDPR uh was about to enter into force
and all of the experts were appearing
almost um overnight and um even before
that uh it was the cloud computing
bubble and um in between a bit of
blockchain bubble. So yeah um quite
quite a bit I'm not as impressed about
the AI bubble as I could have been just
because of uh this past experience.
Let's let's put it that way.
>> I mean, when people ask me why I'm so
impressed by you and I really think that
you're my role model, it's like this
woman does it all. Like literally does
it all. So like come on, you're not
impressed by the AI act. Of course
you're not like you you basically are
like digital law in person. So like why
am I no really like I'm really happy to
have you here and um you have had an
impressive career and so impressive that
however busy you are because I'm sure
that you are busy you still managed to
send me something to read
for today to make sure that this podcast
was as enjoyable as possible to our
listeners. So, let me channel my inner
Shakespeare here and let me read this
piece that you sent me um the first time
that someone preps the the podcast for
me. It's incredible, amazing. Um so the
EU intends to maximize growth and
competitiveness by exploiting big data
but the digital single market cannot
uncritically import the datadriven
technologies and business models which
have become economic mainstream in other
areas of the world. Instead it needs to
show leadership in developing
accountable personal data processing.
The internet has evolved in a way that
surveillance tracking people's behavior
is considered as the indispensable
revenue model for some of the most
successful companies.
This development calls for critical
assessment and search for other options.
So aside from Shakespeare here, what's
this piece about? Where is it taken
from? And why is it relevant today in
2025?
This uh piece is from 19th of November
2015. So it is exactly 10 years old uh
yesterday. And it was said well written
uh by Giovanni Butelli when he was the
European data protection supervisor
and it's from an opinion that is called
meeting the challenges of big data. So
big data was the thing uh 10 years ago.
But the thing is if we switch big data
with literally any other buzzword of the
day, it still tracks. It's just as
applicable. And I wanted us to start off
with this quote because he in this
paragraph and if you read the whole
opinion of course was flagging and
insisting on the fact that we shouldn't
lose our values on the fact that just
because something is trendy just because
something is happening just because
something is adopted somewhere in the
world doesn't mean that we need to take
it as it is. doesn't mean that it's a a
a goal in itself. It might be something
to look toward. However, the means does
do not justify that. So that's why I
wanted us to to start off with it
because it is in a way a counterbalance
to the way I see is the commission's
position uh these days yesterday in
particular which to me um I'm going to
start off with basically giving away the
the main idea here which is that to me
uh the omnibus proposal um that we've
seen in its final form yesterday was a
little bit of a capitulation, a
capitulation to US
uh style lobby um from not just big tech
but also politics um in in the United
States at the moment which all push for
deregulation. let's let the market do
and the business players do whatever
they want because uh they know best and
they will self-regulate which I think
history already has shown us time and
again that that's not the case. So uh to
counterbalance that I figured that going
back to the basics and to the
fundamental rights value that guide us
in Europe is really important. We above
GDPR the the AI act and others we have
the fundamental charter the charter of
fundamental rights of European Union
that's always going to be there it's not
going away and it's not going to be
touched by anyone in any foreseeable
future and I think um also looking at
one of the first recitals in the GDPR
which wasn't in existence of course in
2015 which Giovanni Bhutaru was actually
pushing toward this new and revised and
uh increased scrutiny in what later
became uh the GDPR. Um, one of the
recital in the beginning says that data
processing should be should serve
mankind.
And I really want us to not forget that
because with lobby, with political
pressure, with tariff wars and so on, we
might be looking into a different
direction nowadays. And I don't think
that that's a good idea. Of course, we
need to look at all of the dangers,
looking all of the pressures and so on
and taking informed decisions, but those
are not the only ones. We still have
values and we still know where um we
want European society to go toward or we
should know, maybe I should say we
should know.
>> I I find your perspective of course
fascinating. Um the the question that
kind of comes up is um looking back at
how the digital omnibus was kind of
brought into life and why the commission
decides to step back from what had made
potentially the European Union so unique
in the world. this kind of idea that we
had had this Brussels effect and the
GDPR was expanding all the way to
California where from these kind of
businesses are you know it's like um
kind of counterintuitive what's
happening right now um wouldn't you say
that easing down simplifying some of the
norms that are in GDPR is for the
benefit of mankind because we will all
benefit from something like the
simplifications in relation to
generative AI for example of course this
provocative question is meant to be
>> yeah exactly but I mean simplification
as a word of course denotes in itself
something positive and I don't think so
anyone can really be against
simplification but it the devil is
always in the details it really depends
on what that actually means in practice
um as a wish simplifying things is
always amazing But when we actually try
to do the simplification and this I'm
talking also from in my work as a
professional many times some things just
can't be simplified over a certain level
because you lose the outcome you don't
get the same outcome anymore. So you
cannot do a high level of data
protection with simplifying simplifying
by eliminating protections. That's not
simplification anymore. Simplification
is we keep um a lower level of records.
We you know we eliminate some of the
bureaucracy around things but not the
goal and the the protection that is the
purpose of the whole uh regulation. And
I think that don't get me wrong many of
the things that the commission has put
forward are good uh not even at the
level of intention but even at the level
of outcome. For example, having a
unified list of situations when data
protection impact assessments are
necessary and when they're not necessary
and uh a unified process and template on
how to do it. Great. However, there are
other things that really decrease in my
view the level of protection afforded to
people. Really cut down into how well
are people protected. For example,
uh when you are informed about the data
that is being processed about you,
they've cut into article 13 and there
are carveouts proposed um that make it
so that you you are not informed in
situations that are also very very very
undefined. What is a circum
circumscribed relationship with the
company? I I I could not explain really
I I really could not explain and I don't
know why they chose this wording or the
business is not data inensive. Okay, I
can guess what is behind the intention
there but also I can be the devil's
advocate and and make a lot of
businesses in Europe not be data
intensive. So this if it goes through
will end up being tested with uh
enforcement by data protection
authorities and eventually by courts and
eventually by the European Court of
Justice. Is that simplification? I don't
think so. Simplification needs to be um
to get needs to come together with
clarity. If we don't have clear or
clearer provisions, I don't see that as
being useful in any way. And I've seen
that actually happening in a lot of the
proposals that the commission uh made
yesterday. Just more uncertainty rather
than more clarity.
>> Um from like an analytical perspective,
there is this kind of question that
comes into looking at the grounds, the
reasons that brought the commission to
specific uncertainties. Um, I would say
I would think that, you know, the the
kind of article nine carveouts and and
we will definitely talk about those ones
because I feel like they're on
everyone's LinkedIn feed. Um, I feel
like, you know, the reason for it is
kind of out there. Okay. AI, it's even
written. So, like you could you can't
escape it. Um the ones with regard to
article 13 I think are particularly
interesting because in some cases I feel
maybe it's maybe it's my perspective but
I really fail to understand what's the
point like I see the point with like the
DPIA because I know many people that
complain that they don't know when the
DPIA is necessary and they are
completely like left in the in the dark.
But with article 13 it's like what did
the commission have in mind?
you and me both because like I said uh
I'm looking at the text now. It says
that the paragraphs with the obligation
to inform shall not apply where the
personal data have been collected in the
context of a clear and circumscribed
relationship between data subjects and a
controller exercising an activity that
is not data intensive. And then it
continues there are reasonable grounds
to assume that the data subject already
has the information. So, it's both very
weird wording that I've never seen in
any other uh piece of European
legislation and also this uh relativity
with reasonable grounds. Well,
what are those to assume that the data
subject already has? It's going to
become a very personal exercise for each
and every controller uh to basically say
I have reasonable grounds to assume that
they already know.
It's a bit much.
I really wouldn't leave something so
fundamental as informing about data
processing to something so relative and
so easy to move around. And let's not
forget that we still have 27 member
states in uh Europe and let's not count
that Germany has basically a data
protection authority in every uh land.
Uh that's to say that we have many micro
cultures. These things will simply not
be interpreted in the same way and this
will not make it easier for controllers
that have activities in more than one
country. So again, what simplification
is that? And then also even if we look
at just within one country, the fact
that this could end up being interpreted
differently by the data protection
authorities and by courts is to nobody's
advantage and definitely not to GDPR's
advantage. And even the fact that we
have the European Court of Justice on
top of everything that will clarify
these things still doesn't really uh
cause or or support the idea of
simplification. You know, having to deal
with many many many layers until you get
to the European Court of Justice and
years of fighting it out basically is
not my idea of simplification.
It's also hilarious because I feel like,
you know, when we think of heavy
lobbying,
um, potentially these companies such as,
you know, just to to name a few, Google,
Amazon, the the very the very big group
of of tech conglomerates
potentially, you're not thinking of them
as being companies that have little to
do with data and are not data intensive.
So
>> sure uh but others do and um I'm
thinking here especially about companies
that have a very large number of
employees
and they don't have any other processing
of personal data uh in their product. So
we're it's not a B2C type of
relationship. It's just the company with
its own employees but they are tens of
thousands or more. uh let's think of
state companies and and so on that's
probably going to fit the bill at least
I would assume so that it's not data
intensive you know I have to process the
data of my employees and also it it
could I'm assuming be a circumscribed
relationship
so um that really poses a lot of
questions for me is that really
necessary does that simplify anything
what good does it do for the people for
the data subjects? I don't really I'm
not a fan of this term. Uh I call them
individuals all the time but anyway um
what good does it do for them because
they end up not being informed that
their data is processed and then due to
that they can't or won't know that they
can uh exercise their rights. um every
every other piece of protection from the
GDPR goes away because if you don't know
that someone is processing your this
piece of data for this purpose, you're
not going to even have questions about
it. So, you know,
it's it's a bigger
hit in a way than just changing the
provisions on uh information. To me,
these two pieces, the definition of
personal data and special categories of
personal data and the information are
really fundamental because they end up
underlying or circumscribing the scope
of application and the scope of
protection of the GDPR. And that also
prompts me to say that one good thing
and one good difference between the
leaked version and the final one
published yesterday is that they ended
up not proposing changes to the
definition of special categories of
personal data. That was incredibly
troublesome. was it was one of the
things that was really on my mind for
the past week and I was glad to see that
it did not survive even in the proposal
uh of the commission because for those
that didn't read the leak uh it was
saying that special categories are uh
limited now uh to direct uh identifiers
basically if if it was indirectly
um allowing basically inference and so
on of of the special categories of
personal data. It was all of a sudden
outside of scope. So that to me was a
very very very big problem and I'm glad
to see that it didn't survive.
Um, let's take a deep dive into this
whole theory of article 9 and and and
the definition of personal data because
I feel like potentially is what
identifies also the individual as
someone who has rights. If you don't
know whether that's your personal data
or it isn't, you kind of like lose um
touch. And I and I want to use something
very kind of physical tactical um in
terms of personal data because I feel
like we as normal general people data
subjects don't realize um how much those
data tells about us and
>> exactly
>> differently from okay that's my bag if
you open it you see my passport inside
okay that's something I can identify as
mine as belonging to
with personal data. We leave literally
traces behind our back every single day
and and we don't realize it because
they're not visible. They're not there.
Unless you go um kind of think about it
and you go like, "Oh, oh my gosh, last
week I searched something on Meta AI and
now it's woo on my Instagram reel." Um
we will not open this this um window
further. uh but let's let's take a deep
dive into it. What changed and what
should have not changed and what
potentially doesn't matter if it has
changed or not.
>> Okay, let's start with the basic thing
there which is the definition uh and
scope of personal data itself.
The proposal from the commission does
not change that definition. However, it
adds a qualifier to it which um aims to
codify I'm assuming the quite recent
decision of the court of justice of the
European Union in the SRB case from I
think it was the 4th of September and it
basically takes the main idea from there
which is that and I quote information
relating to a natural person is not
necessarily personal data for every
other person or entity.
merely because another entity can
identify that natural person and it
continues a little bit. Um and that is
not a bad thing in itself. I don't have
a problem with that. Uh because again
it's already in case law. So as
practitioners we need to know that and
we need to apply it because again cl
important clarification um the uh
decisions that the European Court of
Justice issues uh following prelim
references for preliminary rulings have
the same power as the law itself. So
they interpret the GDPR in a manner that
is mandatory to be applied by everyone
not just the parties to the national
case throughout the European Union
actually throughout the European
economic area because that's how uh the
GDPR applies. So um it's it's in a way
unnecessary if you ask me but also it's
insufficient because there is a lot of
detail in the SRB case that of course is
lost in putting just one paragraph into
uh an amendment to the law and one thing
that really makes or breaks let's say
the reasoning in the SRB case is the
fact that that situation concerned the
relationship ship between two individual
controllers. No party, neither the
disclosing one, neither the recipient
were a processor for the other one,
which is really important because when
you have a processor, they process the
personal data on your behalf. And that
means that the whole context of
obligations, qualifications, everything
under the GDPR needs to be um judged by
reference only to the controller. It
doesn't matter literally whether a
processor has the means to identify
individuals or not. And by codifying
only the outcome without any sort of
specification of in which situation it
applies. I do see the danger that for
example cloud providers will say well we
can't identify the people because we
process only this and that data as part
of the service therefore this is not
personal data for us so we're not going
to conclude the data processing
agreement and off you go and that will
not hold that will be terrible first and
foremost and I mean it can hold if the
cloud cloud customer encrypts the data
with their own key that is never shared
with the the cloud provider but still
it's really against the whole system of
protection that the GDPR and the charter
of fundamental rights aim to give to uh
individuals and again that's the level
of detail that's lost in this proposal
and another thing that is lost is that
um well I think I forgot my train of
thought there but I will get back to it
in our conversation. It's totally fine.
>> Um, okay. So the SRB decision was a big
problem in general was like viewed as a
good thing as a bad thing potentially
the kind of like EDPB guidelines and
then came out afterwards talking a bit
about like you know generative AI being
processors what happens in that case
kind of recalling the SRB already made a
path towards this sentence which to me
looks more like I said it by the way I
said it it's there rather rather than
being um kind of normative because as
you said it doesn't specify much. Um
a big point that was made um potentially
also with the with the leaked version is
that these changes were going to
I wouldn't say simplify nor facilitate
but I I would say like enhance the kind
of like data um collection operations
that specific um datadriven
um businesses um can carry out. Um do
you think that that's true? Do you think
that like the way that data was defined
um will help
putting together more data, aggregating
more data or do you think that's
ultimately not going to make any change?
>> I am not sure uh whether it will lead to
more data being processed. To be honest,
I think that whatever we do, more data
will be processed. Um, it's just how
things tend to work. Um, what I would
question though is whether
that's a good or a bad thing and whether
we want more data as a principle to be
processed. Like what is the value in
that? Can we call it a day if there's
more data being collected? Can we call
it innovation? Can we call it an advance
for our society? I really don't
understand that kind of reasoning
because um this is also a bit of a
cultural gap between Europe still and um
the United States where in Europe we
don't value simply money. It's not like
if something makes money it's by design
enough and then we just go with the
flow. It needs to do more than that. It
needs to be a net benefit to society. If
we create more value by uh child labor,
we're not going to allow that.
So it's not just about money. It's not
just about So first of all, in the first
stage, it's not just about data. Data
doesn't translate into money directly.
Um which I think is best demonstrated by
AI because AI companies don't yet have
profits, right? But they have a lot of
data. So not really sufficient there.
Then even if there is money at stake, is
that sufficient?
Is that sufficient for us as a society?
And are we okay with what it means?
>> Oh, philosophical questions of 4:37 p.m.
on a Thursday afternoon, Andrea. Like
seriously. Um talking about philosophy
though there is a very very important
philosophy that came out in this uh
digital omniverse that was questioned
requested three times questioned by many
many people which is this idea of
legitimate interest and of course
>> it's it's a whole new conversation to be
started here but in your opinion does
the digital omnibus solve the issue view
of um legitimate interest or does it
open another window into this very very
long conversation that I don't think
will end today.
>> I don't think it solves the issue also.
I think it depends what we consider the
issue to be
because um
I think um at least some companies have
looked toward legitimate interest as
being the catchall. we have nothing else
to stand our ourselves on and uh and so
we will use legitimate interest because
um I I struggle to understand really why
it has been so difficult and it still is
a discussion what legitimate interest is
and that it has three conditions and it
doesn't suffice that you just have an
economic interest. Sure, that's a must.
That's one condition, but you also have
to uh not have that interest be
superseded by the rights and interests
of the people whose data you uh you
process. So, it's really about
proportionality
in and of itself, which is by the way
another fundamental principle of the
entirety of EU law, not just data
protection. proportionality between what
you want to do and how you want to uh
achieve it and what's the effect is
essential. So does the omnibus even
touch on this? No. So nothing in what
legitimate interest is and its validity
and how you need to uh be accountable
for using it is changed. Literally
nothing. However, what the omnibus
proposal does is to create
um a gateway for AI. I think it says
development and operation
to use legitimate interest even for
special categories of personal data.
That is very problematic for me. not
because of legitimate interest but
rather because the combination between
legitimate interest alone and special
categories is never seen has never been
seen uh so far. Um and also it's
although by the way again an improvement
in the final uh version is that it
doesn't create a presumption that that
is actually an actual legitimate
interest that passes the test. That was
my reading of the leaked version. Um,
now in the final version, it does say
that you need to meet the conditions and
do this and that and uh it literally
refers to not being superseded by the
rights and interests of of individuals,
which is good. It's a it's at least a
step forward, but um I do struggle with
the idea of having special categories of
personal data processed based on
legitimate interest. And I think my
biggest problem with it comes also from
the lack of specificity in
um which data timeline let's say we're
talking about. Um let's take a concrete
example because these have been in the
news in in past months. Uh Meta started
to use uh posts of people past and
future uh to train their AI. LinkedIn uh
quite recently I think uh did the same
and um
the thing is there's a big difference if
you want from my point of view of course
uh if you want to use past data that has
been generated put out there on the
internet by people when these concerns
didn't exist
versus new data like you know now you
are informed this is happening And it's
the data that you generate from now on
that is affected by these things. In my
view, a legitimate interest uh balancing
test is hard to pass for the past
generated data. Again, you could say and
it is a good point to make that this all
lies within the balancing test. That's
totally true. And if everyone in the
industry and in these uh companies was
um uh a good and well-meaning
professional and advised by uh you know
super super super highly regarded
professionals, I would trust this uh and
and go with it. But the thing is data
protection
is less about the the quality of the
advice and more about the result that is
achieved by the entity that builds those
products and services. And they don't
always go in the same direction because
the results achieved by those business
entities are directed by their business
goals and they are only informed by
advice from privacy professionals and
other types of uh consultants. So they
might not fully reflect that advice how
it was given and then of course we can
also discuss about the quality but we
don't know what's what's at stake there.
And so I would not trust that uh
companies will do the right thing
especially some companies that have
already been heavily fined and not just
in Europe for uh privacy practices. Do
we really trust that they will just do
this that they will just do this
balancing test in the right way and take
the right decisions? Why? I I struggle
with that. So they didn't do the right
thing several times in some cases, but
all of a sudden now they will when they
have a very very very clear stated
interest to use personal data to develop
AI and also they are given a very clear
pathway into this new proposal.
I I don't see it.
Um, I'm going to play a bit of like
devil's advocate. Um, just because
basically um otherwise there's nobody to
balance out the kind of opinion and and
that would be unfair for this podcast
because yeah, we're trying to like keep
it uh as fair as possible. Um
maybe the commission should um take some
notes. Um that said um the question is
um I've heard many times actually um
people kind of hinting that the GDPR is
more for privacy professionals than for
the individual data subject. Um the
average person that just visits the
website randomly or wants to ask Chachi
PT what kind of dress should I ask
should I buy because I really like red
and please remember that color.
um
this simplification
could it be seen as something that is
better for the people in the sense that
what you said about legitimate interest
and um using past data I mean if I put
it out on Instagram say just just to
name another big tech um maybe I didn't
care for that data to be used by another
person or the AI likewise maybe I'm I'm
not too concerned concerned about that
data. If it were something very private,
for example, my face when I've just
woken up, I wouldn't be putting it out
on Instagram. Like,
>> I totally understand this point. And
honestly, I have so much to say here.
Hopefully, I will actually remember
everything that just was flowing in my
mind as you were speaking. Uh because
the thing is uh when someone especially
some years ago was uh putting stuff uh
on Instagram, Facebook, social media,
especially if you have a private account
that is not open uh for just the public
to see. I don't think you had the
understanding
that this would at some point in the
future end up being used by the company
whose platform you're using for an
interest that is solely theirs and does
not come back as a value added for you.
And that was uh extremely visible. Uh I
I remember the days when Facebook first
appeared.
I actually used it from the very
beginning. Um, and I I remember when, by
the way, it was so long ago, uh, and it
was so different ago that I remember
that at some point, you know, you had
the the you were scrolling through the
feed and you would get to the end.
There was nothing nothing discontinuous
gym scrolling was just not a thing. And
you didn't have brand pages and, you
know, it was entirely different. It was
it was a way to connect with friends and
family. That's how it appeared. You
could not connect with anyone else. That
was the context. And you know, our
parents and our grandparents maybe were
also on there and they still are and
they still use it. I really don't doubt
that they still don't understand all of
the depth of the data machine that has
appeared behind these services. Um,
we're talking about the service that was
advertising itself as being free and
forever will be free, which of course
was highly fined in in Italy, if I
remember correctly, even under
competition law, uh, for saying that
it's free when actually you pay with
data. Um, so the understanding of people
when they use a service is a continuous
flux in and of itself.
I did not understand in 2007
what
Facebook basically will become. How
could I? Because this is not what it
was. Of course, I understood as things
have changed and that's also because of
the field in which I work. But
that's not natural. Also another thing
to keep in mind uh which is you know
it's scientific it's it's evolutionary
biology people there have been many many
studies on this I'm not inventing here
people are very good from an
evolutionary biology perspective to spot
physical uh dangers we are geared toward
that um we sometimes even sense that
someone might be watching us from behind
and things like that uh we see movement
and and so on. But with digital dangers,
we are terrible, really terrible. Our
brain is not geared to to to understand
the the digital dangers. Which is why a
system where uh control of people is uh
enacted through uh consent is bound to
fail. Because when you so-called empower
people by saying yes or no to things,
that only works if they have the ability
and the time to understand what is at
stake and make informed decisions. And
also, by the way, to actually be given a
real decision to say no.
And we know that one of the biggest lies
is I have read and understood
uh the terms and conditions. And the
same goes for for privacy policies. And
of course, companies don't uh really
make it easy for people. If I remember
correctly, when uh WhatsApp was fined in
Ireland, it was a myriad of more than 20
documents that were interconnected and
referring to each other. And also uh
there was an artist that not long ago
published
an art installation with how lengthy
privacy policies of the main 20
companies in the world are which was
absolutely insanely big and uh it takes
hours upon hours if you actually want to
read those things. Then of course the
text is many times impenetrable like
it's it's at uh n level many times. So
that doesn't work really. Even if you
tone down the the language, the
information is overwhelming and not
really the type of experience that
someone wants to to have, especially
when they're in a rush. I remember uh at
some point I was uh I have a hybrid car
and I ended up parking in a place where
I had to download a different app in
order to pay my my park my my charging
and um I tried to do it. It was crazy
storm with very high winds. It took
quite a few minutes trying to you know
keep myself even straight in that uh in
that wind and eventually I gave up
because there were so many steps so many
things so many you know the user
experience in digital services really
really matters so people will either
click yes yes yes yes yes yes to get to
where they need to be or they will give
up thankfully I could give up because
it's a hybrid it's not an electric car
but otherwise I would have been stuck uh
uh doing that uh that whole thing. So
going back to my point, just using
consent as control is not ideal. So what
is better? What's better is something
called a societal control model. And
this is explained very well in a paper
that I think should be about two years
old by now from uh Daniel Solov and uh
Woodro Hartzog. I think it's called um
Kafka in the age of AI if I'm not
mistaken. And they explained this
extremely well, this duality between uh
the consent model and the societal
control model that having rules, laws in
place that set the standard for what is
acceptable to put in front of people is
a much better way to achieve protections
because then people don't need to take
it upon themselves every single time to
read, to agree, to to basically check,
to do their own due diligence every
single time. It's better like we do that
with cars, we do that with a lot of
other again physical things where we can
spot these dangers, but with the digital
things we don't.
So yes, I'm a big proponent of the
societal control model. And I think that
uh things like having a legal basis and
doing your legitimate interest
assessment in the right way um and maybe
even having some additional controls
than what the GDPR requires would be a
better way.
Um, I love that model and uh how you
explained it and I really do believe
that whenever you put the actual average
person in front of a given choice um
well I'm not sure whether they would
have agre like if you actually give
everyone the right amount of information
I'm not sure whether they would agree
with what the commission did. Um I I
mean I'm I'm young. Uh but I I do
remember when The Kind of Snowden
Revelation came out. And of course I I
have watched the movie uh that um work
of art. Um and I and I do remember that
people were like actually looking over
their shoulder like my laptop is
controlling me. It's like they're
looking at me through the camera. there
still know a lot of people that you know
close the laptop while like when they
are at home because they they are scared
that something may leak through. So I
feel like um the average person doesn't
like being surveiled, doesn't like being
subject to a decision that they cannot
have any control over and um yes is
really cool to use and so is Meta and
Gemini and and all of this um similar
technology
>> between you and the chatbot. That's the
thing because if you tell people and
this again has been studied from
children to uh old people if you tell
them that what they do what they say
will be watched either real time or
after the fact they change their
behavior. This is very clear. It's a
form of if you will selfcensorship which
can work for the better or for the
worse. I'm not judging that now. But
there is a change in behavior which is
also why it's important for people to
know when this is happening because the
thing is you know we focus a lot in this
discussion and in general in in
discussions on processing of personal
data because that's big and we have kind
of grown accustomed to it. But we also
have the other fundamental right which
is the right to private life. The right
to private life is very broad. It
literally includes your ability to
control how you are perceived,
what people know and say. Um, like what
what what is known about you, your
reputation, your body autonomy. Um, and
it's it's it's so it's in a way more
fundamental, if you will, than
processing of personal data. Processing
of personal data is newer. The right to
private life goes back centuries. Like
you can see it even in religious
writing. The fact that you go pray in a
private space is literally
privacy-seeking behavior.
Privacy-seeking behavior exists since
forever. Even some animals have it. It's
nothing that has come with
digitalization and and with machines
basically like processing of personal
data did
uh or became uh well known. So the the
thing is with private life people really
understand
very simple concepts really if you if
you put it that way. For example,
uh one of the biggest myths is if you've
got nothing to hide, then you shouldn't
worry. But it's not about hiding. Do you
have something to hide when you go to
the bathroom? No. But you close the
door. Because the thing is, it's your
private life. It's your private
business. There is literally no need for
anyone to know what you do there. And
it's of course not just the bathroom
here. Your private life is your private
life. Everyone has a a personal sphere
which can be bigger or smaller depending
on culture, depending on who is at stake
and and so on. But the thing is that
that is many times untouchable should
really stay off limits and that's why we
have the privacy directive which is not
dependent on uh personal data being at
stake. That, by the way, is a big
challenge for me now because the digital
omnibus has taken the the protections
around um storing and accessing
information on a a terminal device and
put it into the changes uh proposed to
to the GDPR, meaning it's not going to
be about private life anymore. It's
going to be about processing personal
data. And I really struggle with this
choice. It's it's not a good choice for
me because the whole point should not be
whether something is personal data or
not which might be diluted as we
discussed earlier anyway but rather um
if this is someone's pocket so to say I
can't put my hand in your pocket it's
your pocket unless you allow me to see
what you have in your pocket I'm not
going to put my hand in your pocket and
it becomes something else it becomes is
what you have in your pocket personal
data or not.
>> I love that. It's it's brilliant how you
explained it. Um I feel like a lot of
people would relate to that. And um I
really hope that this podcast is going
to get to a bit more than you know
privacy professionals just to defeat the
point that the GPR is only for privacy
professionals and privacy professionals
alone. Um, before we do let you go,
because I guess that in in the very very
cold Swedish winter, you probably want
to go home at some point. Um,
um, is there anything that you maybe
look um at the digital omnibus thinking
could have been worse or like I actually
kind of like this part. um obviously
taken with all the necessary care.
Yes. Yes, there are good parts about it.
Um and even the intention behind some of
the provisions that I don't like is good
um and could have been handled a little
bit better. But I've already noticed
that I I I've already mentioned that I
do like um most of the provisions around
uh data protection impact assessments
because I it's it's one of the things
that really kills me in practice and not
just due to the different lists of when
uh something is uh is necessary uh from
one country to another um because I
generally work with with clients that
like in all of the past years, it wasn't
really someone that had operations only
in one country.
Um, but even there, the commission tried
to keep some uh or shift some control uh
by having the EDPB propose the unified
list and the model and so on for the
commission to approve. And I think
that's bad. I really do not want to see
the commission which is a political body
be uh enshed with uh issuing uh rules
for for data protection. I I don't think
that's that's a good idea and they
should have left the EDPB uh to to deal
with this. Um that's that's one thing.
Um another very good thing is the
unification of uh databach uh
notifications and not just under the the
GDPR it's GDPR uh privacy NIS and uh uh
other regulations in a single point of
uh sorry my lights are a little bit
weird here uh in a single point of uh
notification so I think that's actually
a very good idea and um controllers will
uh benefit benefit from that. Um what I
don't like there is the fact that they
raise the threshold to notify only in
situations of high risk even for the
data protection authority notification.
Um I
there is a good point to be made there
as well that
companies tend to over notify and data
protection authorities many times
support the idea of notify even if you
are not sure that there's a risk but I
think that together especially with the
other changes it might lead to a culture
of we'd rather not
Just keep it to yourself. Hush hush.
Don't inform everybody or anybody. And
um who's going to know like in that
meme? They're not going to know. Who's
going to know? Um so I'm I'm not a fan
of uh of that. Uh but I would keep these
these two things. I would also keep at
least some of the uh changes that they
are proposing to um
uh the cookie consent mechanism if you
will. This idea that uh you can do it
through uh a signal emitted from a
browser and of course they do refer to
standards to be uh issued later on. So
we don't really know how uh that's
supposed to work. Um what I don't like
there is the exclusion of media
services. That basically means
journalism would be excluded from the
obligation to uh observe this signal
emitted from uh from the browser and I
don't know what's behind it. I can
assume but I'm not liking it. What's
going to happen there? So I would
definitely keep some things. Uh don't
get me wrong. Also, I like the intention
behind clarifying that um if the data is
not uh identifiable for you, the fact
that someone else to whom you give it
can identify makes it personal for you.
So-called indirect personal data which
has uh been caught in uh one of the
paragraphs of the Scanya decision a
couple of years ago. that was when I saw
it when it appeared I was like what are
we going to do with this
wasn't the best choice. Um I I I
struggle really understanding the
reasoning of the European Court of
Justice there. But hey, it is what it
is. And so I I like the fact that they
uh are trying to counteract that uh
negative effect. So yes, there are at
least a few things that I would keep.
Okay. Um, thank you for saying that
because I was a bit worried because I
we're trying to to conclude the podcast
on like a positive note and I was like,
"Oh god, she's going to say no. I hate
it."
Thank you for saving the podcast. Um,
no, but I I agree with you. Um, there
were some interesting points, I would
say, potentially things that were
in in strong need to be changed. Um, and
I think like the the problem when things
get put in an omnibus like it's it's a
big bucket of of things and
um the good things get lost. Um, and
that's that's also sad aside from from
the bad things um which we don't approve
of. Um but yeah um I agree with you. um
there is something to save um and maybe
something to to specify which which
could be saved in the future. Um as you
said
>> that's that's also important to keep in
mind that uh this is just a proposal and
it will follow the normal legislative
process which is dialogue and so the
council and especially the European
Parliament will have a say. It's not
like if the commission proposes
something, that's how it is. Um this no
that that's not how it works. And uh if
I am to look back at uh the GDPR draft,
for example, the European Parliament was
very heavy in um making changes to it
and not accepting uh quite a bunch of of
the initial proposals. So uh I do have
faith in in the the process itself and
um yeah we really need to look at it as
just a proposal. It's I I have seen um
headlines uh in the media in the past
week that the European Union has
unveiled this word that of course AI has
made almost ever present um the the GDPR
changes and so on. No, that is not the
case. uh we need to be patient with that
and uh unlike the AI omnibus which we
are not even touching in in this
discussion right now um there isn't a
time limit and there isn't a pressure to
get it done by a certain uh deadline so
we could actually be looking at years of
dialogue and going back and forth and
who knows I mean the e- privacy
regulation was literally withdrawn twice
So,
it's not the end by any means.
>> It's not the end. And we also have to
remember that this is happening in an
era of social media where it's nice to
share things. It's nice to be on top of
the news
um to be in the kind of ocastic
um memory of of digital regulation. And
um and probably like I would say 20
years ago this wouldn't even have been
in the news maybe like because it's just
a proposal as you said. We'll see what
happens. Um we'll see in what direction
the parliament uh moves and I would say
there are very very good um kind of um
signs that um the parliament will tune
it down significantly. So I I thank you
for making that point because I think
like you know oh it's done it's changed
it's over the GDPR is dead technically
probably just as a cold um if we see it
from from the perspective of like um
being ill you know um someone else there
there still countless doctors to fix it
if if even um it needs fixing. So,
thank you so much, Andrea. This has been
a lovely conversation. I'm sorry to have
kept you inside, especially because your
office wanted to kick you out at some
point. The light were like, "Please go,
go away."
I'm really sorry.
Um, but that said, um, this was all for
the legal fore the podcast. Please guys,
do review the podcast. do leave us a
star because we we're always um in need
for um advice and feedback. So, thank
you so much and thank you foration
again.
>> It was a pleasure to talk to you. Thank
you.
