[Music]
[Applause]
[Music]
Hello, I'm Tom Parker and welcome to
Voices from the SEPs Ideas Lab. In this
episode, I spoke with Art Bogi. Art is
an associate researcher in the global
governance, regulation, innovation, and
digital economy unit at SES. He is a
specialist in behavioral law and
economics, evidence-based regulation,
and public and international law
analysis. During our interview, we
discussed liability in the age of AI,
focusing on some of the challenges this
new digital technology brings and where
responsibility lies when AI systems
cause harm. I hope you enjoy the
[Music]
episode. Well, Artur, welcome. It's a
pleasure to have you here. Thank you for
having me. Lovely. I'm going to get
stuck into the challenges of the
particular topic we're about to tackle.
What are the biggest challenges in
defining responsibility
uh when AI systems cause harm? Thank you
Tim for for the answer. I think that's
spot on and think we need to cover right
now super quickly. uh because it turns
out that when we try to apply the good
old rules of traditional liability to
digital technologies and AI
specifically, we have some really
particular uh problems we need to
address which didn't come up uh before.
So uh the the main three issues I would
say are the passacity so the black boxy
the the self-learning of the of the
system and the uh subsequent inability
to look inside them and to determine
what happened which caused all sorts of
of problems. So I would say that the
first one the inability to look inside
the black boxy nature mean that um the
people who were uh harmed who are
victims of the systems uh cannot really
prove uh first of all who's responsible
because there are multiple entities
involved in in the life cycle of of the
model or the system. Uh that's the first
thing. The other one would be um they do
not know what to even look for sometimes
to pinpoint the blame like for example
uh is the model or a system responsible
for diagnosing me for example uh is it
is it defective and how do I prove it is
if it's defective do I point to data do
I point to a systemic problem do I point
to a life cycle problem within it. How
do I determine that the decision support
system actually did something wrong? So
the lack of knowledge uh is the huge
problem and then finally uh the lack of
insight because even if you heavily
suspect that the decision support system
diagnose you uh wrongly then uh how do
you pinpoint and gather the data
basically your evidence to claim and to
construct a case against the um the the
system provider or the system
constructor or or whoever, right? So,
you do not have access to either the
system itself or to data um itself. So,
that would be the problems uh that we
are facing right now. It's sort of like
being a victim of crime and then you go
to a police lineup and there's just no
one standing and it's a blank wall
and and you're asked to describe your
attacker and what's happened and you
can't there's no information. You can't
see what's happened and so therefore,
how do you ascertain blame? How
therefore is the EU approaching
liability in the age of error? How you
how are you going to sort out those
problems? Oh yes. Well, EU is is trying
uh the best or is trying for for a
couple of years now. Um regarding
regarding um AI liability or AI as a
whole. Um EU was trying to construct a
legal system around AI and rightly so. I
think so. Um we need to distinguish
between uh the safety regulation which
is mostly prescriptive which would be
your AI act now enforced and uh being
applicable onwards um and the liability
part. So an X aante the prescriptive
part and the expost. So the liability
the the prescriptive part the AI act
would be okay this is what you need to
do in order to be actually in line with
what we would like AI systems to be and
then liability part is the exposed so
who's responsible if the things we which
we want in the first place to happen
actually materialize the they could be
the same entities but not necessarily
and so in the liability portion
um the exposed um portion of the uh EU
regulation of AI. We have two proposals.
Um well, the first one is the product
liability directive, a a renewed version
which is which is now um um in force and
will be applicable from the fall of 2026
uh onwards. And that's the EU level of
liability regulation. And then we uh
used to have uh until very recently AI
liability directive proposal which aim
was to um harmonize the primarily
national um liability regimes uh into
one place and sort of bring them up to
speed when it comes to uh AI challenges
that we just uh mentioned. So how those
two were supposed to tackle that and AI
act in in uh in combination as a as a
system is that uh PLLD program liability
directive brought up three new ideas. So
first of all uh we treat uh AI as a
software. So we actually can apply
product liability rules now to AI. huge
step because one of the question is is
AI uh is AI a software? What is it? So
we now know uh AI is a software. Uh
second of all the second of which is um
what do we do exactly with uh people not
being able to to prove what was going
on. So the reversal of the burden proof
uh is now in the product liability uh
product liability uh directive. So now
it if something happens uh we can
reverse the the burden of proof and now
it's the companies the eye companies
that have to prove there was no defect
and that they did everything right. So
the presumption uh they did everything
right uh in order to prevent what
happened. So it's not up to well not not
in in each case but it would not be up
to the um the victims to prove that uh
that something happened but actually to
prove innocence. I know it's it's
controversial but it is probably the
only way to actually tackle it. And then
the third uh crucial aspect of the
programability directive is that uh
court uh judges actually can make AI
companies to share the data, the
insights and the information on the
models on the systems on the data sets
uh with the victims. So they can build
the case um if there is a reasonable um
degree of uh probability that the system
was responsible for the harm which is
huge and that's why I would like to
point out that probably actually PLLD
which was constantly in the shadow of
the AI act actually could be way more
impactful for the AI businesses um
because of this one rule that you need
to actually provide of course with
respect to IP rights and so on and so
forth provide the information uh to the
victim could be more impactful uh than
the prescriptive um AI act. So I think
businesses should really watch out uh
for the PLLD and actually gather all the
data and all the information to to be
able to actually comply uh with it um
than to uh to watch out for the AI act
which is now way of way of being fully
in uh in force and it it is because the
product liability directive actually
applies to all the systems not only to
high risk and general purpose as if the
um AI act and the um late
u AI liability directive in contrast was
supposed to do something different
because product liability directive does
not apply to everything. It applies to
physical and psychological harm uh and
it applies to corruption of uh of data
sets. Um but if uh we would for example
um if we were chatting or someone else
would be chatting uh with a one of the
prominent chatbots and the chatbot
claimed that I am involved in some huge
conspiracy to throw over the the
government for example. Uh this is not
covered by the uh product liability
directive. This is um this would have
been covered by the um AI liability
directive. Uh what if for example we
connect a chatbot to to a conference and
ask it questions and the output would be
just a set of racial slurs for example.
So discrimination that's also not
covered by the product
liability directive. Um what if uh we
discover that prominent chatbots were
trained on data which wasn't freely
available or the output of the model
actually actually infringe of the IP uh
of of uh of other entities that's also
not covered by the P and would have been
covered by the um by the uh AI liability
uh directive. So this is what we gained
and this is what we lose lost. So we
need to now think about what's what
shall we do because the the the three
aspect that I just mentioned um so our
personal rights uh our the
discrimination and the IP problem
remains not unsolved because it is sort
of regulated on the national level but
it it it just became so much more
difficult to to enforce from both
businesses and from the victims because
what you need to do is basically tackle
the 27 liability regimes for all of
those issues. So we lost the
harmonization on the national level
which seems like a problem for actually
innovation because it it creates a whole
lot of costs uh to to deal with it and
also the PL as you said is going to be
coming in in late
2026 and we already know that the AI
landscape is rapidly evolving and and
how how do you deal with the fact that
there's a gap between that coming into
play and today and the fact that you're
losing certain things from the AI
liability directive as you've said um
what's the state of affairs and how do
you deal with liability when it's a fast
evolving landscape as the the reaction
of regulation to technology landscape is
always a problem law is always behind by
nature and I would like to start with
that establish that it has to be
reactive by nature it has to be reactive
exactly by nature so this is not
necessarily the issue but what we need
uh in order to sort of catch up is to
establish the system as fast as possible
and letting it grow with the
advancements. So there there are two
different uh behavioral reactions um I
think between the prescriptive nature of
the uh ex anti-AI act and exposed
liability because if you regulate AI um
in two different ways you you get
different reactions as I've mentioned um
you either comply with all of it and if
not all the agency will go after you or
you leave the market to it. it allow
them to do their best and if only they
and some commit some harm to entities
then you go after because then they can
get sued which which is a completely
different dynamic that's why I think we
need the AI liability system to
complement the prescriptiveness of the
AI act to close the legal system of AI
in EU as soon as possible and actually
letting it grow because we need doc
legal doctrine on it. It's it's the
first time we are doing this thing. So
we need the doctrine to develop. We need
judges to adjudicate on certain cases
because we need to know what certain
definitions mean. what what are they
even exactly what are the precedents
what I already mentioned it what is a
defect uh in the context of of AI we
need adjudication on it and and as soon
as we have the liability system in place
then we actually can move forward with
it and we we really need that to
complement uh the AI act because um in
in the current light in in in light of
the reports on competitiveness of
how EU should go forward, how we should
actually get growth going and innovation
moving. Um, we we know that the AI act
is going to be um changed at the end of
this year um and sort of slimmed down as
as they say. I think we can't do that uh
without closing the AI liability gap
because you can't strip down things from
the prescriptive exante without having
the exposed in place. It should have
been the other way around. sort of
cutting red tape as they call it what
whatever that means uh is not to cut the
AI liability directive because that
actually allows the market to do the
innovation, the growth, the whatever
they need to do and then if they do
something wrong then they can get sued.
um then you actually need to probably
cut down on the AI act if you would like
to set them a bit more uh free, right?
So my pitch would be get this AI
liability system down in place if you
would like to boost innovation, not the
other way around. Well fingers crossed
that happens and it's been a pleasure
speaking with you today. uh look forward
to to hearing more about this extremely
important space. Uh so thanks for your
time. Lovely talking to you. Thank you.
Cheers. Bye-bye.
[Music]
[Applause]
[Music]
