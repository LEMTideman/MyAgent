everyone Welcome to our event this event
is brought to you by dat do club which
is a community of people who love data
we have weekly events and today is one
of such events if you want to find out
more about the events we have there is a
link in the description click on that
link and you'll see what we have in our
pipeline which I don't think is a lot
right now but we will be adding more
events in future so keep an eye on that
link do not forget to subscribe to our
YouTube channel this way you will stay
up toate you will get notified with all
the future streams we have and do not
forget to join our data community in
slack where you can hang out with other
data inas during today's interview you
can ask any question you want there is a
pinned Link in the live chat click on
that link ask your questions and we will
be covering these questions during the
interview
I will stop my screen
now I will open the questions and I want
to make sure I pronounce your name
correctly is it
anahita
yes
great so the emphasis is right yep also
make it shorter if it's hard but yeah
it's
Anita
okay so then um okay let me
open the document okay I have the
document and if you are ready we can
start yeah let's go shoot yeah let's go
so this week we will talk about
knowledge graphs and TMS and how they
are used and research in Academia and
Industry and we have a very special
guest today anahita anahita is
originally from Iran she transitioned
from mechanical engineering and
specialized in applied mechanic in
Sweden then worked in automotive
industry for 5 years then she shifted
Focus to pursuing a PhD in applied AI in
Germany and yeah she combined her love
for life and learning with a passion for
cognitive
sciences and she likes dancing painting
[Music]
running
yeah that is a summary of CH so yeah um
I think it's doing a relatively good job
in summarizing and yeah as always the
questions for today's interview are
prepared by Johanna berer thanks Johanna
for your help and Welcome anahita to our
interview
hello thanks for the invite and hope and
also thanks for the intro was covering
great yeah and before we go into to our
main topic of knowledge graphs LS and
all that um let's start with yourr I
briefly mentioned your background but
probably you can go a little bit more
details so can you tell us about your
career Journey so
far yeah it was always a bit tricky to
know what is the next step but I could
as a summary you said it perfect as um
studying mechanical engineering it was I
think I really loved math but I wanted
to be a practical so that's like how you
end up to being engineer I would say
basically and then I moved to Sweden
because I was really curious to to study
a bit more and be a because in The
Bachelor I really felt I didn't learn
much I didn't really feel to be an
engineer and that's like why I went into
the applied uh mechanics to really
feeling it that it's you can use it in
the in the different analysis and then
started working and being living in
Guttenberg is a bit hard to be out of
automotive industry so he started
everything was quite random I would say
with a bit of higher probability and um
and then uh starting at the automotive
industry it's a lot of Demand on
Automation and I was really lucky to be
in this um company because it was a lot
of
unestablished uh uh coding and
automation
so usually you don't get this chance of
doing a lot of coding in companies as a
start graduate a fresh graduate and then
a lot of cod and also the plan was that
after a couple of years I would pick the
topic that is interesting for my PhD but
I thought it would be two years but
after two years I had no clue what PhD I
want to do it and then after four years
I figure okay maybe I'm looking in the
wrong domain because I spend most of my
time optimization in uh Automation and a
lot of semantic reporting for the
company then I dare to feel that okay
maybe I should move toward computer
science and data science and that's when
I started to shape my PhD proposal and I
had a tough time to get funding in
Sweden and that's why it took me to
Germany to work in frontop sky in the
topic I like to do my
PhD and of course semantic reporting The
Next Step was it for that to have a
knowledge graph for CR
simulations what do actually mechanical
engineers
do in applied
mechanics the program is so focused on
finite element analysis and com and cfd
and so it's like you uh small um break
break down the physics into small
elements and then try to find the
behavior of them I mean in a really
basic explanation so you have a lot of
different physics that you want to
predict the behavior and then um you try
to use different um um equations to find
what is the response to that change to a
force to a um vibration to a flow of
fluids or uh to De formation of a crash
as
well I guess the um application of
this uh would be in automotive industry
like the designing of a car right yeah
for sure so like before in the past it
was making a lot of prototypes physical
prototypes and doing the test and since
they were really wanted to shorten the
development and also save a lot of money
because making prototype is a lot more
expensive than build building in real
car and then they really they are one of
the leading industry in using fet
element analysis so like for example
truck uh development or any other
developments they are far far behind
compared to Automotive because of the
high demand of these kind of
analysis there's a system with I don't
know different moving parts and then you
want to change one of the parts of the
system and you would you want to predict
the behavior after this change right
and this is what you it's usually not
like that so you come with a design so
you say Okay I want to have a car
looking like this and then you say that
I want to have these performances and
then you try to for for example ourself
to see how we could fulfill all the
requirements so the usually the the the
surface or the target of the market
really defining uh how we should do the
improvements and so usually comes with a
targeted Direction uh of the
performance so this finite element
analysis is it in any way related to
machine learning or it's a totally
different thing we could say it's not
that you H you could say it's there
still numerical analysis so it's not
unrelated to uh differential equations
but you are not solving based on uh uh
cost functions and data so that's the
different thing so you try to model the
behavior and predict it uh so you really
a lot of um investment a lot of focus is
on developing good material model to do
the
prediction so this is what you studied
and then you you said there was no way
for you to avoid the automotive industry
and you were doing optimization
and semantic
reporting what are these things like
what kind of optimization you were doing
what kind of what is semantic
reporting so so in which one could I
start for with the semantic reporting
what I'm referring to is that like they
are it's it's an interesting topic to
have like Fair data I mean to to have to
be able to uh regenerate the analysis
because what is seen in ahead of these
all of test and crash test is that you
don't need to do any physical test
anymore so it's kind of that on on a
release of vehicle maybe you release a
model of finite elements and then all
the tests will be done and I don't know
if you know for example of these test
are you rank cap so when a vehicle comes
to the market you really need to rate
the vehicle or you need to pass some
legal requirements to have it sellable
on the market so videos where they put
manans inside a car and then the car
drives to wall to the wall yeah a lot of
different yeah exactly so like a lot of
sensor measurements looking at the
injuries on the occupant on the driver
and so on so the main measure is
actually the injuries
exactly and um I forgot the question
what was it that I end up here I was
asking about optimization and semantic
reporting yeah
and so from the reporting so was in this
point that we need to re be able to
reevaluate or regenerate the results
that we are reporting and uh for that
aspect it was like also within analysis
we generate a lot of simulations like
for example for one vehicle it could be
more than 500 simulations and with this
semantic reporting was main focus on
reusing the data and the long run to be
able to regenerate the results because
like one CR simulation could have like
12 million elements and then it could
take around uh overnight andal like 17
hours on like
1992 CPU so it's really costly analysis
and with that with these semantics was
like really classifying all the measures
we have like the sensor data and
barriers and so on to be able to compare
the results so like to instead of and in
the past it was like generating
PowerPoint and Excel sheet so it was a
lot of scripts that were Auto generating
the Powerpoints but still you really
couldn't compare and with the comparison
I have seen say Engineers putting two
PowerPoint that are generated with
scripts and try to compare curves so
that's a like one of the real change was
that you you can click analysis and you
can overlay all the sensor datas and
after that within Knowledge Graph coming
from the semantic reporting is to find
the patterns in those and within
optimization is like a lot of time
within engineer you Tred to change
thicknesses or adding holes and so on so
it was like trying to use these
geometrical changes to be optimize the
behavior we required what will be so
it's like a lot of topology optimization
and the
designs so for
example uh for pedestrian analysis one
of the main uh components that is really
important is the inner hood that because
you hit the head there and then it's
like how to cut the mass and have still
a good stiffness uh but not so hard for
if you crash to a
pedestrian and um that's the target of
the
optimization and all of that happens on
a computer right yes so you don't do
like I assume you don't actually crash
into pedestrians to to
see no luckily not but but it's all of
these are also a bit of try to model it
so also in a computer we don't really
hit on a pedestrian so we have a head or
we have a leg impact and then there are
there are sensors in them to measures
the accelerations to see if this is okay
or not and then after a while we still
do a really physical test but not on a
real pedestrian so on this model of head
or leg to to do to evaluate how good our
modelings
are oh white called I understand why
it's called reporting but why it's
semantic reporting because you define
like with the is defining all these
measures so it's kind of uh modeling of
data uh behind it so like before that is
just it's also a bit of semantics on
those generating the Powerpoints but
still to to be able to compare these
simulations you need to have semantics
to connect different analysis
together okay yeah I think I more or
less have a picture in mind so yeah
there are different experiments
and um yeah so these experiments could
be
related so for example there could be
like I don't know maybe can you give an
example of two experiments that are
related could be like we are in a car
and there is a we want to see like we
want to hit the wall and see how much
the driver is injured but another
experiment is how much the uh the person
who is sitting near by is injured right
so this could be two different
experiments that
relas no I mean it's like it's really
sensitive analysis I would say so we I
with this relations with small staying
really really much much smaller changes
so it could be that for example when you
for example we make a lot of impact
points in the hoods and to see how uh uh
good the performance of the whole
vehicles and they are usually usually
for example with distance of 100 mm so
quite small still and then one of these
relations could be that like uh how much
the area around it like for example if
it's small slide a bit on the impact how
much robust is that and how much they
are still staying with the same behavior
of uh of the acceleration because
acceleration curves they are really
sensitive they could it's also hard hard
to model them to get the correct
acceleration curve and uh so I would say
from driver to passenger it's a lot of
change to to say how much they relay
because most of vehicles are not really
symmetric either like you have an engine
there and you have like a cooler there
so it's a lot of we try to have a
symmetric impact I mean symmetric uh
intrusion but it's really tricky still
to have it and in a real life most of
the impacts are not symmetric
either so
do I understand correctly that this is
something you started doing while still
working at uh the automotive industry
and then this is what led you to do
graphs
now researchs because uh like you were
talking that uh about these impact
points that are close uh to each other
right and this this means that you can
build a graph of all these sensors right
or impact points somehow yeah yeah and
this is how you arrive at the graph yes
exactly I mean this is also like when I
was trying to develop this webbase
reporting or semantic
reporting uh since it was many analysis
it was really oldfashioned old web
technology and then it was quite
necessary to have a backend and a
database to improve the performance and
also front end and at that time when we
started to decide what kind of database
we need to use and we decided to skip uh
relational databases and that's like I
would say when looking into graph
databases started because automotive
industry has so many changes that if you
want to have a relational database it's
really time consuming to maintain it
it's like when I started with my company
we were looking into implementing um
data management systems because this is
like an old topic with an automotive to
maintain the part because like one uh
vehicle has a lot of different parts a
lot of different models how to connect
these like the input data how to be
related and um and they are looking into
relational databases and like to really
establish it it's time consuming so
that's why uh my mentor advis to use
this um NE 4J or any other uh graph
databases and that is very started but
it's not also easy to just store crash
simulations or any Fe analysis in a
graph database because you can't store
all all of them all the
data I'm trying to visualize this
graph and yeah I don't think I can like
what could what are the notes what are
the ages in this graph what kind of
relationships do we can we model
there if we talk about semantic
reporting yeah so like one of Basics
start is like that you you we follow the
R&D development so like from a vehicle
that is supposed to be on the market and
if you stay with crash because it could
be any different analysis and
requirements like durability and so on
within the crash we know what is the
year of the release of the vehicle and
then what are the required performance
so this is like some kind of kind of
semantics to connect the current vehicle
to the V on the markets and be able to
aim to compare the performance of the uh
this um so kind of benchmarking to see
how we could perform so like from the
weight of the vehicle size of the
vehicle and so on and then coming from
Vehicles then one vehicle that has it
has like a
platform and uh and also um upper body
of like how the vehicle looks like and
then in this so it's like really
capturing the the structure of the
company how they are developing because
these relations also help to relay the
the analysis and the behaviors so what
why for example this platform an upper
upper body is important is because you
could compare the siblings vehicle so
you see the cars that look like the same
but they have different right height or
a bit bigger trunk size or stuff these
vehicles are still could be related and
it was a topical of interest to see if
we could predict uh simulations based on
one platform and upper body to another
platform and upper body and if we go a
bit more detail between simulation was
like to con
convert the physics of the problem to a
graph so like what is really important
from a CR simulation Engineers to detect
a behavior of a crash to say this
simulation is similar to another
simulations from this
analysis con
Concepts so while you were talking I was
taking notes and then I drew a graph so
in this graph I have a note with year
when the car was made let's say
2020 right so this is the car U this is
the year and then we can have notes of
individual cars and the connection
between the car and 2020 could be the
year when it's made then the how do I
say correctly the body type or upper
body type yeah exactly mhm and then I
guess there are other different
characteristics of a car right and all
the of these are nodes and each car is
also a node and the relationship
between the no stages is like okay made
in type of the body or I don't know
engine type like different
characteristics different features mhm
okay and why is it a useful
representation CU if I think about about
machine learning so in machine learning
quite of when we have table data right
and then it could be uh we can have row
each row would be an experiment and then
we can have uh
different features right features could
be year type of the body upper body type
of the engine other things and result
like the target the the thing we want to
predict could be I don't know what do we
usually predict like impact on the
person or on The Pedestrian or whatever
right so like why do we actually need
graphs here what can express with
table I mean a lot of things that you
have it as a graph you can express it
with table but then it's mostly on
abstraction that is nice and also in in
to finding a a bit more complex
relations between them so for example
one of the main gains I have in my PhD
is to have a visualization that you can
uh have a look over 300 simulations so
if you want to compare 300 simulations
in a table with visual look I think it
would be a bit tricky and uh and we like
using weighted graph and uh visualizing
the simulations and then their parts
that have been involved mostly in the
crash it's um was simply clustering the
simulations and also showing what are
the important parts for those crash
simulation or what is the common parts
in those and uh and also like for
example to detect a load pass uh so it's
like when you really transfer the
physics of the problem into the graph
you can also answer different questions
like load pass detection so it's like
load pass is like where is that if you
walk if you trans a structure of the
vehicle to a graph and like each part is
one node and then if they're connected
they have a edge between them so
connected to another parts and then you
try to get the how much each part has
absorbed or been involved in a crash and
then you try to find what is the main
path of the load transfer within the
structure so kind of the longest path
use and that is also a highlight that I
don't know of a way to really see in a
table
so that's what you do in uh your PhD
research right yeah
exactly and as I understood so you have
a system it's based on NE forj and then
you have different graphs and then oh
like you have a graph different notes
and then it allows you to do this
exploration so you can click on the note
and then okay like let's say we want to
look at cars that were made in 2020 then
you can maybe click on the 2020 though
and then like explore different models
and how they perform in experiments
right yeah and see like all this lot
path passess or paths
um and other things right that's a bit
of I would say a lot of some part of
this knowledge graph is in uh data
engineering and I think most of the
thing you can do within Knowledge Graph
maybe I am not really that experienced
to comment it but I feel you can also do
it with relational database it's just a
lot of overhead of Maintenance and so on
between these two but what is quite
interesting for myself is that when you
can extract a computational graph from a
Knowledge Graph and then do further
analysis on that and these that I
referred to they are mostly on graph
data science specifically compared to
Knowledge
Graph so Knowledge Graph is um so we
have a car and the car so the parts of
the car they have different
relationships between each other so they
are connected and then all this uh
sensors um like then also different
characteristics of a car like here uh
type of the upper body all these things
so this is the knowledge graph right yes
exactly and and then you have the node
degrees there so you can do a query
there so what are the car other cards
related to this one what are the parts
and so on so you can do a lot of query
in that level so this is how we express
what we know about the cars but then we
do this stimul simulation right crash
simulation and somehow also record the
results in this graph or exactly yeah MH
and this is the computational graph that
we have like I think I don't understand
what yeah Sor so for the no that's not
the I mean this is so we have the
knowledge graph that includes a lot of
simulations so it's like it hold the
market different costs but then it also
have a lot of simulations of that
vehicle included and then we pick for
example simulations and parts as the
input data and then we make like a graph
like networkx graph or whatever of graph
data science and then that is the one
I'm calling the computational graph so
then it's not related to Fe analysis
directly anymore so it's just graph data
science or machine
learning yeah and what do you actually
mean by gra data graph data science or
machine learning what kind of things you
can do with these graphs so you
mentioned networkx which is a library in
Python right which can work with graphs
so once we express this graph and
network what can we do with this one is
like predicting a similarity of the
simulations or like this longest pass
analysis I told you or a lot of uh
visualizations with the as a uh because
like when you load the whole Knowledge
Graph to for example having this um uh
weight as a weighted graph and
especially it's also not easy to have
weight on all the edges then it will be
a bit tricky to have a good
visualization so that's why I call it
like a computational that you always
look at the part of your whole Knowledge
Graph to visualize it and find the
pattern but it's a still the whole um
data is coming from the knowledge graph
mhm I'm curious so so you mentioned that
graph data you mentioned two terms graph
data science and graph machine
learning so if I understand correctly
the distinction here is like in one case
it's more like doing some sort of
analytics right or doing this analysis
exploring the graph and in the other
case making predictions what's the
difference between these two I think
graph data science and graph machine
learning are the same so it's like
saying what's the difference of data
science and machine learning yeah same
same different I would say okay so what
kind of things you can like okay we you
you mentioned actually things that we
can do but I'm curious when it comes to
machine learning like when it comes to
predicting
something yeah we can see if two things
are similar using different similarity
metrics they are more like graph
similarity
metrics yeah I don't remember like I
remember that there are different
measures it's like Sim rank is the
method iuse used so that's like items
that are referred by similar items are
similar and uh so that's like how you
predict the similarities of items
because you really don't have any
ranking uh for the simulation similar so
you give us the input for example one
simulations and rank what are the
related other analysis to to that so
it's more like unsupervised machine
learning so here you don't really have
exactly Target okay I see and then like
your task is to understand the
similarity and you use different uh in
your case it's simr to understand how
similar to experiments are two
simulations are yeah do you use any
supervised machine
learning I tried uh a part of it and
that was on a really really small uh Toy
example uh so not really on a complete
vehicle and that was what I relate to
siblings vehicle so when you have the
same the idea was that for example when
we have
um uh a one platform but two different
vehicles to be able to predict the uh
the behavior of them and that was like
uh that I had a toy Fe model and then I
was trying to see if I have a set of uh
data uh like a development tree that
means that you simulate ations are
related based on the changes because you
always do a small change so for example
you pick one part and do a thickness
change you pick another part and do a
whole adding a hole in that so
considering this uh three and then the
physical changes relation to see if we
could um uh transfer these uh behaviors
to another uh uh sipling vehicle and
with that it means that the design
changes are the same it's just the
impact mass is
different so and that means we have more
kinetic energy in in that one and uh and
like uh since we are in a high
nonlinearity with crash deformations
it's like when you add a lot of more uh
um mass in that you increase the
deformations and it's really get hard
and are harder when the difference of
this mass is huge but in the smaller
level it was quite uh interesting
results but still since we had limited
number of simulations because I wanted
to have it like connected to reality
maybe you have Max 300 simulations that
you want to transfer it and uh so what I
was doing and was quite interesting it
was pair learning so I was trying to
connect my uh use the connection of
simulations and use the relation of the
edges between the relations to predict
the level of absorption in
them you understand correctly that
graphs give us not only the way of
expressing knowledge storing knowledge
in them but also when it comes to
actually making the
simulations we can also Express like a
note could be a different
experiment and the connection between
them and H would be the change that we
make right somehow cuz like if you make
small change these two experiments are
quite related and then you can observe
you can also Express that you can
express that in a graph database that
these are very related there was only
one small change between these two
experiments and also you can record the
outcome yeah the this change which would
be quite difficult in a table N I guess
yes
yes yeah it would be like like I imagine
if I again talk about um table format it
would be I guess changing one of the
features and then seeing what happens at
the
end I think in this scenario it could
also be that uh you because we we were
looking at simulations where compared to
all of the simulations so you can still
think you have it kind of table or in
the in the in the that framework but I
think what was quite um interesting is
that when the SIM ations have a edge
real connection we know they are closer
than the one so like um it's kind of
additional information in uh in having a
development tree
included and you also work on the LMS
right and you see how these knowledge
graphs or graphs are andms can work
together so can you tell us more about
that I think it's it's a quite the niche
topic now with llms and knowledge graphs
and that's that's really focus in Text
data and with my PhD I really didn't
Focus much on text Data NLP uh so that's
like start it's a quite New Journey for
me it started from Summer after the boot
camp that I did some more of LMS and and
within LMS and knowledge graph a lot of
uh focus is on grounding dancer so it's
like really in this part of
hallucination of uh llms and to see how
you could feed in um uh indexes that
will make more reliable
answers and also I I am like um it's
also I think a quite fun uh domain
because I think they a lot of people
again say Knowledge Graph and lenss but
it's also a lot I feel graph data
science is also uh could support it
because on this uh selecting what is the
most similar note to feed in from a
Knowledge Graph to a LMS it could be
also relations uh that comes from um uh
graph data science on ede predictions
and so
on well to be honest I still didn't
really understand the connection so LMS
work on Text data and graphs work on
graph data on notes and
edges right and does it mean that um
okay we have different features or
characteristics of our experiments of
cars or whatever which are text based
and we can use LMS for these features or
how does exactly the connection look
like yeah I mean one of the funny
example was that like this the questions
that LMS can't answer so one of them is
like I think was like uh person a give
birth to her child in can Canada and uh
and then what was it uh and then the
father was there no what was it I don't
remember the example really
good but uh the the thing is what I
wanted to I now should come up with my
own example because I don't remember
that would be a bit fun but the the
thing is that like within the relations
you have within the because when when we
do LMS we have a part of text like we do
a chunks in them and then based on these
chunks you don't have
um uh you don't have all the data
connections to each other and then and
since also LMS are not good into having
huge amount of input together then we
would miss some of these relations of
data to each other and the thing is that
when we build Knowledge Graph based on a
text data we are transferring we are
generating semantics and add those
relations in the knowledge graph
so and based on that you could feed into
llm so it's a bit more of prompt
Engineering in that direction that what
you should feed in to get the good
answers from llm So within knowledge
graphs you feed in some more of relation
so for example with this example that I
was trying to say is that for example
you don't know about uh who is the
mother of a child for example but when
you have the relations of the people in
a graph and transfer that and get all
the chunks of information about the
mother and the father and the child
itself you you enrich the data to answer
the
questions do I understand it correctly
that we have a piece of text that
expresses different
relationships between entities and what
we use an llm for is actually extracting
these relationships from the text and
then once these relationships are
extracted we can put them in the graph
and then use a graph database or
graph library to further analyze the
relationship we extract no this is just
use of LM with Vector datab basis so you
make chunks and for each chunks you use
llm to do embedding to extract features
and then you store them in the vector
database I see and then and this is like
you have the Chun base so like you don't
don't have much of you don't have any
knowledge graph on that but when when
it's a one of the basic one is that for
example to build a knowledge graph on
that is that for example you are getting
a book and instead of having uh just
this for example 400 tokens store in a
note you also have what are the what are
the chapters of the book so you store
the chapters as extra nodes and then you
say for example chapter two owns this ch
and then you you also add the relation
that these chapters come after the other
chapter so you when you are just having
chunks in a vector database you are
missing the
relations and also you are missing the
semantics so these two are coming these
Rel extra relations and semantics are
coming with Knowledge
Graph yeah
so now I think I understood what you
said so you mentioned prompt engineering
and when we come up with a prompt and we
want an LM to answer a question about
something we want to include
also some semantic information about
that like in your example about the book
you can say that okay this paragraph
comes from this page this page comes
from this chapter this chapter comes
from this section right so sort of extra
semantic information that will help LM
to to give a better answer right yeah
and and like for example when you do a
template for your prompt you can Define
the relations that are important so and
then since it for example and you can
for example put it like a cipher query
as a example for one of the use case and
since it has access to whole of
Knowledge Graph it could make similar uh
Cipher query to find information about
its own use
case so there is a question from Ros in
Chestnut that's it's a very interesting
nickname so the Roos in Chestnut is
asking or yeah this is a question it
sounds like transfer learning is it the
same idea used on
relationship and which content he's
asking is it about the because the only
transer learning I have been doing it's
about the predictions on automotive and
I I'm not sure if that's the question
here but I guess like uh so you
mentioned embeddings making embeddings
from using llm right so we kind of use
the knowledge that is already in llm to
make
embeddings maybe this is it I don't
think so because like in transfer
learning is a lot of time when you are
uh having some layers so because like
with these llms content it's not any
it's more of Rog I would say retrieval
augmented Generations that you use the
embedding you use a model that exist and
you try to do the edding on them and
within the transfer learning I I think
it's if I look at it from window of deep
learning is always that you have a model
an architecture and you try to transfer
it from uh one data set to another data
set and uh and that's in more of level
of fine tuning I would
say so the reason I thought about the
example I mentioned like when there is a
piece of
text and uh
know an example could be that this is
the mother or different I don't know
relationship between things in text and
we want to ask anlm to extract this
graph cuz recently I was doing something
a little bit similar I was not I I had a
piece of text and I needed to create
Json from this text so I needed to
structure to give it some structure and
then of course instead of doing it
myself I just ask nlm to do this and
chat PT did it pretty well so that's why
I thought maybe we can actually also use
it
for in not just only extracting Json
from that but also extracting
relationship from
text yeah it's uh it's quite I have
tested it a bit uh also it's like with a
lang chain I would say that it's kind of
a tree of thoughts that you ask it
question it give you some item and you
go further down the only tricky thing in
that is a bit like um you need a setups
to work because uh because if it is just
couple of
uh
um a limited amount of questions and
depth to walk I would say it's easy to
check it but when you want to extract a
lot of uh content from your text is
quite tricky to trust it and a lot of of
time that knowledge graphs come with
llms it's exactly the opposite you want
to have something that you can trust and
that's why I think it's still the the
old process of building Knowledge Graph
is more is required because if you have
it with llm you are in a loop you you
can never validate it maybe it can help
you but still to verify it is quite
tricky and U with GPT you can also see
that if you ask it for an output today
it could have different kind of results
uh after a while or with a bit different
uh content before uh of uh so the
history of it is affecting on the answer
that it's
providing and right now I'm looking at
one of your GitHub projects so this
project is called
adpt l r n f h y c s s I guess it's
adaptive learning physics right yes
yeah quite tricky name yeah so can you
tell us more about this project that's
uh okay that's actually it's interesting
because it's quite connected to what we
just discussed because this was a boot
camp project so like first time that I
uh we tried to kind of for myself also
to connect Knowledge Graph with a lens
and that was the idea of thinking that
like nowaday you can ask a lot of
question from GPT and how it could be to
have a platform or a code piece of codes
that you can it can make you uh learning
material so and our first Target was to
have it on
physics and uh but as it is with a lot
of projects you aim for something and
you end up on something else so and that
was exactly because we had just two
weeks to finish this project and uh and
then a lot of hallucination and
untrustworthy of the GPT on generating
the con so like how we were uh doing it
it was saying that okay um so we had the
Lang chain with breaking down a topic
and we wanted to make a knowledge graph
for physics that you can uh uh uh
provide material to users so like first
users comes and then you ask some
questions about the interest the levels
and so on and then it provide you
stepbystep uh content question and
answer that supports you to learn a new
domain and that was the initial idea but
since it was building the knowledge
graph really hard and really not good
results out of chat GPT 3.5 turbo and
that's like when we decided to change
the project a bit and instead of having
um um learning platform for uh for
physics uh we moved it to support still
in a learning Direction but like
supporting people to read papers so like
uh if you are not a researcher or if you
are a researcher how we could have a
platform that you give in a paper and it
break it down to you and it could
support you with the same consequence
that you feel comfortable to read paper
in domains you have never read and uh
that was like breaking down uh the paper
to the sections find the relation of
them and also connect it to other papers
and the references and so on So like um
that was the final result of it I I'm
looking right now at visualization so
what happens there is that uh there's
this um part where you upload a
file is all you need in this example so
you you DR or you upload
this and then what it's doing it's
extracting text from the PDF and looks
at different uh terms there different
words right yeah and then it builds a
graph from these words and explains each
of them right or how that does work it
has we have two tracks so first tracks
it's like uh doing semantics so it's
like extract each sections of the paper
and then doing bedding on them and
finding the relations between them and
another track is that it get each
section and use in a some steps with GPT
that what are the keywords for these
sections and then defining the keywords
of them so it's independent of the graph
so it's with a several proms to extract
more knowledge so then I think at the
end of this you see that when you Hoover
over some keyword it show show you a
small uh um box that is describing that
so it's more of having all in one place
that you don't need to move around to uh
to read a paper and uh and also with
these uh sections and this graph and you
like click on the edges of them you can
explode it and see what is common
between two sections and what are the
differences and this is more interesting
when you have two different papers
because for example you want to compare
their method development and then uh
when you click on the edge it explodes
and summarizes the differences and
similarities of those ones and this work
is working only on archive papers
because really extracting semantic with
PDF and getting these sections it's not
so simple so because it could really
defer how people are publishing and
generating their PDF and with archive is
good that they always get in the text
and generate it themselves so it's a
more more trustworthy to extract the PDF
from them and then what you do we do is
that like when you also upload the paper
we could uh get all the references out
and then uh make a summary of the paper
and its references and show what is for
example the most relevant reference to
that because also on visualization we
are using page rank that like this size
of the nodes are referring on how
important that Noe is in the whole uh n
workk that you see and then we could see
that okay what is the m most important
reference within this domain and this is
a help a bit more for people who used to
read papers because for them it's quite
uh timesaving to find the most relevant
related work to look in to dive
in I wish I had a tool like that when I
was doing my
masters so like
I was doing information retrieval on
mathematics on mathematical
formula and um yeah just like reading uh
especially at the beginning like doing
the uh state-ofthe-art
research understanding what is there and
like reading just random papers that are
potentially about the topic and then
feeling completely lost there uh yeah
that uh like I think I would have been
more effective with a tool like that and
I I I I wished I had something like
that also more
yeah yeah sorry please go didn't hear
you
sorry I just didn't hear you so I just
asked what did you
say so like all everything I was saying
you didn't hear no just the last part
because I started to talk and then yeah
so I also wanted to mention that like I
was I was taking a German test last week
and I Was preparing for this German test
and there are so like there are so many
grammar Concepts that they
related example connectors right so
there can be um I know connectors that
connect sentences then there could be I
don't know how much German you know like
some connectors there for how do you
call them dependent sentences sometimes
these two sentences are independent
and sometimes the connectors are
actually adverbs not real connectors
right there are different sorts of
connectors and there are different sorts
of different grammar grammatical
things and for me like when I try to
comprehend all that this is just my mind
blows and having this graph structure
where you know some sort of Mind map
where you can zoom in different concept
and see how they are connected could be
quite useful and I think like with a
little bit of tweaking if I understand
correctly this project can also do that
right
yes I think I mean it's a lot of time
with this finding relation is if it's
rely on llm then it's like how well it's
described and so on but I think chunking
all grammatic pieces into a node and
then finding the relations between them
it could be a great start and for me
this the same all the time it's like uh
learning new things needs to find the
relations between them and that helps to
so okay this is the same as this one and
and when you have no clue and just walk
around it's hard to summarize everything
and learn
it and exactly and and that's like uh
the main reason I think we want we came
here but what was the first goal of the
project was I think even more
interesting in this direction that you
can have a a chain of questions going
down that really uh uh make a teaching
material I would say and then and the
reason we call we were calling it
adaptive is that based on how is your
past of learning these paths will defer
because I think uh peoples have
different
um uh technique or different preferences
and that like in this direction of
instead of having one for all teaching
uh processes that the platform can learn
you uh while providing more content uh
content for you but the scope was too
big for two weeks I would
say maybe later on we could work more on
that what was the most difficult part in
this
project I think that we couldn't
automate generating the graph that was
the more biggest challenge and then
also uh to find to change the project
topic to something that it's
really tangible so it's like not domain
specific that it will be interesting uh
for people with no background of
research and so on um because that was
one of the demands to find a problem
that matter for a lot of
people
like for example if I think about
something that is not related to
research and has a lot of different
complex
relationships relations between things I
think of Game of Thrones have you
watched
it yes and like all these people who do
different things they're like they
belong to different houses and uh
whatnot right and like when you watch
this and you think like what's happening
there it's a having a graph there that
explains what's going on would really be
helpful yeah yeah or which people are in
which chapters and like things like that
yeah
exactly okay was the deployment part uh
also difficult I see you used fly I or
it was something that extremely it's um
we had fun on the demo day stream L
didn't work so it's like if we deployed
it on a streamly then exactly at the
time we started to show our demo live
streamly crashed and I would say it's
hard to to develop with stream L when
you go to more complex visualization so
I think if we go back right the graph
was still okay but like when you have a
sequence of uh Dynamics so you click
here and so it's like mostly of State
Management and front end development
because when you have a lot of Dynamics
and inter connections then it's getting
quite uh so much time consuming to solve
this uh State uh within stream list it
still is great that it supports it so
you can do still quite uh
complicated the visualizations but it
will take
time but yeah it's the interface is
still pretty Advanced from what I see
like there is a a piece of text and then
you hover over a term and then it
explains what this term is and I also
see this graph structure and then you
can explore it which is pretty
impressive for a two we project super
impressive I think was where two so it
was I wasn't alone and we were both
having experience in web
development and uh and also my other uh
the other team member yatan he had a
great web development experience
and uh so together it was a lot to do
say even though we failed on the first
topic so we lost some time still I would
say and it's a part of the process right
yeah um if um maybe do you know any
books or other resources for people to
learn more about uh graph machine
learning knowledge graphs and how LMS
can be used for that I don't know if
there are many books about LMS yet maybe
not but like in general maybe there are
some cool resources that you can
recommend I never learn with books so
that's like I that's quite a tricky
things to recommend really but uh I
could recommend courses so I really
enjoyed the courses from
Stanford uh for graph data science and
also
uh recently deep learning AI they had a
really cool short course for Knowledge
Graph and LM so I I really find it
inspiring and uh within llm to be honest
I think it's a still I mean to to to
learn the basics I assume I think I I
like the course of
Sebastian um it's not also yet complete
I think because I I really like the
content his uh uh
contributing I also don't remember his
last name sorry my memory is never super
duper um I'm just trying
to uh find the last name but maybe I
could give the full name later on and
yeah and I think that's uh quite looking
uh interesting I think uh the the
material because the repo is not
complete at for
either and um more than that it's always
digging in and finding new content so
tricky to have it as a old uh fashion
domain like mechanical engineering that
you read a book and you know it's all in
because it's uh I think it's part of
learning is in this domain is to always
have a head or ear out to look for new
things and um and also learning how to
navigate with all of these informations
not to feel overwhelmed and still moving
forward and the course from Stanford um
that you mentioned is called machine
learning with graphs right yes so by Ure
leovit yes okay he's quite a wellknown
person in the graph Community right yeah
yeah and I really also recommend to
follow uh the graph conference uh that
is uh
uh he's taking care of
it and I think it's it has been so far a
Free Conference to attend because the
the research exchange there is really
great okay yeah that's um we should be
wrapping up that's all we have time for
today so thanks a lot anahita for
joining us today for sharing your
experience um with us answering all the
questions um and thanks everyone for
joining us today too being active asking
questions too so yeah it was fun thanks
a lot it was fun yeah okay see you
around and
