hello everyone Welcome to our AI
governance live talk today I'm here with
Philip hacker welcome Phillip I'm so
happy to have you and hello and this is
the 24th edition of our live talks our
topic today is AI compliance challenges
liability transparency and fairness
Professor Dr Philip hacker he holds the
chair for Law and ethics of the Digital
Society at the European new school of
digital studies at the European
University viadrina Frank Fort his
research focuses on the regulation of
digital Technologies particularly
concerning AI he received several
academic prices for his work such as the
2020 science Award of the German
foundation for Law and computer science
he's also a member of the task force uh
AI governance for the German federal
government and co-chairs the working
group on AI liability for the European
Parliament we're going to talk about
that and he's a star for those not aware
he is he's publishing non-stop so if you
check Google Scholar for his name you'll
see nonstop and I don't know how he
finds time to write so many interesting
and in-depth articles so uh welcome
Philip I'm so happy to have you it's a
it's it's going to be an exciting uh
conversation thank you so much this is
all uh flagrantly exaggerated of course
but thanks so much for the kind words
and I'm really happy to be on your
program Louisa it's a long overdue that
we talk to each other more so this
absolutely there's I was telling Phil we
could be probably be hours talking about
AI comp liance challenges so this is our
correct uh I have some hot questions to
ask Philip but before we continue uh if
you want to stay up to dat with
everything AI governance so subscribe to
my newsletter Louisa newsletter. comom
and also to to receive updates on the
next AI governance live talks also the
next cohort of my AI governance training
so the April cohort sold out so the next
one is May so 15 hours so if you want to
listen to my voice over 15 hours live
with me so may they starting may you can
register the website is under my name
aitech privacy.com you can register so
let's get started so let's start with
the hot topic of the moment uh liability
AI liability so for those not aware so I
will have some quick introduction the AI
liability directive was withdraw and why
it matters so the EU has the EU AI act
everybody knows it so it's the first
comprehensive AI law in the world but
the AI act does not over liability for
those in the audience there are non
lawyers so what does it mean in practice
so liability has to do with uh when harm
occurs let's say that an AI system
causes harm to a person this person I
don't know it's injured or uh any sort
of harm so psychological harm
discrimination you can think about
fundamental rights harm or physical harm
so the person is dead any sort of harm
it it will occur regardless of how
perfect the the law is or the AI Act is
there will be harm so we the AI act does
not deal with liability issues for
example who was the the what's the
causation what is the the cause of this
harm was it and the AI system is this AI
system defective what are the rules uh
for compensating the victim so all those
typical liability rules they are not
specified in the a so the does not deal
with that so we still need something
else um so the we had a proposal for an
AI liability directive which Philip
hacker was directly involved he wrote an
index impact assessment um so we'll talk
more about that just an introduction so
this proposal was withdrawn um so but
then Philip I I just want I want to
start with this uh aspect so I I read
yesterday in euron news there News
website that the European commission
will decide whether they to definitely
scrap its planned liability rules for
for AI in August so we are in this gray
zone so you are are directly involved
with this process I I would love to hear
from you so what's really the status uh
first what happened this withdrawn thing
was a political decision was a legal
decision was pressure from the AI Summit
for J JD events made everybody scared
and then they withdrew so what's going
on that's a good question um now I'm not
the commission but uh here are some
things that that I've heard and I can
share and that um that also makes sense
to me so first off I do not think this
was a direct reaction to to the AI
Summit and to uh JD Vance speech uh for
whatever it's worth because uh this is
part of the commission work program
right and the commission work program is
something that is a long time in the
making now for a long time in the making
we had H have had also calls for a
simplification of the EU digital
regulation and agenda and it's something
that I absolutely share I mean you know
that there are all these digital
trackers and then you have meta trackers
trackers of trackers it's just crazy
complete kite cener has one of them you
know it's it's it's not only the I act
and liability it's data act data
governance act you name it you know so
many different rules and they are not
well coordinated and that creates a
problem particularly for smmes and hence
I absolutely welcome the idea of of
simplifying that and streamling Ling it
streamlining it and making sure it can
be implemented but also meant that the
um commission was looking I guess for
some easy ways out to signal both to VCS
and the I Community but also now to an
international say landscape that's more
hostile to regulation that they're doing
something and the AL was kind of an easy
target because it was stuck anyways um
there was a a very Vivid process in the
European Parliament to Kickstart it and
actually that had already started so I
was giving a presentation and actually I
gave one at the end of January there to
kind of be cross-examined on my uh
report that was a cool experience um but
I think it became kind of one of the
easy things to withdraw where people
thought they could cut red tape they
could have a good narrative and they
wouldn't hurt nobody now turns out that
I think that this was just the wrong
instrument to choose for scrapping why
if you want to cut red tape then don't
start with liability because liability
is not really red tape liability just
says there's harm you got to pay if
there's no harm doesn't really matter
what you do it's just up to the
companies to design the best compliance
structures for them the most effective
and efficient ones what is creating red
tape is much more the um anticipatory or
direct regulation of AI in the sense of
the AI act and and other rules um so if
you want to cut red tape that is I think
the wrong place to start off with plus
as you already mentioned um there are uh
harms with AI systems now I'm a big fan
of AI generally uh and I think we need
more of this uh in the EU and worldwide
uh for many reasons and for for many of
the uh big problems that we're facing on
this planet but certainly only AI that
it's done in a responsible and
sustainable way and and uh to make sure
that this is the case we need certain
rules or certain harms now in the EU we
have a new product liability directive
and this is certainly something we will
come back to during this talk the
product liability directive kind of
sailed in the shadow of the AI act so
the AI Act was the thing everybody was
focused on but actually last fall the
product liability directive was voted
into law as well and it will kick in
will become applicable at the end fall
of
2026 and that that will apply to AI as
well so now you have the product
liability directive and people said like
you say you know oh we have the product
liability directive for AI plus we have
member state law so what do we need the
AI the liability directive for and they
have a point in this but the product
liability directive only covers certain
types of harm it only covers harm as
long as you have physical harm so
basically some dies run over by a car or
gets physically injured or also mentally
injured like psychologically injured uh
or if you have consumer property or
consumer data that is lost or corrupted
so if an AI system goes arai on your
computer and all of a sudden deletes all
of your uh all of your vacation pictures
then you can sue under the plld if that
same AI system racist in a racist way
discriminates against you or hurs
insults at you there's no way you can
have recurse under the plld in the EU we
have member state law for that but we
have 27 of these different member state
laws with very different procedures with
very different rules and the problem is
that both for consumers and for
companies that's not a good way to
handle this because the the ml systems
they're ruled out in the entire EU
effectively globally and now companies
have to content with you know these
different patchworks of of systems so it
was not good for Innovation either I
think to scrap the AL um so long story
short it was scrapped but then the then
the uh European Parliament in particular
the reporter uh of the file Axel fos and
some others came out very in a very
pronounced way and said listen guys this
is not the way you do it without any
forewarning the parliament had already
started a process had already held
hearings had already put up a
stakeholder process in place so now uh
they said we need to reinstate this and
the commission is kind of open to this
but it needs a signal from both the uh
Parliament and the council that they
want to continue going ahead with this
project and as it stands right now I do
not think that this is overwhelmingly
likely uh because there's also internal
conflicts at the par in the parliament
about between different committees
whether this is actually the kind of
Regulation you want to have um I think
it was not a great way to start anyways
because it was purely focused on
procedure and and it did not say
anything about the substance of you know
when is somebody going to be held liable
for example if an AI system not only
discriminates against you you have EU
rules for that to a certain extent but
if personality rights are infringed so
it's a long complicated process uh I was
quite involved in it for the moment that
uh expert group is on hold and we'll see
how it goes I'm I'm really looking
forward to seeing in the next couple of
months and helping um people think along
in the terms of you know what how could
we revive this project in one way or
another um I think it might take some
time uh probably more than until the
summer to come up with a new proposal
for a streamlined and more coherent and
Universal AI liability regulation or
even software regulation in the EU so
you think there will be a new proposal
you don't think this is
something big one that you just said so
you think there will be a new new from
scratch you think that delete and so I
think quite honestly I think the current
propos is almost dead I mean it could be
revived but I'd be rather surprised
after I read your uh impact assessment
you basically killed it Philip you you
you you mentioned so many I I love your
impact assessment it's such an
interesting document so many important
points and also the the first draft of
the AI liabilities before the a right
before the final text there were many
things that were not compatible so you I
think you're happy with that right you
wanted a new new thing H well yes no no
I wanted I wanted to redesign it but I
it's true I think I have to take part of
the guilt because um maybe the impact
assessment sorry no just you CED it with
your arguments not that you were of
course yeah no but I mean I think some
of the arguments or some of the
proposals that I made in that report
were perhaps too far-reaching for some
particularly of the member states who
then became kind of entrenched in
opposition to the file and that maybe
helped kill it in the in the long run I
mean I don't know so anyways I'm happy
to you know I'm happy to contribute to
more ideas to you know how how we can
really generate a harmonized framework
because I think that's what we really
need a harmonized framework in the EU
that's simple and implementable for AI
compliance and and AI liability and you
know why at the end of the day that will
make it easier for us as compliance
experts and and legal experts but also
AI Scholars to think hard about the
right architecture for AI regulation in
general because then you're going to
have the AI act plus a fully harmonized
a liability regime and then the question
really becomes do we need both in all
sectors or is it the case that we can
really start cutting some red tape in
the AI act and rely more on on liability
in some areas so we might talk about
some of the weaknesses of the AI act and
some of the strengths later on and and
that would be you know an interesting
comparative exercise in looking at what
is the best architecture going forward
to give both consumers a way of recourse
and companies some legal um security and
way for Innovation and some on some
things that you said so first on the pl
the product liabilities directive
regarding psychological harm it said
well I think an interesting aspect so it
it covers medically recognized
psychological harm which will be it's
will be actually be a narrow Target but
you mention mental harm so it's it
should be something I don't know exact
so it was in my mind so frust emotional
What What In this in a spectrum of
emotional harm it's it's really so
mention so discrimination fundamental
right harm is not covered but it was a
big expectation from the liability and
also you mentioned so the AI liability
directive from an AI race perspective
what what do you think it's is smart
smart to to take it out so to withdraw
the liability directive from a compet
compet so to try to make the EU more
competitive if we think so now let's
imagine a company that wants to offer an
AI system in the EU so the legal team of
the company will have to have in mind
the AI act the the legal framework of
each and every member state that they
want to offer the system so beyond
people forget that it's not the AI Act
is one of the law that will be
applicable so the whole legal system of
every country will still be applicable
um so they will also have to have in
mind and and plan for that with
contracts with whatever compliance
mechanisms uh for 27 seven different
liability regimes right so especially
discrimination as I mentioned so
probably many countries will cover
discrim related discrimination not I'm
not familiar with the 27 member states
uh laws on on liability laws some states
will likely cover it properly the system
will absorb the idea of AI related harm
or discrimination from coming from an
system other systems will likely not
cover it right because it's maybe I
don't know because it the cability will
be difficult to prove or the The fact
will be difficult so it's
unclear so that's what I wanted to ask
so from a from a a race perspective if
we want to try to make EU more
competitive this is yes is
counterproductive
no um so yes and no I think there you
have to differentiate between the
substant substance and the narrative
what has gone a I think in the EU is the
narrative and not so much the substance
the I act for example I think is much
better than its reputation because in
many respects it
only
hardwires industry best practices for a
limited set of fairly well defined uh
high-risk use cases and for and for
gener purpose AI um the on the other
hand the narrative that has taken over
is that the AI Act is super strict
stringent that it kills regulation I
mean it does have an effect that's for
sure but I think this is much overblown
in fact it might actually be
intentionally overblown by some actors
uh so as to depress the the value of
European uh companies to make them
easier targets for takeover um the the
fact is that um the same now holds vice
versa with the a the AL withdrawal has a
positive narrative in the sense of uh
hey we're showing in the you that we're
cutting red tape and we're caring about
Innovation we're not regulating anymore
but on the substance it actually makes
life harder for many companies because
it has these 27 different rules now I
see in the chat some questions around
you know uh will liability be covered
for if somebody kills themselves so yes
that's that's death uh uh that is
covered by the plld um but Erica says
wasn't it the the procedural rules that
we might come to you know later on you
have them in the plld and they were also
in the AL what were the two main
mechanisms the two main mechanisms were
one evidence disclosure mechanism if you
have a plausible CL case that you were
injured by the AI then you can ask the
company the actually the developer the
manufacturer to give you the data and
the algorithms so that you can find out
whether that product was actually
defective or whether that product
actually uh whether somebody was at
fault and at the end of the day that
caused your harm that is pretty
far-reaching and we know from the US
that this kind of pre-trial Discovery
can lead to real damage in the sense of
competitors trying to you know get
information uh um all kinds of obnoxious
or toxic uh litigation and those are
concerns that were taken very seriously
and hence there is there will be in the
plld and there was going to be in Al uh
a limit to this disclosure obligation of
course uh when it comes when it pushes
up against Trade Secrets and IP law or
IP rights so for these you have special
procedures you can have special Masters
you can have in camera can have special
settings in the core confidentiality
agreement so on so forth um so that's
one angle and the second is the
presumptions presumptions of
defectiveness actually in the AL was
only presumption of causality mostly it
was a very little presumption of fault
so actually this was not so hard but
actually yeah a lot of industry was also
quite up in arms against that that's
true but at the same time you often have
similar rules already in member states
where if you cannot bring evidence for
example in a product case um then it
might be the case that the company who
engineered manufactured the product has
to bring forward evidence to exonerate
them um so but these rules will differ
they were not as far-reaching mostly I'd
say I don't know the procedur rules 27
member states but mostly they were not
as far reaching as the rules that we now
will have under the plld from 2026 on
and that we were going to have Al but
still I think many companies prefer from
the companies I talk to a regime even if
it's a little stricter that is uniform
across the EU so that they know what to
deal with rather than having this
Patchwork of crazy amounts of you know
from Romania to wherever so
um so that that remains a concern that
the message is supposed to be positive
and you know we're enhancing
competitiveness drag you report all
gears Unleashed for competitiveness in
the EU well it's really in substance
that's not the case and that that of
course is is also a problem with uh
communication on the side of political
decision makers who not always the best
ones to bring across the fact that some
of the rules are actually very sensible
and are not as stringent as their as
their mes look whilst others are perhaps
you know much more invasive even if you
don't really look at it very closely so
I would say for example that for many
companies the product liability
directive will be actually much more
impactful than the AI act why because
the AI act looks at again GPA so if
you're open AI or Microsoft Google yes
you will be covered but those are big
companies they don't have much problem
with this uh except for this being a
nuisance um and and high-risk which also
is something that not too many companies
will fall under but if you PL covers all
Ai and all software and in fact what's
happening and what I think is really
underappreciated is what what what if
there is a harm from a non-h highrisk
product so for those who are not so
familiar with thei terminology what is
high-risk highrisk that would be
something like uh life and health
insurance credit scoring uh anything
relating to employment migration uh law
enforcement and education for example
and medical so those are the main big
points so if you for example just do a
customer management system or you use
generative AI but just for emails or
whatever that's not high risk okay so
many spam filter whatever but still that
can be harm if you do that so for
example Insurance that's not life or
health or whatever um and and what
happens then um if you have uh such a
case the judge will ask well was the
product defective there are no high-risk
rules from thei act but the judge will
still very likely look to the highrisk
rules in the eye act and say well did
you follow some of these kind of these
rules even if this was not high risk
because one of the high-risk rules for
example is article 10 data governance if
you need to have training data and
delegation so on data that is
appropriate for the task at hand that is
representative of your target group
basic a 101 data science but of course
only because this is limited to to
high-risk AI does not mean that you can
do whatever for non high-risk AI
products with your training data and
think that you can get away with it if
there's a harm occurring no this
absolutely not going to be the case so
with the plld that does not
differentiate between high risk and
non-h highrisk the high risk rules in my
view likely will become much more
expansive at least in the lightweight
version and expand to basically all of
AI and that's going to get much plus the
the evidence explosion mechanism and the
reversal of burner proof that in um in
conjunction will be much more impactful
for many companies than the I act uh
plus if you have if yeah sorry yeah on a
point that you said uh so I think Erica
in the audience was asking about
chatbots I think it there's a real case
is one of the first lawsuits involving
liability and chatbot so maybe we can
have a thought experiment here right I I
I love to hear your thoughts on that so
in the US you probably heard of that
case involving the company character AI
so the Florida teenager was chatting
with with a with a impersonating version
of The Nest AR garan from Game of
Thrones and then a the teenager
committed suicide after chatting using
characteri and then it was last year and
his mother now is uh suing so I I read
the initial uh F the the first piece of
the process and then I saw the answer
from Cari of course the lawyers are
denying and sort of it's US law so it's
different but Cari is saying there's no
we have no responsibility it's a
different legal ground there's no AI act
in the US is a different uh legal
framework but let's imagine that
something like that happens in Europe
and it's something and we talk about the
the some of the loopholes of and gaps of
the a let's imagine that is is a system
that was created after August let's
imagine that everything is in place that
the AI Act is applicable and also the
plld is everything the full yeah
framework for liability is applicable so
let's imagine someone is chat using a
chatbot and a conversation if you look
at the the screenshot of the
conversation it it looks like a normal
conversation there are no specific as I
don't know uh uh blacklisted works it's
a it's a it was a normal conversation
but perhaps more uh intense romantic and
the the teenager interpreted in a way
and committed suicide do you think that
it involves death so it it would be
applicable potentially the plld uh but
then we would have still have to prove
defectiveness and causality right you
still there is the article I think it's
article 10 I don't remember the number
of the article that helps with the
presumption the court can the Judge can
declare a presumption of cality but
that's such a challenge in AI so it's a
chatbot and you you visit this chat what
to to role play right to to chat with
this figure that doesn't exist there is
this warning in the interface saying
everything is made up so do you think it
would be effective in that specific in
this potential K there there have been
two deaths already suicides involving
chatbot would the P cover that case so
would defectiveness if you're having a
normal conversation with a chatbot and
because you're dependent you 247 for six
weeks chatting you you commit suicide uh
is it would it considered defective I
don't I I have difficulty understanding
how under defec how would causality and
defectiveness work and if there would be
any different legal ground to to make
the the the company liable what are your
thoughts on that really brainstorming
here on the SP yeah super super good
question I mean there was this case uh
you probably refer to that one as well
there was a Belgian guy in Belgium in
also and that and that actually was
there was not chat gbt there was and not
it was a GPT but it was one with very
few guard rails from what I know and so
that's what it really boils down to I
think at the end of the day of I would
say that uh the providers of chatbots um
so the open Ai and so on of this world
cannot really be made liable for just
any random acts that users make when
they interact with their chat Bots
particularly if there is no
specific uh say output of the chat but
that would in initiate or push toward
such a result um that I think was
different in the Belgian case if I
remember correctly uh the Belgian guy
even said something like I'm thinking
about suicide and then the uh and then
the guy and then the the chat pod
replied some along the lines of yeah I'm
waiting for you in heaven or something
like this so the character uh replied if
you have something like this uh so first
off you need some initial grounds that
would be potentially causal and that
would go beyond what you would expect in
an ordinary conversation that kind of
exhorts whatever kind of damage it could
be physical damage it could be suicide
it could also be you know something else
um and then could also be mental damage
as I said psychological harm and then
second you need to show still
defectiveness defectiveness means that
you need to show that there was a design
defect usually because it's not going to
be construction defect nor a warning
defect now the warning does not absolve
you of curing design defects so you
can't warn yourself out of a bad product
if there were reasonable alternative
designs so what in the end of the day
the company would have to do the the
company was developing the chat but is
document in a real really good fashion
the way they uh designed this chatbot
and the security and safety layers they
installed and this is why for example
this kind of content moderation that is
now often decried you know by uh Free
Speech uh absolutists who then turn
around and you know shut down all kinds
of accounts that uh that criticize them
but that's a different story uh uh so a
lot of movement against uh what they
call censorship which is actually just
making sure that there is uh that you
reduce the probability of harm to an
acceptable degree so I think if you
launch quite uh clearly a an AI chatbot
with minimal or no guard rails with
respect to these kind of you know toxic
behaviors toxic output um and mean both
the interface and the training so both
the fine tuning kind of filters and and
find tuning and also the design the
interface so if you look at the inter if
you probably saw the interface of
character I always comment on that so
it's a remember everything character say
is made up and then it's in red and
black and I say you even have to get
closer to the screen to to to see that
is the most important warning ever so
it's interesting so both conduct studies
and document that choice the design both
the training Choice the the and is super
interesting exactly so the whole product
itself needs to be sufficiently safe so
that you can say there was no reasonable
alternative design that would have
enabled you to to uh develop the chapter
in a way that is almost equally sorry
functional or effective but that would
have prevented that harm from happening
and for example if you like that that
chatbot that was used in uh in the
Belgian case if it was if there was
state-of-the-art um procedures available
for security layers to make sure that
certain filters are applied that with
reinforcement learning with human
feedback other types of reinforcement
learning you can get uh certain um bad
toxic output out of the way um even
without you know the more crude filters
because often times then you can say hey
this is just a theater play and you know
like you can get around that fairly
easily with jailbreaks so if there is
state-of-the R Technologies to prevent
this then generally have to use it if
you don't then it's then it can be
considered an effective product and then
indeed um you can be liable but not for
just
everything um there is an interesting
case uh if I uh may say that um just um
it's it's the autocomplete case in
Germany and I I don't know if everybody
knows about it but um it was about
actually very famous person in Germany
was the wife of the uh German president
so so we have the chancellor but we also
have the president like in Israel like
in many other countries president has a
more formal role uh not like in the US
it's more C ceremonial role but that
wife there were rumors that she was a
prostitute before the uh he met her or
that they might even have met on that
way well I don't want to comment on the
substance of these allegations but the
interesting thing was and I don't know
uh but the interesting thing was that if
you typed in her name the Google auto
complete this in 2012 would actually
suggest prostitute so Betina V
prostitute she didn't like that
understandably and sued and the bad the
German federal court um for for private
law it took up a different case and
there were a couple of similar cases but
it decided this one with that uh
judgment said well you know uh Google is
not universally liable but if they do
not uh engage in appropriate mechanisms
to prevent such harmful output than they
are
and what is autocomplete yes it's a
small language model and so that's a
case that really comes very close I
think to uh to our Modern World um and
fast forward to 2025 you have kind of
similar structures of course it was back
then it was not based on the Transformer
architecture and so on but it doesn't
matter all these technical details uh
what is interesting to note is that in
the Google case in the autocomplete case
you only had Google there was no no
value chain whilst now as you rightly
said you know you have uh um somebody
who's actually training the model and
then you often have different
intermediaries who doing fine tuning and
so on hosting and then the user might
interact with that one too in a certain
way so you would have to Define
responsibilities along the value chain
and that's one of the big big things for
AI liability going forward and in fact
this is one of the things that were
supposed to be in the revised Al but
that's now likely least uh you know push
back we are now in deep compliance water
so we we are we moving out to the I want
to ask you other things about the a but
just before we we we leave this topic uh
so I was reading all the news about the
the AI Summit and I saw I think it was
macron that said that the AI Act was
going to be applied in a business
friendly way and I was so in what does
that mean so it's supposed to be the
opposite right so business business are
going to work to behave like businesses
so they will try to not comply as much
as possible but still put the budget in
it it's like to build a budget for that
that fine if the fine comes but what
does that mean what do you think so what
does it mean to apply the a in a
business friendly way well maybe it
means asking for businesses to help him
generate better deep fakes of himself
and then promote those videos online now
just no kid did you see these deep F
absolutely horrendous no what I think um
what he means by it um are two things so
one you know that France was one of the
few countries who really
fought the whole a and particularly
against the GPI uh chapter whil now AR
men the the um the um CEO of Mistral
actually says you know it's quite
workable which you know come on it has
always been and and those those rules in
GPI chapter are absolutely laughable I
mean it's just absolutely
except for some things in article 55 but
the rest like is really it's and even in
article 55 is pure Baseline this is what
every responsible GPA company is doing
and should be doing with perhaps the
exception of um some of these companies
that belong to the portfolio of the
richest man on the planet now um the
what does it mean to be business
friendly I think means two things it
means particularly to make uh to get out
guidelines Fast and Furious so that
people actually know what to expect so
they have some more legal certainty and
the ey office certainly working on it
and second um I'm talking quite a bit
also with people from supervisory
authorities in in in Germany also in the
EU and I think what uh what often is a
misconception by politicians is that
they think if they allocate less
resources to the supervisory Authority
that's business friendly because then
when they're underst staff they can't do
as much that's entirely wrong what they
what they are doing often is to give
advice to companies companies come to
them and say hey we have this case kind
of a bit weird can you tell us this is
okay or not can you kind of greenl it
write no action letter or something like
this they do this of course on only if
they have enough resources because
otherwise they really have to stick to
you know what they really have to do by
the law which is you know enforce it so
I think it primarily I'm not sure that M
meant this but you know what would be
good is to incentivize
National agencies the national
supervisory agencies to be staffed in a
way so they can actually proactively
engage with the business community and
tell them look this is how it works this
is not how it works you got to take the
serious but at the same time if you have
a good compliance system in place you
are at least in most cases fine and you
can do it and you can use it you know H
so it's a lot about the spirit and the
vibe and but it's also be also about
being available and um then of course
National I mean national uh
implementation doesn't have too much
actually too much leeway so it's about
also about um uh about sand boxes but
personally I'm not a big fan of sand I
don't think they're going to be I was
going to ask you exactly that if you I
thought it looks like you're optimistic
about regulatory sand boxes so that
there is this direct contact between the
company and the authority yeah but it's
not a so this is more this is more an
advice session like uh what they often
do is like a a check-in session like
once a month or so where people can
really like a Q&A session I think these
sandboxes often it's much more
formalistic and I think that a company
that really needs a Sandbox is unlikely
to succeed anyways because investors
I've talked to a number of really
successful startup entrepreneurs about
this none of them have been in a sandbox
uh they've all been you know they've
been in white combinator and whatever so
that's a big quality sign you go to a
Sandbox off a negative sign because it
shows that you can only uh survive in
this kind of kindergarten sphere and it
makes it difficult to scale out of this
now if it's realistic conditions it
might be different but um but I'm not so
sure that this is actually going to work
very well I think for a couple of
scenarios probably and also to just get
people on board and have them try out
things that otherwise they wouldn't have
tried out but I think like for the real
serious entrepreneurs I'm not really
sure that they're going to need this um
so yeah I'm not I don't think it's gonna
be such a big thing but okay um now I I
want to hear some of your thoughts on
the AI act in general before we move to
transparency and uh bias so first on the
AI act in your opinion what are the the
the biggest weaknesses of the a where uh
the EU lawmaker could have done much
better yeah I think the overall the
architecture uh sets the wrong Focus um
I think um it's great that we have the
prohibited practices so for example
nobody want I mean very few people and
hopefully nobody in their right Minds
ones like a Chinese style super
surveillance scheme in the EU neither
with facial recognition nor with credit
scoring uh sorry social scoring uh there
is of course a loophole in uh in expost
uh facial recognition in the public
sphere so remote biometric
identification uh that could have been
closed because we have the number of
countries that are in Democratic
backsliding and you cannot trust them
really um and quite frankly even you
know like uh who knows who's going to be
in government five years from now so um
there I would have liked to see even
more robust uh minimum safeguards
against uh government interference with
with privacy and particularly with the
uh use and exercise of political rights
such as in demonstrations and stuff so
that's one uh weakness but it's kind of
a bit of a minor one even though it's
fairly significant but um I think
overall it's great to have the uh
prohibition is great to have
transparency it's great to have GPI
rules gtpi rules I think are much too
weak and the threshold is just much too
high uh we're going to see if maybe the
commission picks up deep seek as a first
case to either lower the threshold or
designate a model as systemic risk
because as some of the call might know
the Threshold at the moment is 10 to the
20 to the power of 25 flops and deep
seek R1 is likely below this um however
it has capabilities that that are
comparable to uh 10 to 25 flop and and
above models uh just because they
there's an x13 right Philip also for me
NX 13 will certainly NX 13 with the with
the condition so lips will certainly fit
one of those condition exactly so they
can designate it but it's below the
threshold where it would presumptively
be considered a a um systemically risky
model um again and so I think those
rules are they're it's good to have them
they could have been more robust in the
GPI side I think well with external red
teaming obligations for example um on
the other hand the highrisk rules I
think could have been a little less
prescriptive giving more leeway to
companies because for example if you
look at bias in training data we're
going to speak about bias later but this
is an incredibly complex topic nobody
even knows what that precisely means
under data governance rules article 10
plus you have post-processing uh
fairness mechanism so it might be the
case that the most efficient way to get
rid of a bias is not to change the
training data which also by the way my
direct performance but to do it
post-processing I've actually designed
two post processing um algorithms myself
with ML colleagues and the good thing
about postprocessing and postprocessing
means that you run the ml pipeline once
you got the final result the score then
you change that score and map that the
score distribution onto a fair
distribution in intelligent ways of
doing this with optimal transfer and so
on so that might actually be the most
efficient one because you can you
exactly know what the outcome is going
to be if you just mess around with the
training data oftentimes you don't
really know what the outcome is going to
be anyways H now you have a problem
because if you don't fix article 10 you
might be violating the I act even a
high-risk rule incurring crazy fines
whilst at the end of the day there is no
problem because you're actually fixing
it later in the ml pipeline so this just
one example of where I'm thinking
um the high-risk rules are really in
need of some revision um what perhaps
they're giving companies a little bit
more leeway particularly if we couple
this with the robust liability system
that says hey and if something goes
wrong then people can actually sue the
hell out of you and this is actually the
much greater incentive because for many
of particularly the smaller companies
they are VC funded so it means if they
have a case brought against them they're
basically killed
so this is a super big incentive because
the VCS will just pull the money or they
won't get the next funding round if they
have one two three not totally non-
mandator claims against them and so I
think this is where we can again see the
uh see the interplay between product
safety and product liability um so those
are so I think overall we might revisit
some of the high-risk cases while at the
same time um I think we should uh we
should make sure to really look at the
big guys to have the GPA rules in place
and structured uh in an even more robust
way to keep transparency to creep
prohibited practices so I don't think
anybody in their right mind would really
scrap the I altogether um and I don't
think that's going to happen frankly but
those are some of the things where where
we might see revisions going forward now
in the new fitness check of the
commission and the simplification
strategy and and I like that you
mentioned both the prohibited and the
highrisk unprohibited I agree with you I
like this is one of the few Provisions
that I think is actually good the social
credit score is written in a way that is
clear so we the the the legal technique
and the exceptions and and the way it's
written is good but if you look at the
first one the the subliminal techniques
it's so so bad the the language is so
narrow so at least that's more symbolic
it's so yeah so we understand that
they're talking about dark pattern so my
PhD research deals with that so I'm
really enthusiastic about that but then
and it's significant harm is and so many
it's written in a word in in with the
language that is it ends up very very
narrow and then the interpretative rule
is also narrow so I I I read that and I
think wow I can't imagine any existing
system that will fit this very very
narrow uh description or description so
I I 100% agree with you and I I said I
write that many times publicly that
whoever is criticizing saying that the E
is horrible is over regulate they
haven't read they haven't maybe they are
not legal they don't have a legal
background maybe they they read
something on the news if you read the
provision it's really is like common
sense but but you you you you said
something uh about the the high risk I
want I want to to bring uh I want to
hear your thoughts so in the high risk
when when you look at article six so for
those in the audience article six are
the rules for classification the two
types of uh groups that will be
classified as highrisk so we have anex
one and anex three basically and there
is a m so article 6.3 says that even if
you are in Annex 3 so that the areas
that Philip was mentioning so education
uh employment and all all the the the
sensitive areas anex three you can still
if your AI system falls into one of the
four conditions you actually so if they
don't pose a risk ACC to fundamental
rights according to and you can show
that with very broad conditions then
it's not highrisk and and in the AI act
in the context of the AI act the
highrisk category is
after the prohibit okay the worst
category the one that nobody wants to be
in is the prohibited practice nobody
wants to be covered by Article Five so
okay let's take out the prohibited a PR
but from article six to article 49 of
the AI act it's all about high risk is
the most important category everything
that everybody when people complain
about a act they are complaining about
highrisk rules the rest is basically
article 50 and minimal risk and if you
have an article six that allow even if
you're covered by anex 3 you can with
very broad conditions say okay I
actually don't pul risk so I read that
and I I I see as with the lawyer's head
I think wow that's extremely alarming
and I already see I mentioned that in a
previous live talk so I see some law
firms advertising like this so if you're
if you are developing AI system an extr
don't worry come here you'll sit you'll
make sure that it actually doesn't POS a
risk to fundamental rights because the
conditions they are so broad they're so
open to design you know it's they say
okay there is it's a process where there
is a human intervention before okay then
the legal team will sit with the
developing team and we make sure that
there is a human process before so I I
keep imagining all those types of
scenarios that will likely be exploited
and I want to hear your thoughts so is
that on purpose was that uh by mistake
or do you agree with me do you think it
actually good that there is some leeway
to for companies to to prove that they
are not high risk what are your thoughts
on that um yeah I think this is an
extremely sensitive Manner and I think
this is one of the provisions where we
also uh that definitely are in for a
revision or at least guidelines uh first
off I mean the cju has said a number of
times um that uh these kind of
exceptions they need to be interpreted
narrowly uh this is I think logically
entirely wrong why would an exception be
I mean you can phrase many things
positively negatively I think this is
there's no argument for that but it's a
fact and so it will be inter interpreted
rather narrowly and also um I agree that
some of the provisions like you know you
had a human assessment before for it's
only kind of reproduced or augmented by
AI it's kind of a bit weird what the
heck is that supposed to mean but I
think for many of the use cases where we
would agree that this is really uh
within the core of anx 3 within the core
of fundamental rights impact you would
not have a way of getting out of this um
I'm I would be a little bit more
optimistic about it in fact I've worked
with companies uh from the highrisk
sector where we've sat together with the
compliance team and the compliance team
and I were both saying you know no no
way you get out there's no way you get
out of this even if they would have
liked to of course but no no way and
plus the there's two things uh one is uh
you still as you know you still have to
register your non-h highrisk AI system
in the EU database if you if you take
that article 63 loophole so basically it
will still be known and you have to list
the reasons of why you think that is the
case and second the uh the the there's a
there's a very pronounced risk because
if you if you get out that you want to
get out of the loophole and you
basically don't do any of the compliant
stuff that high-risk systems otherwise
are required to do and then a court or
regulator finds that actually you are
high-risk then you're really screwed
it's a bit like I I tend to think it
might be a little bit similar to um to
the concept of personal data in the gdpr
where also uh it might be in a number of
cases that this is not really personal
data because you know the the the way in
which reidentification might work it's
not really likely or whatever but most
companies that I know still assume that
it's personal data because they would be
super screwed if basically they assumed
it was not and they wouldn't cover and
they wouldn't apply by comply with the
GPR so I think that for many cases going
to be similar there say if you're uh and
if you're doing anything with employment
based you know uh decision making with
AI as soon as there is a a a substantive
decision or a substantive recommendation
by the AI system I think there's no way
can argue yourself out of uh this with
article 63 I hope the cju is not going
to prove me wrong on this and and I'm
very uh I'm I'm happy to discuss this
further and I think it certainly needs
more Contour but I think that generally
speaking it's it's it's right to have
some kind of leeway there because if you
think about you
know uh uh an system just doing the
scheduling for the interviews that
really isn't something where you would
say that's that has a lot of you know
highrisk potential so um that would be
something where the classic admin task
or just you know uh puts document in in
some kind of folder or something even if
it's related to employment that is not
something where I would think the
high-risk uh rules would be justified
and and thus I think there is I think if
we can narrow it down to these really
ancillary administrative task and maybe
take out some of these um bullet points
that are more sketchy and strange uh and
then I'd be happy to keep it but in that
revised format now I ask you because I
know you are in the EU bubble you are in
touch you're there the speaking with
officials so I know you have a more
Insider view so I I don't know my
viewers from the outside I I I I see the
the w law firms are are advertising this
service and they like wow
okay just come here we'll make sure that
you're not highrisk like you you you you
we will get you out of the highrisk
category don't don't worry so I would
like to see how they do it I think
that's probably just uh just a bait but
uh well fair enough yeah the second one
that I I really want to hear your
thoughts I think it's a bit I don't see
many people talking about this one and
maybe it's a major thing so regarding
the timeline so most people there is the
the the a act entered into Force but
it's becoming enforceable in there is
this timeline so the first batch of
Provisions became enforceable on in
February so from article one to Article
Five and there will be another milestone
in August 2025 and the rest of the a act
in August 2026 but there are some
Provisions that will become enforceable
later and article 111 and many many of
you in the audience maybe you haven't
read it you should check it it says that
for AI systems already in the market so
everything that we have now the AI the
AI act will be applicable only in case
of substantial modifications meaning the
langu the exact language is significant
changes in their design so what is that
Philip help me interpret that so what is
is this what would me a give me an can
you think of an example of a significant
change and when you read the recal it it
refers to substantial modifications and
my in my head probably and I hope the AI
office will do that they should issue or
the board later on some sort of
guideline that will say that something
very small is a maybe uh I don't know
system update something small so that
everything that exists is will be under
the a act otherwise we will see before
August 2026 in July 2026 we'll see a
boom right a major start so many people
launching starta systems on July in July
to avoid
totally I'm so happy you're bringing
this up Louisa because I think it's one
of the most
overlooked uh Bombshells of the ACT
actually when I first saw this I was
asked to comment on some previous
versions of this by national and EU um
persons and I basically WR are are you
serious like this this must be a
joke like really uh and they kept uh
which by the way is also a gigantic
competit and undeserved competitive
Advantage for uh incumbents uh because
if you are a new company then you're
fully responsible on the a whilst if
you're an incumbent then you basically
get this free this free pass and it's in
no way compatible with product safety
law because of course it does not matter
at all for product safety If the product
had been on the market before or not if
anything uh an existing product should
be regulated even more tightly because
you know it becomes obsolete U uh more
probably um sooner and so on so forth so
uh this is really crazy I think and um
indeed the the the problem is that we
have several uh Clauses several
Provisions about modifications
substantive modifications um we article
25 of course where you change from a um
mere deployer to provider if there is a
substantive modification in a high-risk
AI system um you have that so this this
uh and we have a there is a definition
of a substantive modification article
three uh basically says if you change
the risk profile um and there is
something that I would that I would
encourage people to look at also
basically I guess um what you could
there are two interpretations in My Mind
One one is um almost everything counts
as a substantive modification and
because even if you for example
fine-tune a system on a few data points
that could theoretically change the
entire tensor the entire V so all of the
weights and biases of the system and and
it could behave entirely differently and
you would have to test to see if it
that's the case or not um there's a
second interpretation that says hey here
we have a modification in the design
which is something else which is higher
level so it would be like a change from
Deep seek V3 to R1 so or take GPT 40 to
01 like from this from the normal to the
reasoning model um which is of course
not a reasoning model it's just
calculating more um but um something
like this so it would have to be uh more
in the way the product functions which
also then has to display a different uh
uh risk profile so long story short I
think the only thing that safely excl
where you could say that this is really
not a significant modifications if you
just put a different if you put a filter
on it for example a new filter that is
even better at detecting say harmful
output then I would say that's
post-processing filter that would not in
my view count as a substantive
modification because if anything uh it
would you know lower the risk profile
and it's only it it keeps the entire
engine before the same so the same um
there's no unexpected cases that would
come out of this that are new Visa the
previous product um but everything else
is really up for for for
grabs and the problem is that from a
from from a product safety and and a
general I think Common Sense perspective
would like to see this provision uh
being interpreted very broadly where
almost everything counts as a
modification whilst in article 25 one I
think there's much better Arguments for
having for being
more nuanced with respect to you know
whether somebody who's doing just some
minor fine-tuning on a GPI model um with
systemic risk will then also become a
provider that you know has to fulfill
article 55 which really is a major
problem if you're if you even if you are
quite a comp competent and big uh
company but you're not you do not happen
to be open AI or or Google and do you
think that what can you stay 10 more
minutes pH so that we we have a little
bit more or you're in a hurry H I think
yeah we can take like five more minutes
or so but then I think I have so I'll
skip to the last one so I I'll leave my
my I'll skip to bias and fairness uh so
regard I know you have so many art I had
actually a list of questions but I I'll
focus on this one um so when we Jesus we
already spoken for an hour all right
yeah I know right we I I had I I'm
already skipping questions I know we we
have we can do a replay at one point
okay exactly um so when we look at the a
when we look at the gdpr and I know you
also read many many times the gdpr and
the AI act the gdpr has a very important
I don't know in my research also I
looked into it and I think it's it's
fascinating also from an interpretive
point of view so we have Article Five we
have the principles and the data sub
rights so we have uh data data
protection principles and we have data
St rights and we have seen those in play
at play in many uh cases in the
Enforcement cases in the EU so fairness
or data service rights or so many
applications but I say here talk about
principles the AI act does not have that
we not we do not have an article uh so
we have trust for fundamental right the
the general right the subject matter
article so respecting fundamental rights
trustworthy AI but we do not have like
Article Five a list of central
principles also we do not have so we
have a very few mentions of affected
persons but we do not have like a
statement you know if you're we have a
okay the the part on enforcement you can
file a complaint but really light or
some clarifications but we do not have
like data s r in the same way that we
have in the gdpr uh with with a fairness
with with with sorry with bias and
fairness in mind do you think that it's
it's a flaw that should the the EU
lawmaker have thought about that and
when we're thinking about bias and in my
view maybe you have probably much more
to say about that is very is dealt by
dealt with by the ax in a very narrow
way in a very not only in a few St could
be much broader uh so do you think that
this is first regarding the like No
statement of principles and rights is
this a flaw or is this by Design is this
that that's the best way and also how do
you think bias could have been dealt
with in a more in a stronger Way by day
so this is our our last question and
then we we we finish uh that's a great
question so the European Parliament you
know actually in its position had some
principles in uh in it they were not
adopted at the end
um I would have to say that I would be
skeptical that the principles search
principles would actually do much work
because you have such a prescriptive
system already in the gdpr it's a little
less prescriptive um there are less
rules and hence there's more work being
done by the at least recently in some of
the recent cases uh by the principles I
think in the I act it might have been
even a little bit Superfluous so I'm not
sure that that would have help helped
much I think what it would have helped
much more is and that uh brings us back
to the beginning uh makes us full circle
is some of the rights that were
contemplated in the AL because what you
need as a potential victim of uh of
algorithmic discrimination is first and
foremost to know if you've been
discriminated against or not now if you
are want to go into a nightclub you know
I'm from Berlin so uh many wonderful
nightclubs uh and you get get rejected
um you know some friends of mine who are
black sometimes they get rejected and
they say this is racist but you never
know you cannot unscrew the head of
bouncer right um now that's different
with AI systems of course it's a big
plus basically that you can't actually
look under the hood but you need to be
able to do that so the uh cju in I think
2012 or so issued the Meister ruling and
in Meister it said well only because you
suspect that you were discriminated
against it does not give you right of
access um to any of the data for example
of other you know even in in in
anatomized way or whatever synonymized
way the files of the candidate that was
successful and so on um so the first and
biggest question is was I discriminated
against at all or not or was it just m
coincidence or whatever and hence that's
what you need this evidence of closure
mechanism for and that's exactly what
the AL would have done uh particularly
for nondiscrimination so so I think
doesn't matter way put it um whether you
put this in the um perhaps it's
systematically better positioned in an
Al style package than in the ACT which
is more product safety regulation you
now have these small rights but it's not
really big thing um but um like to
explanation and so on but only to the
extent that it's not covered for in
other acts on the GPR right is actually
now with the with the recent CK or dun
PR for ruling it's it's been much
bolstered really happy to see that
um so long story short yes I think
should have been much more um much more
uh aggressively addressed uh this
problem but not necessarily in the iak
but more its enforcement problem for the
framework that we already have there is
already a lot of EU law on
non-discrimination and it's an
enforcement problem there so we need
either an enforcement directive there or
a more General AI liability kind of
enforcement scheme which some of the AL
would have provided that something that
we need to come up with uh with a with a
solution or a placement for that going
forward and um it's going to be exciting
to see how this plays out and how the
all architecture plays out um going
forward to integrate at the same time
the different types of digital regimes
that we have particularly also the gdpr
and you've likely seen many of the
people in the audience loser of course
knows it that we have uh these uh uh the
kind of refocusing of the gdpr regime
particularly for smmes particularly
maybe with a different layers and so on
have a a full conversation only about
that Philip so I oh yeah can it was on
my list I took it out just because to
FOC otherwise we would stay here four
hours but we have to have another one
about that and I know Philip thank you I
have to go now thank you so much I the
audience will likely so for those asking
it's going the recording will be in my
YouTube channel so in a few maybe next
week I I will upload so probably people
are are going to have to watch it again
so you you you shared so uh much
valuable information so people are going
to have to rewatch Philip where can
people find you where are you usually
posting or sharing or maybe uh I think
mostly on LinkedIn uh that's where you
can find me probably yeah okay so
everybody on LinkedIn and they can hire
you also right you're available for hire
no yeah yeah yeah I do consultancy so
you can reach out by email yeah you find
my email address Philip help you uh also
for thank you so much uh everyone for
joining for being all always uh so
participative and asking questions so
don't forget to subscribe to stay up to
date with everything AI govern subscribe
to my newsletter and if you want to join
the training in May so so save your spot
so it will the April one is already sold
out that's it thank you philli so much
it was a pleasure I hope we have another
one I think we're going to have to
schedule another one to finish I think
so too would be a pleasure take care
everyone thanks so much for the great
audience thank you bye bye
