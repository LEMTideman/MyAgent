today I'm here with Barry scannel
welcome Barry to our live talk today so
Barry is a partner at William fry today
our topic is AI governance compliance
and regulations let's talk about some of
the unspoken challenges behind the AIA
we see so many people posting about it
and writing articles but there not
everything uh is spoken about it so what
do you think are some issues behind the
a that people are not talking about or
some some Maybe unpopular things that
you usually don't share so some unspoken
challenges behind the AI act I mean the
biggest one by mile is the um issue of
compliance costs um so when the EU Comm
when European commission did their
impact assessment for the AI act so they
the proposal for the AI Act was
published in 2021 the work in the impact
assessment started in 2017 2018 and in
the impact assessment they try and
figure out one of the things they try
and figure out is you know how many
businesses in the EU will be affected by
this how many AI systems will be
affected by this and what will it cost
businesses to be able to comply with
this so to put things into perspective
when the EU commission when European
commission has started this exercise of
of doing these impact
assessments to Transformer this the
technology underlying the famous T and
GPT that we keep hearing about it was
only written about for the first time in
a research paper in 2017 it was only
deployed for the first time in 2018 so
when the European commission has started
doing this the technology that has given
rise to the big explosion in generative
AI the technology didn't exist so it's
not really the European commission's
fault
um so I think their estimates on
compliance costs was in a very literal
sense off by an order of magnitudes I
think it was missing a zero um instead
of costing maybe 50,000 could cost
organiz some organizations that are
really deep into the Weeds on this
500,000 um so most of the work were like
well actually all of the work we're
doing so it might sound a bit crazy but
um my practice is an AI practice my work
is almost entirely AI law so obviously
you know I CH chip in and help about the
the Technology Group it's like with
normal technology queries that come in
in relation to copyrights in relation to
transactions involving technology and
software license negotiations things
like that but predominantly my practice
is an AI practice and the type of
clients do we have in our AI practice
are major Tech multinationals major
social media platforms the big techs and
from an Irish perspective they're Irish
PLC so they're some of the largest
companies in Ireland and that's who's
looking at AI at the moment they've got
just two reasons one they can afford it
and two the bigger the company the more
likely it is that they've got a very
Advanced Network um and implementation
stage um of of AI I don't know how smmes
smaller organizations are going to be
able to comply particularly startups uh
when it comes to AI now there are rules
around you know lesser five
for for smaller companies there's rules
around regulatory sandboxes there are
measures within the AI act to help
startups and smaller organizations I'm
not sure the extent to which um those
will actually help so I think the
biggest unspoken issue by by a long long
way is that if you're a small to medium
organization um the compliance costs are
going to be
astronomical and you know for some
companies we we already seeing some
organizations say from a for data
protection reasons as opposed to AI act
reasons but like let's say meta's roll
out of their multimodal elements um is
is one example Apple's roll out of their
multimodal AI systems they're pausing
that on a European basis because of gdpr
concerns and I think for a lot of
smaller organizations that might be RIT
large because those smaller
organizations might figure it's actually
economically more viable to just avoid
the European market and actually have to
go through the product regulatory
process to get the C Mark to be able to
provide these products onto the European
market um so I think you know there's
just going to be a gap between bigger
companies and smaller companies and
potentially a gap between products that
are rolled out within the EU and not but
what I would say in relations to to the
EU let's say what Matter's decision not
to roll out their AI in Europe for the
time being so some people might see and
it's a very typical thing you see where
Americans and Europeans the Americans go
you guys regulate too much and the
Europeans like me saying yes but we've
got rights um you know the the flip side
of the coin is that our data our
European citizens data is not being used
to train um meta's AI systems and you
know as a European citizen as somebody
concerned about privacy I see that as a
win so there it's it's there's there's
pros and cons to to each side of it so
uh yeah I think that's the the biggest
unspoken kind of obstacle for now but
you were mentioning that probably small
and medium Enterprises will will have
more difficulty with in terms of cost
also right and bureaucracy and
documentation do you think it it will be
much worse if the company's developing a
high-risk AI system so if the company's
business model isied as high- risk or
from your practice what you're seeing
now it will be as well other obligations
will also be expensive so it doesn't
matter what do you think yeah so like I
mean the if if your system falls into
high risk then you get you know as we'd
say in Ireland both barrels of the
regulatory
obligations um you basically would have
all of the compliance elements So Not
only would you have to kind of risk
avoidance and risk mitigation elements
but also you'd have the active
obligations which would involve you know
um the regulatory elements of getting
the C
um which is the the for for people who
wouldn't know if you want to make a
product available in Europe um it has to
have it's a safety label really I'm not
sure what the US version is I'm sure
there is a US version but basically that
the product um you know let's say if it
was a child's toe it's it's going to
have a c Mark so you have a certain
level of comfort that it's not going to
explode in your child's face um so
high-risk AI will need to have to C
Market will need to have product
regulation and and everything like that
um but here's the problem to be able to
like it's it's easy if you know like
like let's say with high-risk AI systems
uh there's two types of high-risk AI
systems there's Annex one at highrisk
and Annex 3 high risk so Annex 3
high-risk AI systems are coming in and
the um 2nd of August
2026 anex 3 IR risk AI systems are are
things like AI that are used in the um
HR context so recruitment performance
monitoring um profiling AI That's used
in profiling biometric
identification um access to essential
services like um health insurance life
insurance um that's all going to be
considered high risk so if you're in an
insurance computer let's take the HR
context if you're using an off-the-shelf
HR software that might have ai elements
in it and as a user you may have ai act
obligations certainly if you're
providing this um software into the EU
Market you need to cover off well is
this high risk because you need to be
telling your your customers well this is
high risk and you know we you have to be
able to show you have to give warranties
presumably in your contracts you have to
be able to comply with the European law
because if you're providing it onto the
European market you have to get to see E
mark you have to comply
so to get to that point though to know
whether or not is high risk that in and
of itself is not a small process so what
we're currently advising our clients on
um in terms of what steps to take step
one is always carry out an AI inventory
an inventory of all the AI systems
either being developed or developed by
your organization or being used by your
organization and then once you've got
that inventory what you have to do is
you have to go through the half ation
exercise so pursu to the AI act what
type of an AI system is it is it a
general purpose AI system is it a
high-risk AI system if it's high-risk is
it Annex one or Annex 3 high- risk so
Annex one high-risk by the way those
rules are coming in in the 2nd of August
2027 and those are AI systems which are
safety components or which themselves
are actual products which are regulated
by specific European regulations such as
the Machinery regulation the medical
devices regulation things like that so
specific products that are covered by
existing European product
regulations so you already have to
undergo that exercise and that in it of
itself is an expensive exercise that
would cost tens of thousands of Euro
Euro if you are any type of sizable
organization um because you're going to
have to get lawyers to work with your uh
Tech teams to ident to do that
categorization exercise and then where
you identify any AI systems which let's
say let's say if there's not high risk
let's say if it's general purpose Ai and
rules and general purpose AI systems are
coming in on the 2nd of August of next
year and those are you know Foundation
models basically like chat GPT co-pilot
probably going to be caught by it um
certainly if you're doing fine
tuning you're for the the elements that
have been fine-tuned you're actually
potentially going to be the provider of
an AI model so if you're taking an open
source model and you're applying
finetuning with your own data to the
extent that the model exists as
fine-tuned on your data you're probably
going to be considered the provider of a
general purpose AI model and then as
that model forms part of a general
purpose AI system you'll be the provider
of the system as well presumably so like
there's all of these different elements
and then so is this a general purpose AI
system are you a provider are you a user
and then there's rules that say like if
you have a general purpose AI system
that's you know totally safe and fine to
use and if you make a tweak to it so
let's say you have an AI system in a
school and the AI system is used to
correct students essays and you're
telling one of your colleagues in the
staff room um about it and this
colleague Works in HR and they go hey
I'm going to use that that sounds like a
really good system because if you can
grade essays about how good they are
Wonder Could It grade CVS and then you
you are the HR person and you use that
software and say rank all of these CVS
sorry for my American friends resum√©s um
from best to worst and it can do it and
it can do a very good job of it order
the AI act that's high risk and that use
is high risk so you have a general
purpose AI system that wasn't high-risk
that is now because you've changed the
intended purpose of it has become high-
risk and under the AI act that would
make you the school in this instance the
provider of that so that's why companies
need that are providing it need to say
this is what the intended purposes are
this is the acceptable use policy if you
stray Beyond it it's all on you and
that's why you as a user of this
technology have to be Crystal Clear
yourself and particularly with your
stuff to ensure that you know exactly
what you can and can't use it for
because then you could get the full belt
of the AI act
inadvertently um and that's why you know
training is going to become so important
and I'm sure we'll get to that Louisa
but it's it's strange for a piece of
legislation to say it but article four
of the AI Act is specific it says it
deals with AI literacy and it makes it a
legal obligation for companies to ensure
that their staff and people people using
AI systems have a sufficient level
depending on the roles of AI
literacy this basically is a legal
obligation to train your staff and I
think it makes a massive amount of sense
because you're going to be creating AI
policies procedures first of all you
need to train your staff on these new
policies and procedures but you're going
to have to tell your staff the legal
people are going to have to write the
Rules of Engagement so a friend of mine
was in the Irish Army he's a lawyer and
what the lawyers end up doing at least
in the Irish Army I don't know what it's
like elsewhere in the world the orders
come in and then um it's up to the
lawyers um that are deployed to put the
the orders into the Rules of Engagement
so that's basically what the role of
lawyers is now going to be when it comes
to things like acceptable use policies
from AI systems is that you're going to
see the acceptable use policy and it's
up to the lawyers to put that into
language to all the staff within the
company realizes this is what you can
and can't do and training is going to be
a key part of that so you've already
anticipated some of our other questions
so what I'm happy you said right in the
beginning so I think some people in the
audience are probably nervous because
they didn't think they were going they
could potentially be high risk or uh
providers of general purpose models and
so on and I think also I have some
unspoken challenges to to talk about
also but on what you just said also
their geographical scope right so so
many people they say I'm not in the you
so I'm out and then they will realize
that actually they generate their AI
system generate effects in the use so
they will also be involved and be uh be
covered by the AI act so it's
interesting that even if you're not in
the EU you might also be covered so
very like at that point Louis the AI Act
is more extr territorial than a lot of
international treaties there is a lot of
international treaties that are a lot
less International than the AI acst and
that's because for all the Americans
that keep saying that's your problem the
you guys you just know how to regulate
so we say wait wait a minute maybe it's
also your problem ju just before we
continue so some some of the things so I
think we're going to discuss it in other
uh questions as well but I think I I
would say more generally but there are
many exceptions that make me worried so
we I can say some examples for example
the the article that allows companies to
apply for not being highrisk so if you
are if you're within Annex tree you can
still uh there there are mechanisms for
you to say I'm not despite being in
Annex tree I want to say that I'm not
highrisk so I think that exception is
too broad especially because the the the
paragraphs of this article that enables
companies to to avoid this
classifications they are very Broad and
I I can see many lawyers focusing on
that and I think it's uh probably so the
result of the at least the legislative
technique used in this article I think
as a result there will be less companies
classified as high risk than originally
planned that that's how I I read the
exceptions I think this is one see the
the interesting thing about that Louisa
is that so there's a number of
derogations that can apply to high-risk
system so not only must you fall within
Annex 3 the different categories but
also there's these derogations basically
not only must you fall in high risk
Annex three but also so you must decide
is this actually high risk now the thing
is if you decide as a company okay we're
not actually highrisk you actually still
have a regulatory obligation you have to
register your decision that it is not
high risk with the relevant body in the
EU so there's still paperwork to
complete there's still a paper trail you
need and there's still an AIT trail that
you'll need and you know from everything
from insurance and like getting legal if
you consider your warranties indemnities
liability clauses
if you as an organization decide oh this
isn't high risk and then you deploy it
and you've assured everybody all your
customers this isn't high risk like the
scope for problems is huge it's not just
limited to a regulatory problem it's
it's it's endemic it can metastasize
basically because then let's say if
you're wrong or if if the The Regulators
disagree with you not only then are you
in default of the the regulations but
all your cust customers are as well
because they've been assured by you that
it's not highrisk so so this is a big
issue as well and what's they've started
in the EU so there's been recent
elections within the EU um I think
broadly speaking and you probably agree
Louis say not much has changed in terms
of the European Parliament there hasn't
been a massive shift one way or the
other I think it's more or less the same
as what we' had more involved than I
with European politics but I I I agree
with I will go with you on this Mary um
but they've already started talking
about the legislative um proposals for
this legislative session and one of the
things they picking up again is a piece
of legislation called the AI liability
directive now I mean if you're
considering whether or not you're high
risk that's one thing with the AI act
but the AI liability directive makes it
way easier to get sued in relation to AI
so it's introducing this thing called um
it's a rotable presumption but a
presumption of causality so you know
we're all many of us anyway are lawyers
here about you know typical Tor law
especially in common law countries you
have um a duty of care if you bre that
duty of care then you have to link that
causally to the injury and then you get
your damages well with the AI liability
directive all you have to show is that
there was a duty of care and that was
breached and the court can then infer
that it was the AI act the ai's fault
and you don't have to prove that CA
causal element
so one of the breaches of Duty of care
that it's introducing is um breaches
with the AI act so if you breach any of
your obligations under the AI act and
that could be a breach of something like
AI literacy that would be actionable
under the AI liability directive now if
you consider from an IP perspective a
lot of the IP claims in the United
States in particular against AI
companies are failing um because they're
saying oh you use my copyright and the
AI compan comp are going well our
training that is a secret so you can't
tell that for sure and like it's it's
true they can't tell it for sure even
though it might be quite obvious you
need to be able to prove that well the
AI liability directive actually allows
for Discovery or a type of Discovery
disclosure of data and information at a
way earlier stage so you'll have your
traditional legal Discovery but the AI
liability directive allows for
disclosure in relation to specific
elements at a much much earlier stage
when proceedings are issued to allow you
to be able to maintain your claim so
we're not just dealing with regulatory
hazards here but there's also um a lot
of unseen um liability Minds uh that are
concealed as well very interesting that
you brought the AI ability and just on
on the topic of high risk so if you're
high risk or you think you're not high
risk so make sure to hire Barry then you
don't run into problems later on um so
let's let's continue we have so many
questions for today and and um for
example what when you you mentioned a
little bit about small and medium
companies so many startups and
entrepreneurs they they want to do
things right and they are worried with
the AI act so what do you think in terms
especially from your perspective as an
AI lawyer how what what they should do
first so let's see we have some people
here in the audience that are
entrepreneurs and maybe and there are
two two types of entrepreneurs right
those developing AI models or AI systems
those really focus on AI and those that
we use more more I would say tangent
tangentially use AI so what how they
should start from if you were giving
this General tips so what's the first
step well I step one actually is assess
your risk
appetite um if you've got an enormous
appetite for risk you might decide h i
don't have to spend too much on
complying here I think they'll just
focus on the big tech companies and
they're not really going to pay much
attention to a small start start up like
mine for the time being and if you've
got a big risk appetite I mean good luck
hope that works out for you
um so I mean that you know all jokes
aside that's the way the world Works um
that's the way law works is that if you
you if you've got a it was the same with
gdpr you had a lot of companies
especially smaller companies go they are
not going to pay any attention to us
when this comes in and that's why you
companies that were actually only
affecting their GDP your compliance 12
months 18 months after it actually came
in because well they turned out to be
right that um you know the focus wasn't
going to be on them um for a long time
so do I think that's an advisable way to
go about things obviously not I will
never
in as a lawyer say that um oh go off and
uh don't comply with the law um
obviously that's an enormous risk but if
you feel that it's a risk that you want
to take that's on you um but I think you
know if you do decide well actually this
is something I need to address which
would I think be the proper course of
action the inventory is the starting
block always no matter how big or how
small your organization the inventory of
AI systems is always the starting point
you carry out your inventry to identify
what AI you're
using and sometimes you might even be
surprised oh this is an system and you
have to work with your Tech teams
looking at the the definitions within
the AI
act getting the Assistance or advice
from external ERS if you need to and do
that AI inventory and then you can start
the classification process is this high
risk is this a general purpose AI system
and look you might find okay looks like
we we we came out of this unscathed guys
the AI act doesn't apply to us or at
least not in a big way and um then you
can if you are potentially cut you can
start looking at your obligations as in
what's your role are you provider at
deployer um and so on and what are your
various roles the inventory is the
starting point so there is a lot you can
do internally without involving you know
external lawyers and and so on but um
you know carrying out that infantry is
is is is step one um and you know
basically what we call an a I um impact
assessments
so Louis you mentioned that there's a
lot of privacy professionals on on this
um so most people ought to be familiar
with the concept of a data protection
impact assessment an AI impact
assessment is basically along the same
lines where you look at the technology
what's its intended purpose who it's
going to affect what are the risks and
what mitigation steps have been taken
you know so and to be honest I think
these are things like I see I see it in
the comments here and and I get it
always at International
conferences um about this this real
hesitancy and resistance towards the AI
act most of what's contained in the AI
act in my opinion is stuff you should be
doing already if you're if you're
developing AI in particular you should
have ai governance Frameworks in place
you should have data governance in place
you should have risk mitigation in place
you should have all human oversight in
place
so the general public has a level of
hesitancy towards Ai and I think the
reason for that and I'm not being
factious it's down to science fiction
the only um exposure the general public
has had to AI are dystopian science
fiction narratives typically and AI has
only ever been portrayed as um a
nefarious type of technology and I think
stakeholders be the sharehold ERS your
board your SE Suite your
employees um your investors they're
going to
expect um a level of responsible AI
within your organization and they're
going to be able to expect you to be
able to show that so you know Dei is so
important these days for companies ESG
is so important these days for companies
I think being able to show that you are
deploying AI in a responsible fashion is
going to Eclipse those importance for
stakeholders personally
and you mentioned a few times AI
inventory and and my view is that
probably organizations that are already
doing data protection right and they
have a robust Department with uh
processes and with well organized
probably it will be much easier for them
to start with the broader data
governance AI governance AI inventory
that's my view so how especially if they
want to be focusing on the AI act and
and doing all the documentation if
they're highrisk probably if they're
doing data protection right is already a
great step so if they're not doing data
protection right and they don't don't
even know where the data is so it's
probably going to be uh even more risky
that do you agree with me what what do
what's your view in this relationship
between data protection compliance what
they're already doing so how can maybe
how can they think in terms of okay so I
I'm already doing data protection right
so is there something that I can build
from this that I have already built so
maybe especially thinking about not big
Tech or companies with more than a
thousand employees let's think of that
small startup with I don't know 20
employees that has a maybe an outside
lawyer doing the data protection uh work
so I think that's perhaps that's the the
most challenging so there are not so
many people involved so the single the
CEO has to think about that so that
that's where I I think my from my view
as someone I I've in the past not many
people know so I used to be a partner of
a law firm focusing working with
startups so I've been i' I've in my life
I've spoken a lot with early stage
entrepreneurs and they are lost they
have no idea where to start where even
how how do I what do I start when I I I
have a product it works but I want to do
the legal part right so I think I
personally think especially for smaller
companies that thinking building upon
what's already built in terms of data
protection is a is a good start to think
about AI you have some thoughts on that
Mar yeah I think um what is AI it's a
data processing technology what is data
protection it is you know rules and
regulations in relation to personal data
processing invariably the data being
processed by AI systems will include
personal data therefore you know I
wouldn't even describe it as an overlap
in many ways they're the same thing um
so you're correct as you correctly point
out there's no Reinventing the wheel
here you're basically expanding existing
um
Frameworks um
or you know going down a tried and
tested route and replicating that that
you know existing
framework um I mentioned dpas and
comparing them with the AI impact
assessment you you have an idea of
what's required there and a huge impact
a huge element of an AI impact
assessment is actually the data
protection impact assessments and when
we drafted our AI impact assessment a
big component of it is a dpia because
that's that's a fundamental feature
certainly under gdpr under European um
data protection rules where you're using
new technologies to process large
amounts of data you're actually required
to carry out a dpia anyway and there's
references to it in the AI act as well
so I think they're fundamentally um
linked now here's the interesting thing
you might say Okay so the DPO should be
responsible for AI I would say category
T not because there's rules that say
that the DPO cannot dictate the um the
data processing but if you're in charge
of AI within your company your job is to
dictate how the data is processed so I
think there's an inherent um conflict
between those roles um we have seen with
certain clients that AI has been moved
into a kind of a data role so the data
protection role Within These big
organizations has been subsumed into a
larger data role so there's now ahead of
data and under data you'll have data
protection you'll have ai so that's one
way of um the way it's it's um coming
about but it seems to be fall falling
under risk compliance data like Windows
um
headings thank you
for
continue yeah um so um I I
think um if you're a small company doing
data protection the right way I
think you know what you've done for data
protection replicate it you know is is
the best advice I could give but you see
the thing is is that
I've described it before and I'll say it
again the AI Act is the gdpr and
steroids the gdpr doesn't require gdpr
doesn't require product
registration it doesn't require product
regulation doesn't require having to
deal with Market surveillance
authorities and dealing with um how you
know annual checkups and annual
reporting for products and and all of
this so I think there is that extra
element but if you're looking for
someplace to start I think you you know
that's a good place to start but it's a
starting point um that's that's how
preface it so for the audience if AI is
autocomplete on steroids so the AI Act
is gdpr on steroids so now we have the
whole PL I'm kidding so it's an
interesting uh interesting comment and
and thank you for pointing out that I
think it's an important comment on the
DPO so for those not in privacy data
Protection Officer uh it it has there is
this requirement of in being independent
figure right you cannot uh dictate where
how data process internally to the
company works so it's interesting that
you pointed out that some companies are
doing this I think it's very uh
important information here for the
audience so this idea what is really
varies uh Insight from his experience
that companies are organizing it in
terms of data so we have this data
department and then there's data
protection on one side and data
government on the other side dealing
more with AI so I think it's a very
important comment so let's move to some
specific Topics in the a which I think
are very interesting and I think Al also
many people are curious you mentioned
that many people they think of AI as
this science fiction monster that you
come and and Conquer us but AI actually
if we think of AI we are already living
with AI obbi in an ubiquitous fashion I
would say at least for 20 years so if
you all of you if you use any social
network you have your newsfeed your news
feed is AI powered if you have
advertising so you're you're being shown
as this Behavior advertising is AI
powered or recommendation if you buy
something in e-commerce the the the
engine that recommends you to buy
something else it's already AI so in
terms of not not generative AI but some
sort of AI algorithm which is
personalized which is learning about you
and learning from the statistics and
suggesting Things based on other
people's behavior so we are living with
AI for at least I would say
overwhelmingly every with we're living
with AI everywhere I think probably at
least since social media started
everywhere we are seeing Ai and and of
course generative AI I would say the
beginning of this generative AI wave
would be November 2022 with the launch
of chpt we have this suddenly we have
this boom generative AI millions of
people suddenly started playing or using
ad chpt and other AI chat Bots for
productivity and so on just just to to a
brief comment on this idea of that AI is
uh something either is is not new even
if we think of the field of AI it
started in the 50s right we can start
it's much older
uh I would say of course in the 50s no
people didn't have computers at home so
it of course every things evolved and
and most advanced types of AI they they
are here for 10 to 15 years uh so let's
talk about some uh interesting AI
applications that have uh legal
implications so emotion recognition I
saw very recent posted uh an interesting
review of the how the a deals with
emotional recognition so I have a few
comments but I want to to let you're
being so educational today but I'm so
happy that you're you know that many in
the audience they are not familiar so we
because we are in this AI bubble and
this I I write every week and and I
teach in my boot camps about the a so
for me it's everything obvious of course
everybody knows it of course everybody
knows article six but no people don't
know it so I'm glad we we are being uh
we we are we are supporting AI literacy
here as Barry mentioned so article four
says that AI literacy is a legal
obligation everybody if you're working
in in an AI company if you're developing
AI if you're a deployer a provider you
must know about AI so we we are also
supporting AI literacy so Barry you want
to speak a little bit about emotional
recognition and the AI act so maybe also
start with uh what and some concrete
examples right what is emotion sometimes
people read okay emotional recognition
so I'm probably not doing that but maybe
you are doing that so you want to say
something about it so um when open AI
had their live and of course I'm such a
nerd an AI nerd I was watching it live
they had their live um announcement of
gp40 their latest model um one so it's
multimodal and what that means is that
it's able to work by text by Audio by
video and through using different these
different media it's able to identify
human emotions so it live on stage was
able to assess the emotions of one of
the uh one of the people on the stage
and it's fully capable of doing
emotional recognition and AI systems
have actually been able to do this for
for quite some time based on it might be
based on different factors you know you
might be familiar already with what's
called sentiment analysis so if you're
writing an email and if you're pretty
ticked off with the person you're
writing to the AI might say hey do you w
to bring that down a bit and make it
more friendly um that's been around for
a while but the emotion recognition
systems are getting more sophisticated
so
so
um when they were developing the AI act
it is interesting how the lobbying went
so actually Airbus were involved in some
of the lobbying because there's a
derogation in one of the recital of the
AI act which says that let's say what AI
systems that monitor hardness of Pilots
That's not included and because Airbus
were they have those systems in Pilots
um cockpits and they didn't want like
you know is sleepiness is tiredness in
emotional state or physiological state
so they specifically derogate from
physiological States um so in relation
to the education setting or the
workplace setting any AI systems that
are capable of emotional recognition are
going to be prohibited banned from the
second of February so the rules and
prohibited AI systems and AI literacy
come in on the 2nd of February of next
year um so what does that mean
basically it's not talking about AI that
can say this is a happy face or this
person is frowning this person is
smiling that's not what it's capturing
and it specifically says that in the
definition it's it's AI that can
identify emotional states so this person
is angry this person is sad um so if
it's used in the workplace or the
education setting that's prohibited and
it's banned so um that's where we're
going to be in relation to uh um
emotional recognition and where it's not
in the en in the workplace environment
where it's not in the education setting
it could be considered high-risk AI um
under biometric
identification so um I would be just
very cautious when dealing developing
using emotion recognition AI
systems and but I think an important
comment here to add is that still in
educational and uh and work settings
it's is alluding if you show that it's
for safety or health reasons right and
then yeah for for for yeah for medical
or or safety purposes so um yeah
absolutely there there is that
derogation um because obviously if if
you if if um somebody who's having a
breakdown or something and it's used
there or something like that then it's
used but in terms of normal practice
it's it's where it's B but you have to
be able to show it's for those safety or
medical purposes yeah here I when I read
this article I always think of a
malicious company or you know in some
countries they have those uh faceal
recognition systems to monitor employee
productivity right so they want
employees that are extremely energized
and if if you show that you're tired if
you show that you're constantly angry or
as you mentioned some some sort of
emotions that the employer doesn't think
that are suitable for the workplace then
the employer might fire you and and so
on but unfortunately I think the the
exception here is another weakness so it
can I can I can see employers saying
that it's they they might find some sort
of safety reason but what they're really
trying to do is this just an example
right monitor employee productivity
which I think was one of the that's why
it's banned in the workplace for the
for originally that's was that was the
intention but I I think that the
language used and the way that the
exception is framed I I unfortunately
think that there will be many if the
company really wants to monitor that way
they will find loopholes so this is just
my my quick comment on emotion
but that would then fall under the um
performance monitoring element of the
high risk of of the use of AI in the
workplace so I think any type of a
employee monitoring any type really
involving AI is going to be um is going
to be considered high risk so like let's
say in a law firm environment we've got
AI systems as part of our time
management that captures time um now is
that a form of performance monitoring
like do we need to turn off that element
of the AI because it is AI it is
performance monitoring so is that
high-risk AI so there are going to be
these Fringe cases where like you see
now the thing is though is that that's
one of those examples where you carry
out the derogation exam and you look at
the different derogations and you decide
as I would be quite confident in
deciding that is not a high-risk system
but still as a user of that system you
would still have to say or the deployer
the the provider of that system you'd
still have to say noce say in t rescue
you still have to register that with the
EU so you know there's a lot of things
to consider here I'm happy that you're
confident in the EU bureaucracy that
they will catch what and here a bit what
of what we are discussing is what we
learn in law school right the spirit of
the law so what what the lawmaker or
what what's the the real goal and what
will happen on the ground sometimes at
least there is it's not compatible I
think we we'll see in practice let's see
how it works so I'm I I see that you're
you're you trust the European
bureaucracy and your you believe they're
going to to catch whoever is doing
something high risk is going to it's
like it's going to be to fall in some of
the Nets it's going to be caught you
know you're not be going to be able to
no it's not going to capture everyone
but you know I'd rather have these
regulations in place to not um I think
and it's becoming this unique European
thing like about protecting fundamental
rights over um Innovation or or profit
like
you know for me as a European citizen I
I I find myself in a strange position
saying hey I think protecting my
fundamental rights is is better than a
big company having profits so you know
that's that's what's behind the DSA
that's what's behind GDP orst behind the
AI act so like new rules coming in like
the audio media Services directive the
Digital Services directive they're
making the internet safer for my kids so
you know by the time my my children um
you know when they're in their mid-30s
and they get their first mobile phone um
from me um mid 30s
wow yeah if yeah like that's if they get
their way ideally they'll never get one
um but I do feel that European
regulations is making it a safer place
for you know children to be able to be
online and things like that regulating
social media I think that's important I
do think that's important like you were
talking earlier about recommendation
systems you recommend like there's
prohibited elements of the AI act that
deal with um use of subliminal
techniques um AI systems which exploit
vulnerabilities based on things like age
that's a good thing so we already have
rules in the DSA in relation to
recommend their systems these are now
going to be supplemented by rules in the
AI act so where you have um you know
there's there's one of the issues in
Ireland at the moment and that's been
talked about a lot in relation to
recommender systems is um e disorder
content being pushed to young
girls um that's obviously not a good
thing to happen but now because of what
the EU does and the teeth that the EU
gives to Regulators we can put a stop to
that and really help people and make it
a safer environment for children so I
think overall that's that's a good thing
and you know what I say in response to
these things that oh the EU is stifling
Innovation I disagree if you're a
company in Boston you're no better or
worse off than a company in Berlin
because if you both decide not to make
your product available on the European
market you're you're you're starting
from the same spot you could actually
argue that the the company in Berlin is
better placed because they've got an
existing framework and all these
guidelines to help them get their AI
Systems off um off the ground that
aren't necessarily there in states
without regulation so if you decide to
not deploy into EU you're in the exact
same spot as any other company that
decides not place a product on EU but if
that Boston company decides I want to
sell my AI product on the EU market then
they're there in the exact same position
as a company in Berlin that wants to
sell their product in the European
market so I don't see this as stifling
um Innovation that's my personal opinion
thank you Barry and let's let's continue
with those uh more specific topics I
think one of my favorite ones at least
discussing also from a policy
perspective I think it's very
interesting so regulating dip fakes so
the AI act talks about DP fakes so what
do you think will be the main challenge
so I'm this this is something that I'm
passionate about I write a lot about
that about the concrete cases and the
harm and the people involved so what do
you think the way that the a act
regulates the fake so how is it enough
are you uh do you think it's going to uh
to to work in practice what do you think
are some of the challenges unsolved
challenges in regulating The
Fakes so here's the thing with deep
fakes I don't think so deep fakes are
are an issue I think they're dealt with
appropriately in the AI act I'll go into
that but I don't think um deep fakes are
as big a problem as they're presented to
be and this is
why if you look at social media
platforms like there was a viral photo
that went around about six months ago of
um a ship that was on fire the cargo
ship that was on fire and the post was
uh another American ship has been missil
by hooty
Rebels but that's not what the picture
was of at all it was a picture of just a
random ship that happened to have a fire
in it it was taken five years
ago that wasn't a deep F you don't need
deep fake for most of the information
online people are very good already at
just being liars and just being bad
people for the majority of of
misinformation and disinformation online
you just need bad people who are lying
and regular photos out of context so you
know when before Joe Biden announced
that he was stepping back from not
accepting the denomination there was a
video going around at the D-Day
commemoration um in France and it showed
him it seemed that he was daughtering
off on his own from the group of leaders
but that was a cropped image he was
actually walking towards as a
parachutist but that went viral online
was that a deep fake no it was a cropped
image or an highly edited image that was
used and used out of context so I think
that will continue to be the problem now
that's not to say that there isn't the
problem with deep fakes you had the um
there was a story of I think again it
was a Joe Biden voice a robo calling
people saying ah you don't have to vote
um you know that's a problem but going
on to how to deep f are dealt with under
the AI act um synthetic content so
content produced by AI systems is going
to be need to be marked as synthetic
content into
metadata now where it's it's it's um
involving some like public figure or
public information something like that
they it'll actually need to be
watermarked but here's the thing those
bad people online that are so Dept at um
that are so Adept at you know just
editing a photo or so Adept that um you
know just lying they're just going to
crop out the watermark so that's the
problem and there isn't any real other
way of dealing with
it yeah very interesting first that you
you you you pointed out that the big
problem usually one of the main big
problems in deep fakes is ex spreading
so if you have something uh uh fake
image that you want to cause harm and
then you need some engine to spread and
us usually the engine is social medias
and it's already happening right we see
now it's approaching American election
so we see basically every day a new deep
fake so it's good that you pointed out
that even if we solve the Deep fake
problem if we manage to have content
authent content Providence signs TR in
place and disclosure in place lies will
never go away and if we have social
media probably we're going to be able to
spread lies really fast so even with the
a I don't think we of it's unrelated
right with the problem of fake
information regardless of AI and and
image or audio Edition we still have the
the problem of misinformation there it's
important to point out though as well is
that like these are extra things on top
of what's already there so like if
there's a defect of you doing something
illegal that's just defamation you
already have legal recourse anyway you
know so I think this is meant to help
the general public but we already have
rules around like I see some of the
comments there
deep fake sexual acts deep fakes in like
certainly in Ireland in the UK in most
of Europe it's illegal already to
publish um deep fake images of somebody
like posting someone's face on a
porographic film that's already illegal
there's already legal recourse there
there's already legal recourse there in
terms of defamation things like that so
we're talking specifically about how the
content itself is is dealt with and
again none like if if you have an AI
system that's able to put someone's face
on a pornographic video that AI system
probably isn't going to be complying
with the AI act anyway if it's if if the
AI system allows you use the AI system
for illegal stuff I doubt it's going to
be massively concerned about oh no we
need to Watermark this illegal content
um so and and I think you hen Upon A
really important
Point um a really important point which
is I think when it comes to Deep and how
it's dealt with you know in terms of
misinformation disinformation we're
looking at the wrong type of AI the AI
we need to be consider concerned about
is the recommendation system AI it's not
so much the Deep fakes it's the actual
recommender systems that pushes this
viral um
misinformation and just still want f a
quick comment article 50 of the a talks
about U two ways two different types of
ways to help deal with de F so one is
for AI providers so you as as Barry
mentioned so you have to add some sort
of metadata to show uh authentic so
someone was posting here c2p so content
Providence so need you need to Watermark
to add something to this image to
metadata to show uh that is uh synthetic
and here we have a technical issue so
far right so even if you look at open
ai's website they say content Au
authenticity is not the best strategy
because it's so easy to remove so for
example if you screenshot an image if
you're talking about a fake or or an
image any image if you add some metadata
to to to show that is real or this is
original or that is synthetic if you
screenshot or if you upload it on social
media you basically remove the Met most
social media platforms currently remove
the metadata so even by mistake not
maybe you wanted to just screenshot and
post it to show other people what you
just found and then you remove the
metadata so this is is really a we so
the AI act expects us to have a high
standard uh metad technique for Content
authenticity and we still don't have so
this is I would say a still interesting
uh technical Challenge and also I
think I think the the biggest issue for
deep fakes isn't so much the
conversation we've been having so far
here but I think it's actually in the
likes of financial services and
fraud um where more and more financial
services institutions are relying on
Biometrics and you know sending selfie
in or you know this biometric
authorization um and increasingly um you
know like these are attacks I think
every company will be familiar where the
the CFO gets an EMA gets an email from
the CEO saying oh hey emergency uh we
need you to transfer 50,000 over to this
account thanks and they've just hacked
the CEO's account or whatever but now
there was this issue in Hong Kong where
it was actually done by
video there was the CFO of the CEO
authorizing the transaction so really
you started with a fishing attempt then
he he was suspicious and then they say
Okay so let's jump into a call and that
was a deep fake of the CEO or CFO right
it'll be interesting to see how the um
that investigation goes um I'll be
keeping it close ey on it but um I think
that is an issue um increasingly and how
do you ensure that your staff you know
we all are sick of this training I'm
sure we all get the same type training
in relation to how to deal with fishing
attempts and so on but you know problem
I think the big problem with deep fakes
is from a cyber security perspective
it's that it makes um social engineering
significantly
easier um and it makes it a lot easier
to trick people um from a fraud
perspective so that would be my big
concern in relation to deep fakes and
you can regulate until the cows come
home um in terms of deep fakes if you
are a
criminal um that is going to be doing
this you're not going to be bothered by
what the EU or anyone else has to say in
terms of regulating deep fixes so um
that's part of the issue as well so you
know that's why you have the likes of
Interpol and europol um spending a lot
of time and resources and dealing with
the Deep fake issue as well because it's
a huge criminal issue not just in terms
of AI act
regulation thank you bar so I I know
you're busy today so we'll have to skip
two questions I want us we'll soon have
to finish but I want us I want people
you it's has been so interesting this
conversation I want people to get to
know your personal opinion a bit more so
regarding AI Tech or regarding AI in
general I want to know I want everybody
to I want to share with the audience so
how do you feel about it so are are you
in what team are you so are you
optimistic are you excited do you use AI
do you think that AI is overall benefic
and we're going to have lot of uh
positive outcomes from it or are you
more in the skeptical uh Team so I I
don't want to to use it so much maybe
you have ethical issues with it or are
you worried are you more uh focusing on
what can go wrong so it's it's
interesting to see how the AI debate is
very polarized kind of some people have
taken extreme positions of AI is
extremely bad and everything that comes
out of it is probably bad or AI is
always good and it will solve all
Humanities problem and some people's uh
with eal ISS so I want to I would like
to know your personal take on it so do
also do you use AI how how excited are
you about it what what's your personal
opinion there's two Shakespeare quotes I
always use um when talking about Ai and
what my thoughts are one is nothing is
either good or bad unless thinking makes
it so so you know AI isn't going to be a
good thing or bad thing it's how people
use it like any other technology and the
other one is um there are more things in
Heaven and Earth ratio than are dreamt
of in your philos phos ophy so in terms
of whether AI is good or bad I
personally think it's going to be great
provided that we regulate it properly
but you know we're doing a lot of work
with
Healthcare um organizations at the
moment and Medtech and we're seeing
what's coming down the tracks in terms
of AI and without any exaggeration AI is
going to add not just years but
potentially decades to people's
lifespans it's going to be able to
discover answers before you even get it
because it's going to be able to
identify different biomarkers over time
and how biomarkers change and massive
populations you know it's really going
to have a very profound effect on
society um and the thing is is that
people ask for
predictions and you know when the
printing press came in when gutenberg's
printing press came along um nobody
foresaw the implications that would have
I mean when somebody developed the
ability to mass-produce books what they
have foreseen well this is going to
cause a great schism in the Catholic
church and it's going to cause
Catholicism to Branch off from
protestantism and like you know there's
these unforeseen
consequences um but I'm extremely
optimistic about AI um but it's
predicated on us being sensible with it
um and there are you know signs of hope
that we will be sensible so the AI act
doesn't cover military uses of
AI and we are already seeing militaries
around the world using AI
um um to to kill
people and there's attempts by being
made by the US to uh have an
international treaty signed that AI will
never be allowed make a decision to
launch a nuclear missile and or nuclear
weapons and there's hesitancy about that
I guess the the current state of the
world with you know Russia China and and
the United States and Europe and NATO
and everything else going on that treaty
is in place and if you're to be scared
about anything with AI That's what you
should be scared of is that we don't
have that treaty in place and you know I
know I mentioned dystopian science
fiction earlier but there's a reason
that virtually every dystopian science
fiction involving AI involves the AI
having access to nuclear weapons now I'm
not saying that the AI will get sentient
and decide to wipe out Humanity for that
reason I'm saying the AI is just a a
data processing technology that we
giving handing over a nuclear weapon
suit I think that's bad for the record
but I think in terms of Civilian uses
for AI I'm extremely optimistic but I'm
really hopeful that from non- civilian
purposes and uses of AI that we treat it
very very carefully because I think it
has capacity for enormous hard so that's
that's where it stands as you just spoke
about nuclear weapons let's move to more
optimistic topic let's talk about jobs
and employment and so many people in the
audience I've been in touch with after
in my boot camps I have onetoone calls
with the participants and I know that so
many people are interested in transition
to AI so they are many privacy
professionals they are trying to to
transition to AI governance so many
people in students in law school they
they are asking so how do I specialize
in AI so how do I get a job in AI I'm
not a developer so many people are like
us bar they don't develop AI we they
don't know how to c they don't want to
learn how to c but they want to work
with in AI so one of the so AI
governance is a big thing right became a
thing now people are getting certified
so there's a big uh big Market uh
blooming out of it so I want to ask you
so as as a partner of a a prestigious
Law Firm so many people would like to to
work at William fry and other uh law
firms like yours so if you were hiring
an AI lawyer if someone wants to be to
work in your department
what skills would we looking for so what
what type of if someone wants to prepare
to apply for a position in your Law Firm
or in other law firms working with AI
what what do you think are important
skills or or characteristics or what the
the person how the person should prepare
to to this so I think it sits within the
technology Departments of law firms so
like having experience in intellectual
property data protection um commercial
agreements soft Ware agreements that's
the basic level you know so you have to
have if I was firing someone the base
level I would be looking for would be an
excellent technology lawyer with
experience in all of that you know data
protection IP um commercial agreements
IP agreements um software license
agreements so on but for me personally I
think the real differentiator is having
an understanding um of the technology
having a good technical understanding of
the technology being able to pick up a
technical paper and read it and then
understand at least half of it um I
think is really important because in
terms of let's say let's take an example
of reproductions within AI when you
consider you know um you know isn't
coding a piece of data an active
reproduction for the purposes of
copyright you actually need to know
what's technically happening what's
actually um going on like what is Vector
and in in a large language model you
know for the purposes of actually what
is the infringing infringing act from a
copyright perspective you actually need
to understand how the CSD systems work
so I'd be looking for somebody who who
has ad ability to to understand so I
think and you know how do you go about
that there's no easy way um I think I
did a PhD on it um there is no shortcuts
you like to catch up and it's something
I do I mean when I get home and you know
put the kids to bed and then I am locked
in in research papers and reading and
reading reading same as you Louisa um
you know it's just constant and I guess
I found my passion I'm really lucky but
I think you need to have that kind of
Reckless enthusiasm for for the area
because it's so new and there's so much
to learn about it but I think you have
to be willing to just throw yourself
into it completely and be very you know
ferociously curious would be um how I
put it um but certainly that technical
element um I think is super important I
think you agree with me but that is a
very competitive environment right of
course everyone wants to have a piece of
the AI cake so it's probably it's
probably hard if if you I don't know if
you have an open position right now but
it's probably uh hundreds of CVS and
people everybody wants to to be in Ai
and for for the hype for the coolness
and also for the career
the growth and everything if you if
you're in AI now you have the potential
to grow and become a leader in a few
years so it's I think it's very
competitive with everything in life if
it's something very competitive remember
if you were I don't know apply for a
university position or a Masters or an
NBA you know everything that is
competitive you have as Barry mentioned
you have to be really focused and
obsessed like the same way with
entrepreneurs and and also if you want a
difficult uh position or something that
is everybody wants to be there so you
want to be really obsessed I am I think
Barry and I we are very similar in the
sense we all so we we both of us have so
for for people in the audience that
think we we are just single people
without kids so both of us have kids at
home so we we are extremely busy and
still every day even uh being dealing
with it every day we are still every day
focusing and and breing and and up so
there's there are new reports and laws
and lawsuits almost on a daily basis so
every day you have to you you cannot
just say Okay I want to work in AI but I
don't have time to read I just go to
work 9 to5 it does I don't think it
exist if you want to be a leader if you
want to really get the best position I
think you need to go beyond it's not
just that it's not like any job or yeah
I want that position that everybody
wants but I also I don't have time to
read I don't have time to research I
don't have time to keep up to date I
think it's the bar is is high especially
because it's competitive thank you just
just yes as a final note and I think you
see this a lot as well Louisa it's the
I'm being reminded of it in increasingly
lately um it's called The Dun and
Krueger effect I'm sure many of of the
people here will be familiar with it but
it's this idea of people thinking
they're experts when they're not and
having a little bit of knowledge is
actually a very dangerous thing because
increasingly you're seeing people who
think they're experts in AI but actually
really are limited in their AI knowledge
and are spreading a lot of
misinformation about AI um as a result
because they they feel like they're
experts because but because their
information is actually Limited they
don't realize how far away they actually
are from having a good level of of AI
expertise so I think one of the big
problems and I was talk I was speaking
at an Irish State body today and it was
one of the issues I pointed out was
being able to discern which expertise to
use and look you know I'm a I've been a
fan of Louis's for a long time long
before she approached me for this talk I
was following her I think she's
excellent Louisa would be absolutely one
of the people I would have in my top of
my list um for for people to go to for
AI information but I think organizations
need to be very Discerning now about who
they trust and where they get their
expertise from because you know we will
all remember gdpr there was a lot of
experts very suddenly when the gdpr was
coming in two years ago LinkedIn was
full of web tree and metaverse experts
they've all disappeared to know where
they've all gone and but there's a lot
of uh prompt engineering experts know
and AI experts know so these are things
and fully appreciate the RNA of me
saying this calling myself an AI expert
but um you know hopefully the proof is
in the pudding but I think that is
something to be wary of and um it's
important to have leaders like Louisa um
in this space as well so uh look thanks
very much for for setting up these
spaces Louisa it's important work my
pleasure really really this conversation
was really a pleasure so thank you also
for the audience for being so engaged
Barry do you want to say some last words
where can people find you so you are
what social networks are you active in
or you want to tell people anything last
message before we we conclude yeah
absolutely if you want to follow me on
LinkedIn um just do a search for me
you'll find me this might be the last
few days of where I have more followers
than Louisa so I'm trying to enjoy
them um she's caught up at me and
overtaking me um so please get in touch
with me in LinkedIn and do a search for
William fry artificial intelligence next
week going to be publishing I'll reveal
it here we haven't revealed it anywhere
else but don't be telling anyone because
it's it's still a little bit of a secret
but we're revealing our um AI ACT guide
so it's going to be this big bumper
insights um into the AI act and and all
of the work we've been doing for a long
long time um so um that'll be released
today week coinciding with the AI AC
coming into Force so keep an eye on it
and follow me on LinkedIn I'd be
delighted to to have you and Louisa just
wanted to say to you again thanks so
much we talk about your posts all the
time in William fry my my colle say oh
did you see loua saying it's like yes
I've seen it um so um thanks very much
for all the work you're doing as well
thank you bar so if anyone is interested
in AI training so at the AI Tech privacy
Academy we our summer boot camp sold out
so I'm really busy every day I have a
boot camp so it's really exciting we
have two cohorts one about the UI act
and one about emerging challenges in AI
Tech and privacy starting in September
so save your spot because it they will
so so sell out as well thank you
everyone for joining thank you Barry and
see you in the next live talk bye-bye
everybody
