hello everyone Welcome to our AI
governance live talk today I'm here with
Gary Marcus hello Gary welcome to the
show hey I'm excited to be here so today
we are talking about uh taming ciling
col Valley and governing AI so we Gary
has his book so you can show the
audience Gary you have the book oh I
didn't know I was gonna do the van white
thing look at this lovely paperback
released in paperback to be inexpensive
and accessible to all you see if you
haven't bought yet you can go so it's
also it's paperback in hardover and it's
everywhere only paperback only paperback
only inexpensive and where can online
every online store the people can find
it or anywhere specific theoretically
yes okay to hard and today we are
talking about AI policy AI regulation
all the hot stuff in AI uh and you if
you follow uh Gary on Twitter and now on
Blue Sky also where else on LinkedIn you
know that Gary has spicy opinions on AI
and on Silicon Valley on AI regulation
and all those topics we are covering
today uh so I'm excited to be here with
Gary and I I'm happy that the audience
is already uh so active so before we
start I want to remind you so if you're
not a subscriber yet so subscribe to my
newsletter L newsletter. comom to stay
up to dat with everything related to AI
governance AI policy AI regulation the
latest lawsuits AI copyright lawsuits
and so on Gary also has a newsletter
right it's Gary marcus. subs.com
yeah right so you can also subscribe you
should subscribe to Gar newsletter if
you want to join the AI governance
training December Edition so register
soon so it's starting in the first week
of December so don't miss it the we you
can join using the website under my name
so be every time I have uh when when I
have guests in these live talks I like
to hear a bit about your personal story
Gary so I was checking your your linked
in your background so you have a PhD in
cognitive science from the MIT right
from uh few years ago and you so your
background is cognitive science and
today you're still a professor of
Psychology and Neuroscience at NYU so I
wanted retired retired Professor I'm
Professor Merith which which is like an
honorable discharge I retired at the age
of 49 and and another trivia fact is
that I never finished high school so
I've been a rebel since I dropped out of
high school and went straight to college
so that that part I didn't know so I
want to hear so how was that that story
from high school Rebel to cognitive
science to AI what made you attract what
what made you move to this field of AI
and now you're you're so also focused on
AI policy AI regulations so what what
did you find interested in AI well first
of all the order is backwards so I was
interested in coding from the time that
I touched a paper computer when I was
eight years old which was also my first
experience explaining computers to other
people on the local television channel
uh when I was eight um that immediately
got me into Ai and AI got me into
cognitive science because I thought well
these AI systems aren't very good how do
people do these things so I was into
cognitive science by the time I was 15
and in fact part of the reason that I
dropped out of high school was so that I
could pursue cognitive science in
college there was no way to do it in
high school I was already working on AI
then I wrote a um Latin to English
translator in the computer programming
language logo which is a little bit like
lisp um and so I I was interested in AI
for a long time but also kind of
disappointed with it and that did lead
me to a long career that was always kind
of straddling the two primarily in the
cognitive Sciences looking at Child
Development cognitive development but
also always looking at neural networks
um so my dissertation was actually
comparing human children how they
learned language with artificial neural
networks that are kind of the ancestors
to today's large language models and
showing some important differences
between them um so I've been interested
in AI for a really long time and I think
my role in AI is really been to look at
what's going on and take a cognitive
science perspective and say how does
this compare with what humans are doing
um that always made me hated because I
always said that what we've got is not
really good enough and nobody wants to
hear that their pride and joy isn't good
enough but the reality is I think it's
not and we might talk about this long
Arc about diminishing returns I've been
calling that for a long time um much to
the consternation of the field but I
think in the last few weeks that's
actually become a common view so Mark
andrees for example and ilas sit ever um
have acknowledged that we probably are
actually hitting a wall with large
language models so anyway that's one
strand of what I've done throughout my
career the other strand is policy that's
much newer um I've always been like I
guess I'd say politically interested but
I didn't really work on policy until I
thought that people kind of lost their
bearings around chat GPT so I was always
interested in some of these issues and
so forth but um when
chat PT came out I feel like the field
of AI went from a research field a
research driven field primarily people
just trying to do the best they could in
AI to something that was very clearly
shaped by money and priorities of money
and the notion of responsible AI just
like disappeared overnight you know it
went from like a valued thing to like an
afterthought I think a lot of companies
back down on their promises around
responsible Ai and at the same time
because generative AI is unreliable for
technical reasons which related to kind
of my life's research around
generalization and so forth because they
were inadequate for technical reasons
that opened up some problems and because
suddenly they were in widespread
distribution you 100 million people
overnight started using chat jpt we
suddenly were in this kind of Perfect
Storm of corporate irresponsibility
unreliable technology but distributed
large scale and that really concerned me
and so I've spent a lot of the last two
years working on policy trying to
explain to people why large language
models and the kind of ecosphere around
it are morally and technically
inadequate and trying to think about
what we might do about it and taking
from that so you mentioned in your book
so how Silicon Valley always tries to
downplay risk right so you mentioned
that briefly so people don't want to to
talk about it also because there's money
invol involved so of course the CEOs
they don't want to say that is not good
enough so hype sells more when it's it's
excellent it's going to increase
productivity it's going to shape the
industry and so on so I took some quotes
from your book so you you mentioned uh
for example when Yan leun so Yan Lun is
met as Chief scientist for those in the
audience that don't know uh so he
claimed in a series of tweets on Twitter
in November and December 2022 that there
is no real risk reasoning fallaciously
that what that what hadn't happened yet
would not happen ever so L&M have been
widely available for four years and no
one can exhibit victims of their hypothy
dangerous so this is by Yan Lon and you
also cited certainly not by me I said
certainly not by me exactly I mean Yan
Lon said that I yeah Yan yeah exactly so
Yan leun and um May 2023 so Microsoft
Chief Economist Michael Schwarz told an
audience at the world economic Forum
that we should hold off on regulation
until serious harm had occur there has
to be at least a bit of harm so that we
see what the real problem is is there a
real problem did anybody suffer at least
$1,000 worth of d because of that should
we jump into to regulate something on a
planet of8 billion people when there is
not even $1,000 of damage of course not
these were so Yan leun and Michael
Schwarz so not not Gary uh so this idea
this whole idea of downplaying RIS and
of course now we are November 2024 so we
had robocalls we had we have deaths so
there was that Belgian man that suicide
himself last year so this year we had
the Florida teenager that suicide
because of an AI chatbot we have
ubiquitous hallucinations so the
hallucinations problem has not been
solved and there has been from a privacy
perspective reputational harm
misinformation nonenal pornography and
we keep we we will soon switch to your
uh list of the most dangerous parts of
generative AI so from I think you you
you get Silicon Valley much better than
I so why do they keep doing it they keep
denying or downplaying what's behind it
so there there's something I don't as
I'm a lawyer so from a legal perspective
I I see that as liability as bad so they
should not be uh they should be
realistic they should be they should
know that they will be held responsible
so why why do you think first first
question why do you think they keep
doing that still denying downplaying
risk so it's all good it's shaping and
kind of not talking seriously about what
what what's going
wrong uh well I think one of your
premises isn't quite right or at least
they're not sure that it's right which
is um that they will be held responsible
I think in fact that they've been held
responsible for very little the law may
not hold them respons responsible and
one of the worst well no let me start
that sentence again in in the technology
sphere one of the worst laws in the
United States is section 230 which gives
a lot of protection um at least to the
social media platforms and it remains to
be seen how much uh protection that
gives to AI companies you know a bunch
of people think that it shouldn't
protect AI companies but we don't know
so section 230 is for those in the
audience who don't know it is a law that
basically says social media can post any
anything they wanted not be held
responsible for it so it's a very large
um Shield against liability in the
United States now it was written in
simpler days it was written when all you
had were things like Compu serve which
were bulletin boards people would just
post things and the companies the the
media platforms if you even wanted to
call them that weren't ranking things or
anything like that you just had a list
of messages in order it's very primitive
stuff and at that point they were given
this particular ction from liability
then when Facebook invented news feeder
roughly around then um the social media
companies started prioritizing what you
see and still got that protection but it
changed the game so when they were
prioritize things what they did
deliberately was to prioritize the
things that got the most engagement
because more engagement means you're on
the platform more they can sell more
advertisements and so forth and that
might not sound awful but what it turns
out that it does is it actually
prioritizes misinformation
because misinformation outrages people
and outrage actually drives engagement
and so what happened is you suddenly had
social media companies that were
Distributing lots and lots of
misinformation and section 230 did not
hold them liable for that section 230
gave them kind of a free pass on that
when I spoke at the Senate you may
recall that I did that um last year next
to Sam Alman uh there was bipartisan
support to change section 230 such that
it did not apply to AI but that actually
happened after I testified in the Senate
Mark zugerberg came you probably saw at
least a clip of him um turning around
and apologizing to the parents and kids
um you know at that trial there was
again not trial hearing at that hearing
there was again uh bipartisan support
for changing section 230 um in in
certain ways so that it wouldn't give
the technology company so much liability
and nothing has happened and we can talk
about like why Washington has had been
ineffective it's probably not do
anything in the new Administration but
who knows we can talk about that too if
you want but so I would start there I
would then talk about sb147 which was a
California law that was proposed that
would give the companies liability at
least for catastrophic harm which was
defined as half a billion dollars or the
some equation I don't know what the
variables to convert that into a certain
number of lives lost and even that the
tech lobbyists like Mark Andre and so
forth ventri capitalist were able to
shoot down you they were able I think to
intimidate uh we don't know the causal
mechanism but they protested very loudly
and we know that gav Newsome the
governor didn't sign that bill and so
you know fear of liability that's one
side of my answer I'm gonna give two
sides one one side is that that
liability laws in the United States
obviously they're different in the EU um
are not at all clear and so the
companies are like we'll do this and we
can fight out in the courts you probably
saw and maybe many of you listeners saw
this famous quote from Eric Schmidt he
was talking to a classroom earlier this
summer I guess it was probably off the
Record but it got reported anyway um he
said you know if you could take over
sorry if Tik Tok got kicked out of the
US I don't remember the exact words um
you should all try to basically take
their business and the the famous phrase
was something like and let the lawyers
sort it out later and that is the
message to Silicon Valley let the
lawyers sort it out so you know try to
capture as much of the business whether
that business be social media or AI or
whatever and worry about liability later
let the lawyers sort that out and of
course given how financially well off a
lot of these companies are they have an
expectation that they'll probably get
what they want and it's not really just
the lawyers it's also the lobbyists so
the lobbying apparatus of Silicon Valley
is extremely well organized extremely
well funded I think most people have no
idea how active and Powerful it is and
so that's another reason why I think
they don't care and then there's a third
reason which is the media very rarely
holds the big tech companies responsible
for very much of anything a little bit
around social media it took them years
to catch up on that it's going to take
them years to catch up on AI I just give
a slightly different example of that
which is people like Elon Musk and not
just Elon Musk have learned that you can
make promises that aren't delivered so
for example musk had a product called
autopilot which sounds to the average
person like something will drive your
car for you but it doesn't actually do
that there is I think one lawsuit
pending about that it's kind of false
advertising but he's never been held to
account then he offered a product called
Full self-driving which sounds like the
car should do full self-driving it
doesn't he also promised for example
there would be you know a million Robo
taxis on the road by 2020 never happened
he's never held to account for any of
this it just drives his stock buckes up
so Sam Alman took a hint from that and
now he's talking about how like is going
to solve physics which I don't even know
if that's a coherent concept but it
sounds good and you know it drives up
the evaluations of his companies never
held responsible you know Elizabeth
Holmes was held responsible for Theos
but that is very rare in general Silicon
Valley feels free to say whatever it
wants and knows the worst case the
lawyers will sort it out it's just a
cost of doing business so they're not
real worried about liability and so
forth so it's not that they bring this
watch Washington perspective so let's me
bring Brussels perspective so as you
might I posted on that so this week the
new product liability directive what
entered into Force so it's it was appr
published in the official Journal so it
going to enter into force in 20
days and it says that it's applicable to
AI it expressly mentions this we are
talking now about Europe so what might
happen in Europe just to bring a
different side of the debate and maybe
that's why they're stressed out we know
that meta Spotify and know companies
they they have recently signed this
letter so it Europe is over regulating
so we don't like that so let's bring
this you brought liability so let's
bring a little bit of that so this new
product liability directive it say it
says that it is applicable to AI as well
so it's not focused on AI it's focused
on software so it was amended uh to to
also cover software but it mentions that
it covers Ai and at some point it also
mentions that in the blackbox argument
right so it's difficult to explain
sometimes it will be difficult to say
that it was because of that ARG gthm
because of that AI system so they said
if there is if it's too difficult the
burden is too high for the victim to
prove then there can be presumption of
of culpability so this this is very
strong this type of we call it in law
strict liability so this is big this
this is actually so I wrote about it in
my newsletter this week I'm going to
write again uh tomorrow um so I I think
so this might you brought a Washington
perspective but I think Europe things
are changing especially so we have the
ax I have my own criticism about the ax
and we have product liability which I
would like to ask you about later in the
interview by the way we'll talk about
when we when we mention the Brussels
effect I'll talk more about that so we
have we have the a act we have product
liability directive and we soon have ai
liability directive that that some
experts now are pushing for AI liability
regulation which will be stronger will
be directly applicable to all member
states so I think uh things are changed
at least in Europe and we can criticize
that some people are saying that it's
not good for the startups that they're
well behind in competition so that l i
will rephrase my my my initial point
that well from an European perspective
at least from countries that are paying
attention to regulation and liability
and trying to to put things under
control what they're saying what they
keep saying is going to be bad for them
I say it short short run long run it's
bad so maybe just to to bring a
different
position I mean I don't doubt that the
companies are saying that and of course
they were threatening California saying
we we will leave if if if you pass this
but I think that that was that was bogus
they wouldn't have actually left no not
one company would have left California
if 1047 had passed they would have just
seen it as okay we have to do some more
compliance maybe a startup would have
left but none of the none of these big
well-funded startups none of the
publicly traded companies and so forth
um would have left California and
they're threatening to leave Europe they
think they're it's just a threat or
they're they're going to leave Europe
what's I think it's probably an empty
threat I mean don't forget that Altman
already threatened to leave Europe once
o over um I guess it was Italian uh
uh restrictions on on chat GPT and he
didn't actually leave um I would be very
surprised it's it's a big Market um I
mean related to that and related to the
Brussels effect which is you know I
guess you want to talk about more later
but what I think is going to happen here
is is is that Europe's going to set the
rules because the US is moving the
opposite direction like um I don't know
if people are following uh Elon Musk and
Vic Ras I I may mispronounce his last
names um Doge thing but what it really
is is not about saving money or at least
that's not the main purpose I think the
real purpose is to undermine regulatory
bodies and there's a decision called the
Chevron decision that the Supreme Court
just made not that long ago that that
they've already pointed to as a tool in
that Quest so their goal is going to be
to try to undermine every regulatory
body I mean in part because like musk
has companies he doesn't really want
them to be regulated and so Europe is
going to have strong regulation around
Ai and the US is likely to have almost
none and I think the long-term question
is going to be does that matter for the
US um the companies are going to be
obliged to do a bunch of stuff for
Europe um and US citizens are kind of
like sitting docks and one of the things
that they want this to do uh want this
to work around um people like mus is
clearly misinformation he loves
spreading misinformation he doesn't want
any responsibility for misinformation
and in fact you know he wants to
threaten Europe it's pretty clear um
with the uh dissolving the the
dissolution or weakening of NATO and
that's partly for him although maybe not
for other people about not wanting to
comply with rules around misinformation
musk sees I think that the way for him
to run the world is through
misinformation so he's very attached to
having the legal right to do that he
gets upset when Democrats spread
misinformation but he love spreading
inform misinformation um uh for
Republicans you can see it literally
every day in his Twitter feed um and so
we're going to be in this situation
where the only real regulation is the EU
a lot of people will want to follow the
EU standard other other countries will
and so that's actually giving EU an
interesting source of power because the
US is abdicating its role you would have
expected that the us would have looked
at the EU AI act and said okay that's
good but we can do better I mean that's
what I had hoped the US would do and in
fact I said that in the Senate I said we
should take the leadership here not let
the EU run it all um and the Senators
nodded and they kind of you know they
seem to agree but the reality is that
the US is not going to take any
leadership role in AI regulation at all
and so the EU is gonna be leading that
way by default now I understand that
it's not perfect and you got a critique
of it and I'm not as up on you uh about
it and probably it is weaker than it
should be and people you know the big
companies will figure out all the
loopholes and so forth but we are in a
place where I think the US is just
entirely abdicating on this so just for
the audience to know I wanted to ask
Gary about the Brussels effect so the
Brussels effect is this expression it
was coined by ano Bradford it that that
refers to the fact that Europe is
leading in regulations so it happened
with the gdpr so General data protection
regulations so when Europe uh enacted
the GDP
the Brussels effect happened so many
countries and as a Brazilian I know
Brazil was one of them so many countries
uh borrowed some of the concepts and
ideas and principles and the idea that
uh data protection privacy is is really
is a fundamental right so protecting
human dignity and so forth so this was
it happened with data protection and and
I've been asking my guests in in recent
editions if they think the Brussels
effect will happen with AI and the
majority of them have said no it's not
going to happen because the AI is from
privacy because privacy is a right
protecting data so data protection
you're it's a lot about protecting
rights or protecting data and when we
talk about AI is is this big big thing
that everybody's going to is thinking
that is the future is going to drive the
economy so bring major profits and no
country wants to and Al also related to
National Defense so no nobody wants to
to be tied on that they want to do
whatever is possible to compete against
China and and and other uh adversaries
so would say so what what is your this
too it's too broad an argument part of
it is correct and part of it's not so um
it is true that no defense department is
going to be constrained at all right I
mean the only way you get any constraint
there is the way that we did with
certain kinds of chemical weapons where
where there's a kind of social stigma
and we could wind up with a social
stigma around using AI power drones but
I think that the you know Generals in
every country want to use them so badly
that that's not too likely to happen it
could happen for example if there's
enough targeted assassinations with
drones and maybe people's public opinion
will change but short of that I it's be
pretty hard to constrain the generals
but that doesn't mean that on some of
the other aspects of AI regulation that
um there's not going to be any desire by
countries to do things I me in fact all
the time I hear well the US can't
regulate AI because China won't and
China actually has regulated AI more
than the US I substantially more than Us
in some ways I don't like in mostly ways
that I think are pretty reasonable um
and so it's just a red herring and so
like China I think will do their own
thing people will either take the China
model or the European model that won't
be a us model and people are going to
see that democracy is falling apart in
the United States and if they care about
that then they're not going to want to
follow that um part of the model they're
going to see that cyber crime is going
to go way up in the US and they're
probably not going to want that and so
forth and so I think a a lot of
companies uh sorry countries will want
to regulate Ai and either they'll go
with the Chinese model which they will
do in certain parts of the world
especially as I think what's going to
happen in the Trump Administration is
the us is going to kind of withdraw have
these high tariffs and so forth
discouraging other countries from being
a trade partner with the US and so
that's going to push people more people
towards more countries towards China um
and so some people will adopt China's
you know ideas around AI regulation most
of which I not all of which again are
reasonable um and then others will
choose to follow European model nobody's
going to follow the US model there isn't
one so the regul so just to go back to
the AI act and what we so National
Defense it's not so the AI act doesn't
apply so it doesn't even matter so it's
also an exception for the AI act but
when we think about for example general
purpose so uh AGI so this big hype word
or that the the in the the AI act we
have general purpose AI models and
general purpose AI models with systemic
risk so they have to we have uh the
Articles covering each one so if you're
classified as having system even if you
are for example open source which is
something I think meta is upset about so
even if you are open source but you are
considered as having systemic risk
you're going to have to comply with some
of the transparency obligations and
other uh obligations in the AI act so
that that part I would say most
countries they are refraining from
especially because general purpose AI
models are currently associated with wow
big disruptions maybe AGI maybe
something very big will Happ happen so
if I regulate that too much current
general purpose models have of course
nothing to do with AI That's Just TY but
we can we we can talk about that
separately if you want um I think a lot
of people labor labor under the belief
that that you know GPT five or something
like that is going to be AGI but I think
that that's an absurd view it is in fact
shaping a lot of world policy uh you
know like the US is putting a lot of
money into AI because they seem to think
that we're not very far from AGI and
that it's important to be um first to
have AGI my view this is a really bad
bet um maybe I'll just say a little bit
about the diminishing return stuff um I
have long argued that it is inherent in
how large language models work that they
will hallucinate make boneheaded errors
and so forth I actually anticipated
hallucinations in 2001 in the book
called the algebraic mind um this these
problems have been around for really a
long time and they follow because all
these systems are doing is statistically
accumulating models of how people talk
but not how people actually think and
they're just not going to get better in
this regard and so I've been saying we
would reach a period of diminishing
returns and now I think we have in fact
reached that period of diminishing
returns if we are then we're certainly
not getting to
AGI I
see from a regulatory perspective I
would say from lawyer talking from the
side of the lawyers so as we lawyers we
don't learn about technical stuff and we
hear that people are saying things that
might happen disruptions technical
disrupt technological disruptions that
might get out of control so when we look
at the classification as having systemic
risk in the a that's what more or less
they have in mind so they have some uh
for example the number of parameters
power okay can I interrupt for one
second you can have systemic risk
without AGI I just want to clarify a
couple things so AGI will come someday
maybe it will be in 20 years it's not
going to be in two years that's absurd
um but it could come in 20 years um but
we have systemic risk now so we have
systemic risks around discrimination we
have systemic um risks around
disinformation etc etc that are here
right now so um one and this also goes
back to something you said earlier that
I wanted to clarify um the the one risk
that the big companies want to talk
about is that robots will take over the
world and I think that's because a it
makes the AI sound like so cool and so
powerful and so it's part of the hype
apparatus and B nobody has any idea what
to do about it so there are no concrete
steps that they have to take um and it's
you know abstract enough they don't feel
super responsible um to it whereas all
of these near-term risks they also have
nothing really substantive to do around
them but they're here right now they
don't want to be held responsible for
them so it's it's almost like a
rhetorical move that they will
acknowledge that something really bad
like the annihilation of the species
could happen I think it's very unlikely
but but the you know bunch of C have
said yeah this stuff could annihilate
the species that is in large part like
theater to distract from these other
systemic risk that can happen right now
even if we don't have HRI so back to
your question but I just want to clarify
all that no great that we're going to
the next topic is the are the 12 biggest
immediate threat from generative AI uh
yeah let's switch to that now so in in
in Gary's book he talks about the 12
biggest immediate threats to generative
Ai and I'm going to to build soon a
bridge with the AI act and how it's at
and and why how I think at least the
lawmakers are thinking and and lawmakers
from different countries are thinking so
I I just uh for the audience to to to
know what uh Gary mentions in his book
so this is the list so the Liber the
first one deliberate automate uh Mass
produce political disinformation two
Market manipulation three accidental
misinformation four defamation five
non-consensual def defix six
accelerating crime seven cyber security
and bioweapons eight bias and discrimin
ation nine privacy and data leaks 10
intellectual property taken without
consent 11 over Reliance or unreliable
systems and 12 environmental costs so
these are uh what what Gary uh raises as
the 12 threats and biggest stress we're
going to talk soon about uh what we
should be insisting on so the the
positive side so now we are talking
about uh the bad side and my my question
on that so again going back to
regulation so do you think having that
those threats so some of them as as as
I'm very familiar with the AI act so I I
know that some of those are not going to
be solved by law itself so for one of
them hallucinations right so accidental
misinformation law cannot really solve
that for example uh some of other of
here law wait wait a second wait wait
wait a second um just for sake of
interesting argument here um if the
penalties were severe enough then the
companies would be incented to build
better Technologies you know or we could
refuse Market access you know if you
couldn't meet some standard um we could
say you couldn't do that you couldn't
put this Market we do that for drugs for
cars for airplanes and so forth we're
giving a free pass to large language
models because of a perception that
they're going to make a lot of money
mean is responsible for that that the
solution the legal solution would be
holding companies uh liable for the
output of a large Lage mod for example
example if there is let's say yeah I I
think there's two different Avenues you
could pursue one would be economic fines
penalties Etc um for you know def
accidental defamation for example um you
know have some legal penalty if if you
spread enough um misinformation of
certain sorts so you know in the US the
First Amendment pretty much protects
things um unless they're you know
sufficiently bad in certain kinds of
ways but we could say if you do this at
large scale that's different from an
individual we could have you know we
could treat the commercial production of
misinformation accidental misinformation
um of certain sorts as as a you know
punishable thing so one thing is we
could you know charge you per incident
could be a small amount of money but you
do enough of it and adds up so that's
one side of it and the other is we could
just deny Market access so we do that
with drugs like if you don't show that
the benefit of your new pharmaceutical
is is greater than the cost then you
know you go back back to the drawing
board it doesn't mean you could never
reduce produce this drug but maybe you
have to supplement it in some way or
maybe you have to think of a of a
different drug and so like it's really
important to remember that the large
language models are just one possible
approach in an enormous space of
possible AI techniques many AI
techniques don't suffer any of the 12
things that I'm talking about or maybe
one or two at most so like the GPS
navigation stuff that that plans your
roots doesn't have all of these problems
it's a kind of AI that is basically
without serious harms as long as it's
competently implemented um it so happens
that the most popular AI that we have
right now is the riskiest and most
dangerous and we could as a society say
come back to us when you stop having
these problems when you when you don't
need to use the amount of energy to
power New Jersey to train one model um
that you might not even release come
back to us or whatever we we could set
various standards and
um from a legal perspective at least an
European perspective there's always this
argument of it's not feasible now so I'm
not going to do it so if it's not
feasible I I do my best so that's
something that comes again again in the
a for examp not feasible to meet such
and such standard you mean so for
example I cannot meet the standard of
zero hallucination so that's it you want
you want larg I me you could have a
negotiation too you could you know say
show I mean first of all we don't have
good metrics around that but let's say
we did we could say look fine 1% but
tell us why the benefit of this um makes
it worth the the cost of 1% error in
fact it's probably more like 25% or
something like that but you know sure we
could do that I I'm I'd be very happy to
see some set of Standards around
performance the fact is we have none and
that we have decided as a society in the
globe that we are so lustful for this
technology that we have basically no
standards around it which is different
from you know almost everything else in
the planet it's different from bread
it's different from electrical outlets
it's different from cars like I think we
have been brainwashed by the the leaders
of AI into thinking that we couldn't
have any standards around their
products just another maybe challenging
you bring another example so for example
article 50 of the a act brings
transparency obligations and one of the
transparency obligations is that if your
if your AI system produces synthetic
content you have to add metadata right
provent signs it has to be marked as
synthetic AI generated but they say it
has to be effective robust interoperable
as much as possible so at this point I
to my awareness there is nothing that
the minute you say as much as possible
that you give away the store right so so
which is supposedly super strict so they
say is not strict enough right so let's
take interpretability um as much as
possible for an llm or as much as
possible for any technology state of so
whatever is available so it's it's there
is an obligation of transparency
obligation that is adding Provence signs
but then of course there is a chance for
the company to say the way the article
the provision is written the company can
say but there isn't anything that is
robust effective interoperable at this
point so that's it that's that's and and
that's the that's the point I'm trying
to emphasize I think it is correct there
is no
AI technology that is robust
interpretable and I forgot your third
criteria um there just isn't right now
and so one decision Society could take
would be okay fine come back to us when
something was R robust interpretable it
was the third one um inter interruptable
robust effective I read so many times so
robust okay robust effective and
interpretable so you can say look you do
one interoperable maybe I pronounce
wrong inter like interoperable
interpretable no inter that you can
transfer from one platform to the other
interoperable sorry my my bra operable
inter I would have like I would have
preferred interpretable interoperable no
interoperable well I would have put
interpretable there but I didn't get to
write the rules
um robust we'll stick with robust there
there's there are some AI techniques
that are robust like the GPS navigation
stuff but chatbots are not robust right
they just aren't no nobody has a robust
chatbot so as a society we could either
say fine give us whatever is the best of
the garbage that you have available and
we'll take it or Society could say no
none of this stuff meets what we mean by
robust come back later do some research
you guys are all research companies or
have big research Wings figure it out
and then come back and then we will give
you access to the market like Society
could make that decision it is not and
it is disturbing to me that it is not
but in principle we could do that so we
don't say for a drug give us the best
thing that you have for Alzheimer's and
we'll put it on the market we say if
none of the drugs for Alzheimer's are
actually you know causing more benefit
than harm then we just won't release one
yet and so we don't really have a drug
for Alzheimer's because nothing has met
that standard we have no rule that says
you can't have a drug for Alzheimer's we
all want to have a drug for Alzheimer's
but nobody's figured it out yet there's
there should be no rule that says you
can't have general purpose AI that might
be a wonderful thing but none of the
general purpose AI that people have
built to quote general purpose AI is
really that good it's all Jack of all
trades master of none and you know
there's a few uses for it but um it's
not clear that those uses really justify
the cost and the biggest use is probably
programming but we don't have good
long-term data about whether the code
that is written actually introduces Tech
technical debt so we're not quite sure
how much that's a benefit for society
certainly it helps with brainstorming
but how much benefit is the society um
that helps with brainstorming relative
to all the other risks that that we've
both talked about today you know it's
not so clear and one should have to make
that argument and let me let me push it
a little bit harder which is open AI
said that there were greater risks to
society from bioweapons from the new
model
01 right and the question should be
should 01 be available to society should
should they have Market access and the
United States right now the rule is they
want to access the market they can go
ahead there is no standard there and you
could say well how much does it elevate
the risk of a major bioweapons attack is
it 2% okay maybe we let that go because
we think the benefits of the 01 model
are so great I mean I might not but you
know somebody might whatever um what if
it were it increase the risk by
5,000% should we still just leave that
up to Sam Alman I mean it's literally
just up to Sam Alman if he wants to
release it because it will help him with
the next round of fundraising he can
there's nothing to stop him now in
Europe I don't know all of the rules but
I suspect probably he could there too
and you you can um tell me but I don't
think that he should right I don't think
that a technology that increased the
bioweapons risk by of let's say a major
attack of I don't know 10,000 people
dead let's say um I don't think you
should be able to release a technology
that that radically increases the
probability um of those kinds of attacks
or at least not unless you really
justify um the benefit so if you could
show that but it will also save you know
10 million people and here's why here's
how it's going to help with um you know
medical diagnosis which certainly could
be helped by 10 million people because
um doctors make mistakes and so forth
unless you could make that demonstration
I'd say okay let's not release it yet
like show us that the you know the net
benefit to society is enough to justify
the harm and if not we withhold Market
access that's what we should be doing so
just to be pessimistic here especially
as there are so many people from the US
in the audience uh and for those who
hold very high expectations from the EU
the a act doesn't do that and other laws
also don't do it so maybe it doesn't so
far is just we have hope so as someone
who is familiar very familiar so I teach
the a Act um so it's it's a nice it's
nice to have a risk-based approach so
they have prohibited AI practices they
have high risk they have transparency
risk so limited risk all the rest of
obligations for general purpose
obligations for general purpose with
systemic risk but when you zoom in and
you and well law I always say in my uh
lessons my classes that law is not like
philosophy right you you you have an
article so let's imagine you're the
lawmaker drafting a provision a legal
provision it's not like philosophy that
a good idea is good enough have to to
write it in a way that will be good that
will have good uh consequences so you
have to write it in a smart way so just
to give an example uh so so many
examples but just in terms of loophole
so if you look at the classification of
high risk is the most important category
in thex is probably being classif so
prohibited I have my criticism so the
the provisions in the the Article Five
that deals with prohibited AI practic
are very narrow so when the topics are
nice so for example AI manipulation is
prohibited and then you look at the
article you read you I every time I read
that that paragraph I say nobody's going
to be to fit this description so narrow
so specific just so many words and verbs
and in law when you you can you have a
choice right you can make you can draft
a provision that is broad that many
people can fit there or you can be
specific there are pros and cons for
each side but if you're very specific
and you put significant harm appreciably
impairing all those Wars they matter and
they are like a a a gift for the lawyer
of the company they would say uhuh I
don't appreciably impair the person's
decision making ability or it's not
significant harm it's a minor harm so
all those Wars they matter so we end up
with a very very narrow provision and I
I read that I said well people everybody
that is criticizing the EU they haven't
read the articles when you read
especially if you're if you have legal
training it's very narrow and as a
lawyer I can think as as a lawyer of the
company so I could handle it it's it's
it's badly written and and another
example so perhaps the most important
classification in the AI Act is high
risk so many articles they cover
obligations for a systems classified as
high- risk and on article six third
paragraph you have a major loophole so
even if you're classified as highrisk
even if you're mentioned in one of the
titles of annex three you can rely on
four one of the four conditions and say
hey I am classifi potentially classified
as high risk but I don't P significant
harm to fundamental rights and when you
read the conditions as a lawyer you say
wellow it should not have been written
that way so this is my my my side critic
you said you wanted to hear from me
about the AI act and for all those the
US here that had high expectations and
every time I post about anything a new
regulation some people come and they say
e Europe is so bad they're destroying
Innovation this regulation is going to
destroy the ecos I always think you
probably haven't read if you had read
you would say it's actually sometimes I
see what the FTC is doing the US
sometimes it's very powerful FTC has I'm
not sure now with the new Administration
but so far the FTC has been highly
effective with against yeah FTC has been
has been under lean con terrific we
we'll see what happens um new new uh
Administration as you say um usually on
any given call I think I deliver the
worst most depressing news but I think
you just beat me I mean it's very
depressing that the nominally best laws
around this are so easy to lawyer around
I'm not shocked to hear it let's be
hopeful I like to be optimistic I'm not
a person part so there are two hopes so
first there are future proofing
mechanisms in the this is great so there
are four future proofing mechanisms so
the Au can can try to fix and actually
for example they can amend Annex 3 so
this is one thing uh so there there's
going there are going to be revisions of
the AI act also the AI office they just
created this AI office it's very
interesting you Union level uh
governance organ that they are being
actually very proactive so I'm hopeful
that they maybe they will they also the
AI board the AI office they will issue
interpretations and they will kind of
fix it so that that can be but the way
now way as a lawyer Reading now it's
weak so everybody is so there is hype
there is AI hype and there is AI
regulation hype so wow the EU is
destroying the all those people that are
saying that they haven't read one by one
the Articles because when when you read
it and you think from a leg but you know
who has read the articles probably are
the lawyers for Google open AI Microsoft
they probably already found those Loops
they are happy they're probably very
happy so and I see law firms there are
so many law firms I see the the
marketing from the US they're saying you
think you're high risk maybe not come
for us let's think about about it maybe
you're not high risk let's discuss the
topic so it's a great business model I
see it's like it's like the doctors that
diagnose the bone spurs in Donald
Trump's feet that you know probably
didn't actually exist in order to keep
him from having to go to the Vietnam War
it's the same uh same kind of lawyering
exactly so let's uh move on now so we
spoke about Let's uh just want to hear
your opinions there's this big thing in
the US especially now now more in the U
I see it's having traction so regulation
versus Innovation right I see Yan leun
is one big uh he's the big he raises the
flag that EU is over regulating and I
read that letter so that I I wrote an
answer so they wrote this letter Europe
needs regulatory certainty on AI and
after reading it I understood that
especially So Meta my my I think meta
was the big leader that there it feels
very much like meta writing that so they
had two main problems first is gdpr so
basically it's a big and then we we are
moving to your what we go into the
specifics let me just insert one thing
um regulatory certainty is a good thing
for companies do you know where the
greatest point of of regulatory
uncertainty is now it's obviously the
United States we have no idea what the
Trump Administration is going to do it's
internally consistent it has you know
strong personalities that have conflicts
of interest we have no idea what the US
regulatory policy will be on AI two
years from now whereas we have a very
clear idea not complete uh with respect
to Europe and so like the argument that
Lon is giving is actually exactly
backwards with respect to the new
reality I just thought that was amusing
yeah so ju just a quick comment on that
and about this uh thing of innovation
and and I want to hear your opinion as
well so from my understanding that's
what what what concerns meta and
probably other big TCH so two things
first uh scraping right so in the EU we
have the gdpr and then to process
personal data and and we are scraping
from the internet you you must have a
lawful ground to process personal data
and if you're scraping there will
there's going to be some sort of
personal data mixed with the all the
internet data so they say you have to
have LW of ground so you could ask for
consent but meta doesn't like consent
they don't ask for advertising they're
not going to ask for training large
language models but they most companies
now are relying on legitimate interest
and there's a big topic now in Europe so
what are the so you have to pass a
three-part test to be to be able to
comply with legitimate interest and it's
still uncertain I would say we do not
that this balancing test is not well
established so most companies are not
doing right so this is one thing that
bothers meta so they wrote there that
data protection authorities keep
spreading uh they don't have the same
opinion but meta could do one thing
which is the same problem they have in
the advertising industry they could ask
for consent so they they made that you
heard the story right the opt out form
so if you are in Europe or other parts
of the world you can opt out of AI
training so it's opt out is not opting
so they say regulatory certainty there
is consent would be certainty perfect
certainty is really established in the
gdpr they could do that so they don't do
it so they they frame it as uncertainty
but just that because they don't want to
do the most certain uh lawful ground
which is conent this is one thing the
second one is the open source uh part
that open source if you're even if
you're open source if you're classified
as having systemic risk you have to
comply with some obligations meta
probably is sad about that they didn't
want to comply with extra things with
jamama so that that is this is my view
um so when you you you saw the letter
you've seen this movement towards not
just Yun but other Executives so what
are your thoughts why are they pushing
for this in Europe so they you think
that what do they want really because
there there is certainty there's the
problem is not lack of certain we have
gdpr we have a we have product liability
we will soon have ai liability so what
what's the real message you understand
Silicon Valley much more than I so what
what's going on here I mean I I'm not
sure I followed all of the details there
went by a little bit fast I think in
general meta wants to be able to protect
its rights to sell your data and to
train on your data and they will do
whatever they can in order to protect
those two rights and and that means
doing sneaky things like opt out rather
than opt in so that you know as many
people as possible sign up for the for
the kinds of things that they're doing
um there's some subtlety to the argument
there that I think I didn't get but
those are in general their their two
motivations and I mean they're highly
dependent um on selling your data and
and they haven't really made a lot of
money from training large language
models they have made a lot of money
from being able to Target stuff to you
based on training other kinds of AI
models um and they are under the um
assumption that they're going to be able
to you know monetize these AI model
these generative AI models one way or
another and they want to um you know do
everything they can to protect those and
and what's your your personal view on
this debate on AI regulation versus you
know I don't think it exist in in in our
privacy
Community say the I mean the general um
what's the word that I'm looking for
like dichotomy false dichotomy that
Silicon Valley has been trying to set up
is the idea that regulation and
Innovation are necessarily inherently
incompatible which is stupid because you
know sometimes regulation actually
inspires Innovation so regulation around
emissions inspires electric cars and
regulation around safety in cars
inspires seat belts and airbags and so
forth so you know sometimes regulation
actually inspires the innovation in fact
I wrote an essay about a year ago about
China's regulation um and said that it
would be a great irony of History if
their crazy regulation um that says that
all of their generative AI has to be
compatible with what the party believes
um actually inspired advances in AI so
because in the US you can spout whatever
misinformation you want without
consequence you can use llms which are
not reliable um and don't stick to
anything right like I laughed when I saw
that um Apple has in their system prompt
do not hallucinate because this system
doesn't can't comply with it um so China
is forcing their Developers for the
public use not the commercial use but
for the the consumer facing use to make
their llms or might be something else uh
compatible with the part what the party
says that's actually extremely difficult
technically and um I think as as I know
they still don't have consumer facing
llms because people can't meet that
standard because of course people can't
guarantee that any given llm will do any
particular thing on any occasion um
Emily Bender's stochastic parrot phrase
kind of applies and if you're stochastic
parrot you might say anything at any
given time and and so um that may Force
um this is would be an example of a
regulation possibly inspiring the
greatest innovation in AI history it
might Inspire someone to build an AI
that could actually follow directions
which most you know no generative AI
really can do and so that would be a
kind of crazy irony of history but it
for now that's just a thought experiment
but it it shows how completely wrong the
notion is that um regulation and
Innovation are are incompatible um or
I'll give you just one more example like
Airlines like regulation around safety
is part of why people are willing to fly
consumer airplanes so it has inspired
any number of Innovations around like
maintenance to make sure that you know
the maintenance is done properly and
nobody wants to get on an airline that
has like even like I don't know two
crashes a year like if if some Airline
had two crashes a year because they
weren't following the regulations and
and had lack maintenance or whatever um
or lack policies around when people
would fly planes nobody would fly that
Airline like if you have one accident in
four years people are like I don't know
if I want to be on those guys right you
know there were I guess now there have
been three accidents on the Boeing m Max
flight and people are like I don't know
if I want to take a boing Max even
though statistically it's still even the
max 737 Max is probably much safer than
driving your own car or whatever but um
because of the Dynamics of how these
things are reported and so forth no
Airline can afford in fact to violate
the safety regulations is extremely rare
that any of them do and so Innovation
around safety has been inspired by
regulation this dichotomy is just one of
the most absurd things I've ever heard
but it is part
of um mid 21st century style to say
absurd claims to promote them on the
Internet and eventually people believe
them and so that's where we are just
what that you brought regulation and I
saw in your book that you're really
interested in this topic but actually
from from a legal perspective that's the
challenge of Regulation because you
mentioned Aviation we can think about
cigarettes right now before cigarettes
were incentivized doctor said it's
actually great for stress great for
children teachers smoking then now we
have the packaging so all this and taxes
and so on but then we have sugar right
sugar is basically almost unregulated we
have just the labeling sometimes is red
the amount of sugar that's the big
challenge but it's not an easy it's it's
I I love the debate in as someone uh
that went to law school so I I love that
but then who what's the analogy so large
language models are they like the
cigarette like heroine like sugar like
the plane that that's where the fight
will be right they're like heroin
they're like heroin in the sense that
people got addicted to them despite the
downsides I I'll go with that okay that
that's a hard one girl okay so so so
maybe they we should have Z no no llms
be only in the in the dark markets
there no I mean I wouldn't go that far
but I mean they they do have an
addictive property and I think people do
underestimate the downsides and they
ignore downsides like the cost of the
environment and so forth um it's not
clear to me that there's a overall net
benefit to Society of llms but most
people aren't going to want to give them
up um
I mean you know heroin has its own set
of problems llms it's a different set of
problems right like um the cost to
democracy of living in environment where
nobody trusts anything is pretty high
it's hard to put a number on it's not
the same as you know number of deaths
per year caused by cigarettes which you
can try to estimate but you know we have
we have turned a corner in terms of
society's trust and llms are
contributing to that they're not the
only factor I think social media has so
far been a larger Factor but l LMS
because they can automate the generation
of misinformation are definitely
undermining trust in society and that's
a cost it's an immense cost that's hard
to you know quantify but is a serious
cost for society um and then there are
other costs that they're also a little
bit invisible because people don't like
to report them so um cyber attacks are
clearly also being escalated by llms
that can participate in that chain in a
number of different ways um tricking
people out of passports uh p passwords
and um also finding zero day
vulnerabilities and so forth and so
forth um the companies that get attacked
by cyber crime don't like to report it
because it hurts their brand that they
were attacked and so we don't have
accurate numbers but when I I've talk
informally to Chief Information officers
and so forth they all say that you know
the costs of cyber crime are going up
and the probability or the you know
regularity of it's going up and so forth
so there are all these hidden costs it's
it's um in some ways it's like when
uh different um manufacturers used to
dump chemicals in the water and so
nobody noticed it for a while um there
were big costs uh but you know they're
sort of behind the scenes there's a lot
of behind the scenes costs of llms that
I think actually are quite
serious and before we finish I want to
to talk about the topics that what we
shooting SE Stone I want us we have two
I hope you can say five more minutes G
then we I want to share with the
audience the topics that you think we
should insist on and I wanted to your
final message to the audience so you
finish you you're you're actually very
optimistic in your book I like the way
you finish so you'll you Empower people
so what what you can do and I think it's
nice to be optimistic and think we can
do something so it's better for our
mental sanity so this is what uh Gary
says in his book so what we should
insist on in terms of uh important
rights and I want to hear your opinion
so we did not have a lot of time so very
quick I just want to hear your opinion
on data right so I read everything and I
want we are quickly discussing copyright
so data rights which is related to
copyright privacy transparency and
transparency his breaks down into four
pieces so data set transparency
algorithmic transparency Provident
science environment and Labor uh
corporate risks liability AI literacy
independent oversight oversight in
layers incentivizing good AI agile
governance and AI agency International
AI governance and Research into
genuinely trustworthy AI so this is you
I invite you to get Gary's book and read
of everything that he writes in all very
interesting so I would say this is a
really great overview of what's going on
now so if you if you read a book to
understand what's going on and and what
we should insist on and the most
important uh also policy related parts
or ethical parts and and legal part
Gary's book it's excellent and on data
right specifically so you've been
following AI copyright also try there
was just one recently in India a new one
so they keep coming H and from book
authors from creators from news media
companies so from different sources so
basically everyone that owns copyright
is suing AI companies what are your
thoughts what's the way out so there we
have been now the newest law they have
been coming up with ideas so for news
media companies we see more and more
licensing right they want to to have
licensing deals it looks like news
companies they are in bad financial
situation so licensing deals would be
great for them but for creators and book
writers it would be different so what
are your thoughts what what would be the
best way out for copyright
issues I mean I think licensing is the
answer here um no I forgot sorry I
forgot to mention so licensing and and
and and being compensated by out to
Output so every time your your the
output mentions your work so those are
the main yeah I mean the problem there
is technical right so um like I
mentioned in the book some work I did
with read South and where uh we gave
prompts to systems like mid journey and
and um open AI uh do things like Italian
plumber and the system would come back
and give you the Mario character from
Nintendo and this is an obvious
copyright violation but what's
interesting is the systems can't
actually do attribution they can't say I
got this from Mario and so this is a
part of the problem with the licensing
is the current technology that we're
using which is llms and uh Technologies
like stable diffusion what what they do
is they explode the information that
they've seen into billions or trillions
of little bits of information and then
they reconstruct those billions or
trillions of of bits of or quad whatever
it is um bits of information into
statistically probable outcomes that
gives you your hallucinations because
statistically probable is defined in
terms of co-occurrence of words and
stuff like that um and it also leads to
occasional plagiarism doesn't mean that
you get plagiarism every time but the
plagiarist thing is the most
statistically probable output so the
most statistically probable output for
Italian plumber is Maria um but along
the way of breaking that up into the
little bits of information the systems
actually lose track of which information
is which because of the ways in which
they basically do superposition I guess
would be one one term for it of of these
little bits of information and so um the
right licensing deals should really
actually do attribution on a per output
basis so this particular ular output
draws from you know this character and
this character or these lines of text or
whatever and there should be payment for
that um current technology does not
support that there may be some future
technology that does and I think in some
ways the companies would actually be
happy with that like if they had a clear
answer that everybody could kind of say
yeah that's fair that would be better
for them than just sort of getting sued
left and right and being hated and so
forth and then they could just mark it
off as a cost of business but again
because of technical inadequacies of
this technology we can't actually do
that yet so what people are instead
doing is they're licensing the inputs
rather than the outputs of the system so
I use so many documents and we'll we'll
charge you X dollars um I do think some
form of Licensing is the answer in the
end what some of the companies are
trying to do is just theft they just
want to say well we can't build our
models unless we steal it and that's not
true they can pay for it they can
license it and they have lots of money
um and we should insist that they do it
the historical parallel here is Napster
right for a minute Napster was stealing
music left and right and you had people
saying well information wants to be free
we shouldn't have to pay for this and
the court said no we have copyright laws
and you will pay for it and we sorted it
out now we have streaming and and people
pay a monthly fee and it's all fine and
and we will wind up with something
similar I think there are there
are we there's every reason in the world
to compensate the artists and writers
and so forth who are being ripped off
but that's actually one of the easier
questions you know harder questions are
like how do you handle defamation how do
you handle bias um and especially like
what do we do about misinformation it's
a really complicated question here with
respect to the creators we should pay
them let's go figure it
out thank you y and what's now we are
over our time so what's your final
message an optimistic message let's see
there are people here that want to do
something they want to help make sure
that AI will work for us so what what's
your message your your optimistic
positive happy message I'm kidding
doesn't need to be happy so what's your
message to the audience that wants to do
something I will couch it as a happy
message we could as a society if we
chose to organize and boycott generative
AI until it worked better until it
stopped stealing stopped polluting the
environment stopped um uh discrimination
against people who aren't white males uh
stop hallucinating stop defaming people
or reduce those to some you know more
reasonable level than they do now we
could organize a society it doesn't
often happen but there are moments in
history where a lot of people have
organized and thought something that was
unfair Gandhi salt marches the nessle uh
uh formula boycott and so forth we could
say as a society even though all the
money and all the power is with the
government and and the tech oligarchs I
can't believe we didn't use that phrase
today because that is driving a lot of
this you know we could say no to the
tech oligarchs because what matters to
them is money and the way that they get
money is they show these statistics that
show Rising users that's how they
justify the the huge Venture Capital
valuations as they say their users are
going so if we all sat around read my
book realized how awful um things are
going to get if we don't take action we
could together work together boycott
this stuff for a year or two and get
them to make something better and more
fair and so forth the world would be a
better place so that's the optimistic
thought thank you very much Gary thank
you so much for coming and where people
can find you so now you're not not on
Twitter or still on Twitter what's the
the final I hoping not to return to
Twitter except to occasionally post
notes saying I'm having a great time in
Blue Sky you can find me there so you
can find me at Blue Sky and you can find
me at substack I guess and Linkedin
those are the three places that I am
most often I also trying threads and
Mast it on and so forth but Blue Sky
substack um and Linkedin the best places
to find me thank you so much Gary it was
a it was such an an excellent session I
think the audience so you see I don't
know if you if you could read what they
were writing so all the time I I
couldn't see it at last but um but I
could tell it was a wonderful interview
because I had a really good time so
thanks a lot pleasure thank you so much
Gary thank you to the audience for being
always so nice and and active and
participating and making everyone uh
feel part of it h and if you want to
stay stay informed so don't forget to
subscribe to my newsletter so December
we are meeting again for the next live
talk and if you want to join the
December training uh register so there
are few spots left so thank you every
much thank you Gary thank you everyone
and I see you in December byebye
everyone byebye
