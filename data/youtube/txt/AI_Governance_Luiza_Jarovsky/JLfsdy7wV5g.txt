hello everyone welcome to the Edition
number 21 of our AI governance live talk
today I'm here with eoma and junma thank
you welcome EA so thank you for
accepting my
invitation thank you so much it's a
pleasure to be here with
you my pleasure is mine by the way we're
going to talk about E's book so is this
one oops oh it's it's blurring so the
Quantified worker if you haven't read it
you should it's it was the 14th book we
recommended in the AI book club so we're
going to discuss the book today so so
many of the topics we are covering so
she uh covered it in depth in her book
um so ifom she is an awardwinning law
professor at the Emory School of Law
she's also the founding director of the
AI and future of work program she's a
renowned expert in ethical governance of
workplace technology and she's also the
the author of The Quantified worker
which if you haven't read yet you should
um so welcome for by the way I'm so
happy that the audience already knows
how it works so you're welcome to share
where you're connecting from so let's
take a look at some of the places so I
see antp Switzerland Chicago uh London
the Netherlands uh USA Long Island South
Africa Mexico Portugal So you you're
welcome to keep using the chat to share
your thoughts so
it's it's it's great when it's
interactive so I see more people so
Germany us you can keep sharing
W feel free to share where you're
connecting from and if you have
questions if you have comments it's a
great idea to share them in the chat um
I'll see in the chat
okay yes so uh two quick reminders
before we start with with the topics so
if you want to stay up to date with
everything related to AI policy
compliance and regulations so don't
forget to subscribe to my newsletter
there are already I think 42,000
subscribers so Louisa newsletter. comom
and if you want to join the January
cohort so the 16th cohort of my AI
governance training uh don't forget to
save your spot um so first I I always
like to start with something more
personal about your story so why did you
decide to to research this topic so your
your scholar your an awarded scholar in
this area so why law and technology in
the workplace so how did the the the
whole thing began began so was there a
specific moment in your uh legal Journey
where where you realize that's the thing
I'm must study in depth so how how was
this this meeting right so you know this
thing has been such a long journey um it
actually started when I was a graduate
student at Columbia University in New
York City and uh when I started my
journey there I was a sociologist uh you
know sociology student uh studying for
my doctoral degree and my focus actually
was on looking at uh individuals who had
exited prison and who were entering
society and entering the workforce but
as I as I was interviewing them um it
just came up over and over again they
felt essentially that they were being
kept out of the workforce not just by
you know laws and issues like that but
by the fact that many companies had
turned to automated hiring AI hiring
which could very easily identify who had
been in prison or who had taken out long
um periods of time away from the
Workforce and essentially exclude those
people and that was very eye openening
for me to hear this so even though I
wrote my dissertation focused on
re-entry organizations that question of
oh is AI hiring actually helping or
hurting that question just stayed with
me and it prompted me to do more
research in this area to see is is is is
there an actual legal issue with AI
hiring and if so like how should we
amarate and so that what actually then
led to the book which took about six
years uh to write we can see that so
it's such an a detailed book so all
possible sources are there in example so
it's such a excellent book to to learn
more about the topic um about so right
about the book so I want to start uh
talking about the book uh some of the
one of the parts of the book you you
discuss the mechanical manager and it's
such an interesting part in detail then
you give many examp examples and I think
for the audience it would be excellent
so many people haven't read it or
they're not familiar with workers Sur
villain or what can go wrong so it's
just okay it's just AI software so
everybody's using it why why is it bad
so why cannot if all my competitors are
using it why I shouldn't use it I'm
going to lower my cost and so on so I'm
going to site some of the the the sub
items in in this part two and then maybe
you could share some of the most
interesting examples that to glorify
especially those in the audience that
don't know why it could be potentially
harmful so she she cites in her books so
she brings automated hiring and
discrimination personality job tests
automated video interviews the unbounded
workplace and worker surveillance
workplace Wellness programs
telecommuting and health surveillance in
the covid-19 era so what would be I I I
would say eye U eye opening examples
that would maybe convince the audience
or show the audience that those
Technologies can be harmful yeah that's
such an important question um you know
one of the uh chapters in the book talks
about automated uh hiring and the
Discrimination that actually can be
associated with it but I start with the
chapter right noting that it's actually
a paradox a lot of times um you know
Business Leaders they're turning to
automated hiring as something that they
see as more fair right something that
they see as less biased than human
managers um but the unfortunate thing is
that the bias becomes baked into the
automated uh systems it it comes baked
into the AI therefore you know tring to
automated hiring doesn't actually
eliminate the bias so I I'll just give
an example of that I also share in the
book and it's called the the story of
the Jaren um and what happened there was
that a company was planning to roll out
automated hiring um for all its you know
branches but this company was smart
enough to hire a lawyer to do an audit
before they you know did this big Ru out
and essentially what the lawyer did was
set up an audit to answer the question
taking all the training data right which
is like the resum√©s of people that work
in that company if we feed it into this
machine into this AI machine what is it
going to conclude are the two most
important factors for what we should use
to look for applicant right for what we
should look to select candidates well
once they did that the results were that
the candidate should be named Jared and
also that the candidate had um played
High School
lacrosse right so those were essentially
the two factors that the machine would
then be using to select people for that
company and when you think about that is
is that really predictive so so we tend
to think of AI as prediction M machines
right we think of if you're using AI for
hiring then supposedly it's predicting
who's going to be a good worker for your
company but in that instance what is
actually being shown is that AI is not
so much a prediction machine as it is a
matching machine it is matching and it's
sort of like holding up a mirror to
things that have already been done in
the past and it's replicating them so
why did a the the AI machine choose
Jared and you know someone who had
played high school acros that's because
that company had hired a lot of people
named Jared and who had played High
School across right so you I think any
sort of logical person could say well
that does not really have any
correlation or or more importantly
causation for you know job performance
but the AI machine
because that was what has happened in
the past is reflecting that back and now
you might also be asking yourself well
that seems innocuous right someone named
Jared someone who had played High School
lacrosse you know that that seems okay
because it's not like you're using race
it's not like you're using uh you know
gender right the protected categories
well let's break it down who's named
Jared right I think logically we can
also think well if you're you know an
American you sort of logically know
people named Jared are usually
male but you don't even have to just use
anecdota you can actually look at the
Social Security uh statistics for who uh
is matched to certain names so the
Social Security Administration actually
um collects statistics for which names
are male or more predominantly female
and the name Jared is over 90%
associated with
males so already just with that name
you're you're actually having a proxy
variable for gender okay once again if
you look at the same Social Security
Administration statistics they match
certain names um to different racial
categories and once again the name Jared
is also 90% And More going to be
correlated to a white person so already
you see that the name Jared is actually
a proxy variable for a white male okay
but what about High School lacrosse what
could possibly be wrong with that right
you could say you know Business Leaders
when they're picking High School
lacrosse is because they want somebody
who's a team player right that's a team
sport if you're team player you're going
to be a good worker because it means you
can work with lots of different people
you're going to be corporated
well the problem with that is it's
specifically High School lacrosse
there's lots of other team sports right
there's basketball there's football why
not those are the theme Sports so when
you break it down once again High School
lacrosse is a sport that's played in
high schools that are affluent right
high schools that are located in wealthy
neighborhoods because the way that the
American system is set up your high
school gets money based on the property
taxes of the houses around the high
school so if the if the high school is
in an affluent neighborhood then that
high school is going to get more money
from um the taxes so once again you now
see the socioeconomic status
implicated right so in that way using an
AI hiring system can actually hide that
there is discrimination happening right
because it can use these variables that
are appr proxy variables for you know
protected uh characteristics or
protected categories but on its face it
can look very innocuous while still
really you know enacting the same
discrimination that we want to uh
avoid and it's interesting that so the
the legal instrument usually so we have
you mentioned a few times so this
concept of protected categories so we
have race gender and other categories
but the challenge with AI it's that what
what you mentioned a few times also that
it's by proxy so it's more is not it
will not mention because it's from a
certain race or a certain gender but
through proxis and through the how AI
training works right so everything as
you mentioned B into the system
so and and then we can think about the
policy and Regulatory challenges so we
traditional instruments such or or legal
Concepts that just protect the
categories they will not might not work
so well when exactly exactly exactly
involved and you're what the story that
you you this this case that you brought
it's similar to the Amazon one you you
you're probably familiar with that right
very very simple The Good the good point
of Amazon something positive is that
they decided to not use the tool but for
those in the audience so Amazon was
developing some sort of AI algorithm to
to help with the hiring process and then
they realized that for the same CV
everything the same when there was a
male name or typically male name uh the
system was recommended the male so the
only difference was one had a typical
female name and the other CV had a
typical male name everything else the
same and the system was recommending the
male the CV from the the typically male
uh name so why that was happening just
because the majority of the engineers
that was specifically related to
engineering were male so that was and
then they the good good part is that
they made that public and they rais
awareness about what was happening but
but it's such a it looks like uh it's a
common occurrence right right and as as
we see more and more and some companies
so they have they receive thousands of
CVS so we already expect that they
probably or we we hope not but we uh
think that they are probably using some
sort of AI uh system to to to to select
and and just a side comment before we
continue so actually so in my training
sometimes I meet one on one with the
candidates and with the participants and
we discuss career and AI governance
career and sometimes I I tell them uh if
I were you if I were looking for a job I
would have in mind that potentially
there's an AI looking at my CV so I I I
would think not only about the recruiter
how does the recruiter would look at me
but is there so from a code perspective
right so is there any war that I should
avoid so strategically speaking so and
do you also think that way so is it a
right approach or or it is absolutely
the right approach so I think you know
the reality of the world we live in is
that a lot of businesses um a lot of
Corporations are turning to these AI
hiring tools so you do sort of have to
study how these tools work um and I say
that because I actually have yet another
story that illustrates that you can
be extremely qualified in fact you can
be
overqualified and still not be picked if
you don't tailor your resume or tailor
your CV to exactly what these AI systems
are looking for I mean it's it's very
unfair it's very sad um and but that is
why you you do need to know how these AI
systems work so the story is that um you
know this is something that happened
when I was still teaching at Cornell and
so this story was shared to to me by one
of my students this student had applied
for a job right with a corporation was
denied right like
rejected they went to a job fair right
and they met a representative from the
corporation they chatted with this
representative talked about their
background Etc and then the
representative was like oh your sound so
amazing you should apply for our company
and the student said but I did apply and
I was rejected and the representative
said oh no that that can be right
because you sound perfect for us and you
know the representative said send me
your resume directly send it to me and
I'll find out what happened do you know
what the issue was so the student St had
was a a PhD student in computer science
um at you know Cornell information
science which is one of the top programs
in the country actually and uh the but
prior to that the student had gotten an
MA in computer science and it turns out
that the way they programmed the AI
hiring system it was to look for people
who had at least an
MS so literally one letter because the
student had an MA in computer science as
opposed to an MS even though the student
was now a doctoral
student that meant they they could not
you know progress because the the system
was intent on looking for that s so
literally one letter off um so that just
illustrates to you that you know the way
the system is designed the way the
system is programmed can be not very
sophisticated
sometimes um or can have like blind
spots even if it's so super
sophisticated it might not just be able
to deal with all the contingencies of
life so you do have to be a little bit
Vigilant if you are applying for jobs uh
because these AI systems are designed a
certain way they programmed a certain
way and you sort of have to learn you
know what's working in your field and be
able to sort of indicate that so imagine
if that student knowing that that's an
issue if that student had put on her
resume I don't have an M Ms but I am now
a doctoral student so just the fact that
she put that I don't have an MS would
still progress her resume um to at least
be reviewed so that they can see that
she actually is now at an advanced
degree than someone who has an an MS
it's it's just incredibly it's it's
outstanding and and I think that from a
brother maybe philosophical perspective
or extrapolating from that so uh AI is
shaping us so when we could see that
even from social media right so it
prioritizes engagement so we see people
posting things that maybe they wouldn't
tell that face to face with the person
then they say they get more attention so
kind of the the social media algorithm
kinds of shaped our discourse and
political discourse
gets now we see that the way we apply
for a job or the way we behave it's
being shaped not by what real values or
or principles are but what we think that
the how how we can please the AI right
so right right in a way we are already
being conditioned or controlled by it
even yeah we had certainly being
conditioned and it is worsome right um
it is worsome you know that sort of
conditioning um and I think it does
behove Business Leaders right as they
adopting Technologies to think about
what is sort of like the unintended
consequences that could potentially flow
from this adoption of technology I think
a lot of times the is a bandwagon effect
to adopting Technologies where um you
know Business Leaders are thinking well
that company has adopted that technology
if I don't do it then my company is
going to fall behind etc etc but I would
just caution Business Leaders right to
really think about before adopting any
technology is this technology
appropriate for for my organization is
it even appropriate for my field and
then furthermore um as I'm adopting this
technology ology what sort of safeguards
right what sort of guard rails am I
putting in place just to make sure that
I'm not getting the unintended you know
consequences that are you know more
delerious and not what I want to happen
um I think those considerations are
quite important you kind of anticipated
my my next question so last my last
guest was Gary Marcus and we spoke a lot
about AI hype and the public discourse
around Ai and politics and silicon VI
and when we pay attention to the to the
discourse the mainstream discourse it is
like this so AI will increase
productivity we reduce costs it's
incredible we are increasing computing
power and that's the future this is the
mainstream so we we should do it because
it's good for capitalism it's good for
reducing cost and that's what why it
that's what capitalism is all about and
what you just started that conversation
now but what would be strong
counterarguments so let's as a business
owner or as entrepreneur or a CEO so you
have to please your stakeholders
investors so following capitalism's
logic you should adopt whatever will
make you reduce cost and increase profit
yeah what would be strong principles or
ethical guidelines that that you should
take into consideration so before let's
imagine exactly the example you gave so
and I'm sure we have business owners in
this audience or employers people who
are maybe taking care of uh human
resources department so what would be uh
I would say main the I don't know two or
three main principles that they should
think about before going uh full into it
full speed into the the the novelty or
the the efficiency or or reduction of
cost what would be very important
elements that they should think about
right so I I I actually want to use a
different type of technology to really
illustrate this lesson um thus far we've
been talking about hiring but a lot of
times um AI Technologies are adopted in
the service of um worker surveillance
um you know so to use a more neutral
word you know worker monitoring right
and as a business owner of course you
have an interest to monitor your workers
to make sure that they are being
productive but also to prevent
wrongdoing misconduct in the workplace
because that's something that you would
be answerable for right under the law on
the respondent superior uh if your
workers are you know conducting
themselves in a way that's um illegal or
uh you know hurting someone else but you
know one big consideration is that so
many empirical Studies have shown that
over surveillance of workers actually
has the opposite effect which is it
makes workers less productive it makes
workers less creative and it actually
serves um to drive
wrongdoing uh even more covert so to
speak right because if people know
they're being watched then they are
going to be a little a lot more covert
in terms of what they're doing so you
may not you know you may be even less
able to find wrongdoing if that makes
sense so so I think Business Leaders
should think you know always ask
themselves if I'm adopting Technologies
to watch over my workers um if I'm
adopting technology that is intrusively
right always looking over my worker
should
what message am I sending to the worker
right am I creating a relationship of
trust am I creating a relationship where
you know the worker can be themselves um
which frankly has been shown to allow
workers to be the most productive
because they're not doing the extra work
of hiding who they are they're not doing
the extra work of looking over their
shoulder right they just focus on the
job um so I do think those
considerations are very important um
also just from a human rights point of
view right do we want a world where you
know human beings are essentially um
expected to perform like machines right
where you can't have like little breaks
you can't um work at your own pace you
lose autonomy you have to do the work
exactly as described you can't you know
take shortcuts that might work better
for you you can't do something called
practical drift which is figure out for
yourself how to do things but you have
to do everything by the book right from
a human rights point of view that is a
terrible world right we would then all
feel sort of like surfs we're we're
essentially digital surfs at that point
or you know even worse we would feel
like machines we would be cogs in the
machine so I do think that in adopting
any technology we have to ask ourselves
these bigger questions um and we can't
buy into the hype of just oh everybody
is adopting this technology or you know
the claims this technology is just going
to quadruple your productivity of your
workers like you have to actually look
at the scientific evidence out there um
and the scientific evidence is actually
quite clear on what does increase
productivity um and it isn't necessarily
watching your workers 247 it's actually
having workers feel like they have a buy
in into what they doing feeling like
they have some sort of sense of Pride
feeling like they have some measure of
autonomy feeling like they have some
element of creativity that makes the
work fun for them um so that's this is
my advice to Business Leaders um out
there I love how when you were sharing
our event today you mentioned so I loved
how how the wording that you use so
usually we think about workers rights or
or workers dignity and you went much
Beyond you said how we can make sure
that workers can we can support workers
creativity right and and wellness so I
love that you you you use that that type
of wording so much Beyond sometimes they
strictly legal and just on the worker
surveillance part I think it's uh there
are so many people in this audience uh
from the the Privacy field so they're
thinking about privacy aspects in data
protection so last year I had uh my
guest one of my guests was Professor
Nita farahani and and talks about
cognitive freedom and and neurot
Technologies to monitor workers this is
I would remember the first time I
learned about this type of Technology
was she made a short uh video for her
book uh and and then I watched it and I
was so shocked so some it looks like in
some countries it's common practice so
for those in the audience so you kind of
have those neurotechnology devices so it
can be more or less sometimes it's
really like a cap with with NE brain
sensor sometimes it's kind of this guys
does some sort of Wellness right so
let's let's check that you're let's add
some wearable devices to check that
you're you're feeling good that you're
sleeping well you're sleeping enough
hours and then what goes behind it and
what can happen and it will be difficult
to check what exactly is the employer
checking is so the the employer can
actually monitor much more than that so
I'd love to hear your thoughts on on
that that type of invasive neurot
technology if you have right so yeah I
mean I think we we are seeing more and
more of I guess a societal
acceptance right um of uh I guess a
human uh machine
collaboration that is a lot more
invasive for the human right so I say
that because we have had human machine
collaborations for quite some time I
mean look at what we're doing now we're
speaking into a computer right that is a
human machine collaboration but the
difference here is the machine is
separate from me right I can get up and
walk away and the Machine stays is there
but I think as a society we are sort of
becoming more and more comfortable with
the idea that the machine should always
be with you right starting with like
your cell phone right like my cell phone
is never like I think more than one feet
two feet away from me and I think that's
this is the same for all of us so in
some ways we can be classified as Cy
bogs because we now always have a
machine sort of attached to us um in the
case of workers what I'm alarmingly
seeing is that this is actually becoming
more literal right so instead of like oh
yes my you know my cell phone is right
next to me there's Now sort of a push to
actually attach the machine to the
worker so for example um Amazon actually
has a patent and I talk about this in
the book they have a patent for a
bracelet for workers to wear this
bracelet would um provide what is called
hop tip
feedback now they describe this as like
a tingling
sensation right but you you might ask
yourself this sounds a little bit like a
shock color like is this a shock color
is this how how painful is this tingling
sensation right like how intense is this
and the idea is that this bracelet would
guide workers in the workplace when they
are um fulfilling orders so they can
know if they're reaching in to the right
bin or the wrong bin Etc and and so
you're seeing sort of like a a a level
of direct control over the worker that's
that that is a tad bit more invasive
than what we have now and also Amazon
also has another patent for a
cage uh it's described as a worker
protection cage I also talk about it in
the book and the workers would be inside
the cage and the idea is that they would
be inside the cage cage to be working
with very large robots and that this
cage would protect them from the robots
but you're still asking yourself this is
a human being in a cage right so so I am
worried right that we are and this also
goes I think to a lot of the uh stuff
that uh Professor Nita faran speaks to
that we are sort of as a society
becoming too
comfortable right with the idea of
greater and greater and greater worker
control right of this sort of like um
very direct um you know bodily control
of the worker um which includes you know
the the worker's mind um and I think
that is that is very concerning um you
can also see this play out in the
philosophy of Personality job tests I
know that was something you referred to
earlier and I have a whole chapter in
the book dedicated to this uh
personality job test because I think
they are quite indeed um a vehicle for
discrimination um so in the book I talk
about a story of um you know a a young
man named uh uh uh Bean right so
beam uh was a college student when he
was diagnosed with bipolar disorder
right um also known as manic depression
um beam was was brilliant he had gotten
a perfect ACT score but because of his
diagnosis he took time away from school
went home you know started to get better
he wasn't quite ready to go back to
school but he wanted to be productive he
wanted to be uh you know earning money
he went and he applied to bag groceries
right this is somebody remember that was
in college had gotten a perfect ACT
score he's applying to bag groceries he
kept getting denied he kept getting
rejected he was rejected at so many
different Grocery Stores um fortunately
for beam Kyle beam his father R Roland
beam is an attorney and I've met him
actually so Kyle talks to his father and
his father says you keep getting denied
for jobs I don't understand these are
entrylevel jobs you obviously are very
brilliant and Kyle says yes you know
they at each of these jobs they had me
fill out an application and some of the
the questions were asking about my
personality and they seem to be similar
to the questions that I was asked when I
was in the
hospital so basically some of these
personality job tests had taken
questions from the
dsm5 which is a manual used to diagnose
mental
illness so they were basically using
this personality job test as a way to
figure out who had a mental illness and
is exclude that person from the
workplace which is actually very much
illegal in the United States right it's
against the Americans with Disabilities
Act because even if someone has a mental
illness they can still contribute right
as a productive member of society and
they still should be um allowed access
to the workplace so this idea that we
want to control everything about the
worker including their personality is
driving discrimination as well
and I would say that specifically about
the so when we talk about surveillance
it's it's bad and it's when we talk
about the work relationship we have the
power of symmetry right there is this
extra element also when we talk about
parents and children or abusive
relationships and other situations that
we can see Power symmetry but the work
relationship is the typical relationship
where we have power of symmetry so one
side is the worker who has to usually in
a so if you're talking about the CEO
versus any regular worker so the person
has the bills to pay so it's more
interested it's really needs the
sometimes really needs the money so it
will accept whatever is being offered so
it will consent or we'll say yes I'm
happy right the typical the lawyer you
write the consent form are you
comfortable with and you'll say yes I'm
comfortable whatever the the boss says
that I have to sign I will sign so we
can even question if this should so when
we are thinking about policy and
regulation so that that is a concern
right that we should keep having so is
it should we use consent as an
instrument when we're deciding where to
use it or not and and we'll talk about
soon we'll talk about the AI act and
some of the provisions but I think it's
also an import that from your field so
this is when you talk about surveillance
is bad in general but especially when we
have those types of power symmetries
right yeah so the P power symmetry issue
is actually I think one of the biggest
issues when it comes to regulating AI um
because think about it if you are
regulating AI for consumers right
there's a difference there because as a
consumer consumer you can choose not to
use certain products you can just say
well that product um is not uh going to
respect my privacy so no no thanks but
as a worker uh that becomes a lot more
sticky right because we all have to work
for a living right uh well 99% of us
have to work for a living so if your
employer says oh can you sign this
consent for us to rule out you know
these machines and the workplace you're
going to feel a pressure to do this in a
way that you don't for a consumer
product because that's literally your
access to a livelihood right um so uh
you know professors like Dan solov has
talked about something called a murky
consent which is that yes sometimes
you're giving consent but for you know
for for one you're not giving a fully
informed consent right because it's not
like the workers know exactly how these
things are going to actually work right
the employer usually has more
information about how these things will
work in the workplace these AI systems
will work then the worker ever will so
there's that and then for for for
another your consent might still be
murky because what you consent to today
might have a different resonance
tomorrow right so for example I can
consent to having a video of myself
taken without realizing that in a few
you know years time that video could be
automated right that video could be used
to create an avatar of me right and and
so the the the fact that the technology
is still ever
evolving right really makes all content
it really makes all consent like murky
because you can still be consenting to
something that you don't fully
understand correct perfect and I want to
bring a feel of the provisions of the a
act so in my AI governance training we
discussed the AI act in depth and and
there are few mentions of workplace and
and and and Provisions involving workers
but I wanted to bring two of them that
involve one prohibited AI system and one
uh category of high-risk assistant so to
to hear your thoughts and and maybe also
an American perspective what would be a
us perspective of the same type of
provision in your thoughts uh so one of
the prohibited Article Five of the AI
act has prohibited AI practice so those
practices no they cannot be offered in
the those types of AI systems they
cannot be offered and and this is one of
the items I'm going to read it the
placement on the market the putting into
service for this specific purpose or the
use of an AI system to inform emotions
of a natural person in the areas of
workplace and education institutions
except where the use of the AI system is
intended to be put in place or into the
market for medical or safety reasons so
that that so we have the prohibition so
you cannot use the system to inform
emotions in workplace in the workplace
or education institutions but if it's
for medical or safety reasons you can so
of course the big we as lawyers we know
it okay the big thing is this
exception my thoughts of f first my my
first question so maybe my imag your
imagination as someone who's ears and
ears studying the topic so you have more
examples than I so when I read this
provision they what I think they're
talking about for emotions in the
workplace so what would be typical
application so I usually I talk about
frasal recognition cameras in the
workplace could be uh using U detect
emotion inference and detection we could
think about sentiment analysis in audio
so you think of sales representatives or
uh customer service uh and also so audio
and text I also can think about wearable
devices for wellness so you can say it
is for wellness but then you also
productivity but are there so my first
sub question I'll talk more about that
but are there any other examples that
that they're not coming to my mind this
area of in Emotion
inference yeah emotion inference but
just for health like so so so there was
a funny thing that happened um well I
shouldn't say funny because it was
obviously a very serious time which is a
pandemic right you know um I think the
covid pandemic did actually really
introduce a lot of workers to
surveillance for the first time right um
You had had some technologies for
surveillance but frankly many employers
were not on that train yet um but with
the covid pandemic uh there was Now sort
of I don't want to say pretex but there
was a demand of like okay well we want
to check that everybody is healthy um
and uh you know coming into the
workplace um and so now now you had like
the thermal scanners actually a lot of
com uh corporations adopted those where
you're coming into the workplace then
come to find find out those those
thermal scanners are actually not very
accurate and actually can be very skewed
like you know think a menopausal woman
having a hot flash and how that could be
misconstrued for oh you have a fever um
and then another issue was that you know
once you have that mission creep right
um of surveillance for a legitimate
purpose it can have this Mission creep
of surveillance for other less
legitimate purposes so I think I think
that part of the AI Act is opening a
door um it's it's opening a huge trapo
um you know so to speak where you know
companies can really fall down a hole of
like just justifying everything as like
for health reasons right that's what I
imag so you can say yeah we have that
employee right some sort of specific
health issue we just want to avoid
have a heart attack so we're just going
to be filming all the time and check
everybody and I really also worry about
um how it could be very culturally or
socially biased so an example I love to
give is um you know as an American I
smile a lot but I know that I can't do
that when I go to certain countries
because I will look
suspicious for you know constantly
smiling right whereas in America if
you're not smiling constantly then it's
like oh my gosh like are you up to
something is something wrong so do you
see so there's this cultural bias of
like what is actually a universal
emotion detection do you see so emotion
inference is just so um
scientifically um not grounded because
it's it's assuming that all humans are
exploting emotion the same way so a an
American with a big smile right is as
just as friendly as a Russian with a big
smile and that's just actually not true
right some cultures don't smile as often
or see smiling you know as
suspicious so I think just the fact that
you're now going to use this tool for
like all human beings you're already
introducing um a a situation where some
people will be be
disadvantaged because they don't match
the standard uh you know um person that
you use to train the system and I think
also when we think about audio or text
analysis or sentiment anal we can think
of non-native speakers also right so
someone who is having actually
difficulty with the language but is not
actually it's just a difficulty not
necessarily so and we know that those
systems are less accurate when you have
someone who is not the typical speaker
all those typical AI issues especially
when we add them to the workplace so the
consequence is not only an AI power
decision it might be you being fired so
it's right you're fired or you don't get
the job so one chapter of the book is
devoted to automated video interviewing
there has been an
explosion uh for automated video
interviewing since the pandemic so many
corporations actually only do automated
video interviewing um the for one the
problem with that is that a lot of
candidates don't know this they think oh
I'm doing my interview yes I'm speaking
into a computer but it's just a
recording and then someone a person is
going to review it a lot of times those
companies do not have a person reviewing
that it's actually being graded by an AI
first and foremost before uh the ones
that pass that check get seen by a human
so what you get is a lot of people being
disadvantaged because they don't fit the
standard human so one one of my
interviews was a um a student who is an
Asian American speaks good English um
fluent English but their accent is
different so they found that they kept
failing the interview so to speak and
they had to essentially demand a live
interview with a human just so they
could be understood well another um uh
you know study found that these um
automated video interviews also
downgrade people who have Southern
American accents right because their
their Cadence is different so those
people was being downgraded as less
confident because the the machine was
understanding them to speak with an
accent like they were asking a question
when it wasn't you know the case uh so
you know these things are problematic
because often times the way the systems
are trained is not to include the
diversity of The Human Experience or The
Human Condition it's really more like oh
we have one standard human being in mind
and everybody has to match that quote
unquote standard human being um and
that's grossly unfair and this is
something that see if there was another
human on the other side it would be
something that click you don't need it's
nice to think about so we tend to
compare right how biased human humans
are and how biased machines are but some
some things a human will detect in the
first Glimpse in the machine just
because of the way it's strained will
maybe never detect so I want I want to
actually jump on that point because I
saw it in the comments as well so
somebody said is there a soft soft text
in this narrative that things go well in
a AI in HR without AI tooling no there
is no such uh subtext I think what for
me you know as a lawyer and sociologist
what what I am saying is we ought to
think about how these AI Tools in HR or
other work or and business context we
ought to think about what they are
actually doing not just what they the
hype is what they claim to do we need to
actually look at the real life examples
of how they're actually working how
they're changing the nature of the
workplace how they're changing the
employment bargain for workers and then
as a lawyer we want to think about are
these AI tools are they actually in
conformance with the law so it's not
really about asking this question of are
AI tools better than humans or not as a
lawyer my question is are they following
the law or not right so if humans are
not following the law bad if AI hiring
tools are also not following the law and
how they work also bad so I don't think
we need to really engage in these like I
think false binaries of oh AI hiring
versus human hiring which is the better
one what matters is are any of these
types of activities are they actually in
conformance with the law and then
sociologically how are they changing our
society are they actually creating the
type of society that we
want and and on the on the legal uh
topic again so I want to hear your
opinion about the US law covering that
I'm not familiar with the US employment
law so I just want to bring another
provision uh from the AI act so we spoke
about one of the prohibited AI systems
that includes uh uh talks about
workplace the second one I want to bring
so Annex 3 has the list of areas with
where if you have an AI system in one of
those areas you're going to be
classified as highrisk there are there
is the derogation there is the big
loophole exception we're not talking
about that at this moment but so let's
just take a look at this one involving
employment so employment workers
management and access to self-employment
so AI systems intended to be used for
the recruitment or selection of natural
persons in particular to place to place
targeted job advertisements to analyze
and filter job applications and to
evaluate candidates so this would be a
first category of highrisk AI system and
the second category would be AI systems
intended to be used to make decisions
affec in terms of work rated
relationships the promotion or
termination of work rated contractual
relationships to allocate tasks based on
individual Behavior or personal traits
or characteristics or to Monitor and
evaluate the performance and behavior of
persons in such relationships so those
AI systems would be classified as high
risk and I want to so this is the EU
perspective how is the US doing in that
area so AI in the workplace are there
would those these types of AI systems be
also consider high risk in the US or it
will highly depend on the state what's
going on right now in the US that is an
excellent question so um unfortunately
there is no uniform uh guideline on a
federal basis for how AI systems
actually are are classified or even
regulated in the workplace right so that
is such a huge Lacuna in the law um for
uh you know for businesses in terms of
how they should approach uh AI systems
there's just none um there's some
guidance right coming out of federal
agencies so the EU has given a guidance
saying that companies should also know
that if they are using AI systems that
are then having a disperate impact on
protected categories that would still be
considered illegal just as if they were
using a human manager that then had you
know that then resulted in a disperate
impact um and so so there's that from
the EOC for for for people who are not
aware the EOC is the equal employment
opportunity commission um and the EOC
oversees sees title seven of the Civil
Rights Act um which is a title that
governs uh anti-discrimination policies
in the workplace and that title says you
can't discriminate against people on the
basis of race gender national origin
religion Etc and basically if you are
using AI systems in a way that it does
have a disperate impact from people on
people in those categories the EOC says
that is actionable as in that is
something that the employer would be
held liable for um now there is a
question of okay but the employer has to
even know that that is happening right
because like I just described to you the
AI systems can be using proxy variables
where the employer is not even realizing
that there is this issue so I actually
have advocated for a um auditing mandate
to be something that's um promulgated by
the federal government for all AI
decision making systems um thus far that
has not happened on a federal level it
has happened uh on a municipal level so
the city of New York New York City
actually now has a law saying um it just
went into uh effect earlier this year
saying basically that um if you're using
AI for any kind of consequential
decision making including for hiring you
actually have to have an auditing uh
mechanism in place to essentially be
checking that that AI system is not
being uh
discriminatory and that auditing
mechanism would have to be by a third
party or or some sort of human oversight
measure by the employer uh um so I I'm
not quite sure if it has to be by a
third party I mean you have to obviously
provide proof that you've been doing it
uh the way that I advocated for it is
that it should be both it should be
internal and also a third party um so
because obviously the third party can
only tell you so much right um and there
might be some things that are sensitive
that you don't necessarily want the
third party to know so then you can also
do the self auditing on a more frequent
basis just to catch things and I
advocate for a safe harbor meaning that
if people are actually doing the audit
Audits and they find a problem I I argue
that the law give them like a period of
time to fix things so it's not like oh
if you do the Audits and you find a
problem then you as an employer you're
immediately liable because if you put
that then no employer is going to want
to do the audits because that would put
them you know at risk but if you have a
safe harbor then that would actually
encourage the employers to keep doing
the
audits I love the idea of the auditing
and and and when we were talking about
so human oversight is a big word now
right right so when we trying to tackle
the AI blackbox problem so for those in
the audience so it's difficult to
explain sometimes not even the the the
company who which developed the AI
system can explain why that decision why
the system reach that decision that
specific case so we have this complex
black box and part of the arguments or
policy Solutions when we are okay so how
do we fix that or how do we uh do
something about that so they say human
oversight having someone who will
overview the AI and part of my PhD
research so of the chapters I covered
cognitive biases so I'm fascinated about
this this topic and I I I always think
about automation bias right so for those
the audience who are not familiar so
automation bias is this tendency observe
in various studies that people tend to
agree or or or think that the the
decision taken by a computer or by an
system is true or is better than a
decision made by a human and what would
be the Practical consequence so you are
the human oversight person and then you
see okay the AI algorithm recommend
X you have to check that but then okay
it would be difficult it would be
complex to why to checking and and then
you end up okay probably it's probably
correct so tend you have this tendency
to follow so I would like your your have
you uh what are your what's your view on
that so is so I see especially for the
AI act it relies on oversight and human
oversight comes over now that many
countries are regulating AI so it's like
a magic one so you have ai but then you
add a person to do human oversight and
then we we fix things so right right
right so so um you know the The Scholar
um the legal scholar that has written a
ton on this is uh Danielle Citron
Professor Danielle Citron who's at UVA
law and um is such a fantastic scholar
and and she's I believe the person that
came up with that phrase automation bias
um which is this idea that you know
people tend to like trust automated
things they're like oh it it knows what
it's doing subjective it's not biased
and and that's actually a bias of humans
to trust um automate things too much and
because of that right the human in the
loop right that's usually how it's
referred to oh we need a human in the
loop a human in the loop because of that
the human in the loop solution is not
such a perfect solution right because
there is a a a chance that that human
becomes too dependent on whatever the
automated system is is doing and just
thinks oh it's okay it must be good um
so I actually advocate for something
else which is society in the loop so
instead of just having this all and like
oh one human is just checking this
checking this checking this it's
actually Society is checking this so I
know it sounds funny like how can all of
society be checking one thing but it's
really all about how you build that in
right so um if you look at my paper
automated governance um in that paper I
talk about the fact that in the US
government many agencies are already
using AI
systems many systems but we don't even
know we don't even know right so first
of all there's that lack of societal
awareness of like the fact that
automated systems are in play and then
there's a lack of oversight of how these
systems is are being used to manage
citizens so in that paper automated
governance I think Society in the loop
can look like this first at the design
stage so human in the loop is always
Imagining the human after it's not
Imagining the human who's doing the
design right it's just imagining the
human after the machine has been adopted
and you know just put into place but we
actually need to imagine the human at
the very beginning who's thinking of
this system creating that system
thinking about what are the design
features for that system system and
that's where Society especially can come
in because the government can mandate
certain design features do you see what
I mean so the for example the G the
government can say oh you're creating an
AI hiring system you need to put in
place design features that would allow
for an easy
audit of that system so that's a problem
that we have right now we have so many
systems that are actually very difficult
to audit because they weren't designed
to be
audited right so the government as part
of society as the governing part of
society can say oh you're creating the
systems that are going to be used for
important decision- making okay you have
to have these specific design features
that are actually going to help us audit
it better and then you're also going to
have these sort of checks and balances
this guard rails do you see what I mean
so that's how Society is in the loop
from the very beginning all the way to
the end so I love that we talking about
Society in the loop we we are our time
has ended so I want you to finish with a
message to the audience so taking from
the society in the loop so if let's
imagine that so some people in the
audience are employers some are privacy
professionals some are in other fields
so let's imagine someone who wants to do
something maybe not specifically uh an
employer but someone who who is
interested in our conversation and and
wants to do something about it in spe
specifically about AI in the workplace
what would be your your message your
your maybe uh an an inspirational uh
quote or something to to to help them do
something what they want to they want to
turn off now the computer and do
something about that so what what could
they do all right I'll Channel my uh my
inner Martin Luther King
Jr Dr Martin Luther King Jr so actually
Dr Martin Luther King Jr was actually
very patient when he came to technology
we only hear you know all the stuff he
had to say about race but actually you
know Dr Martin Luther King Jr talked a
lot about economic Justice he talked
about issues of you know poverty he
talked about issues of workers rights
and one of the things that he said was
these machines that we see do amazing
things they also can be used to do
amazing things for human beings if we
think of it that way so I guess my sort
of takeaway is instead of only just
thinking about how can we use AI systems
to squeeze you know the most
productivity out of workers how can we
use AI systems to like save all the
money right for ourselves how can we use
AI systems to frankly grab right from
the commons um some might even call
theft you know how can we think about
this in a different way where we're
thinking about human development human
Prosperity human well-being so you know
it's not that we want to throw away the
technology I think that's throwing away
the baby with the B bath water right
it's not that we are saying oh last stop
all AI development that's not it it's
that we're saying can we redirect AI
development to actually be also about
human development can we redirect AI
development to be about helping humans
first and foremost that's it beautiful
what a what a beautiful way to end uh
where where people can find you so I
know Blue Sky still Twitter not Twitter
anymore Twitter's like I'm still on
Twitter I'm hanging in there L what else
where where can people find so yeah so
first and foremost blue sky is where I
am most of the time I'm still lingering
on on Twitter just because I think you
need all sorts of voices there um
LinkedIn for sure um just type my name
in all those systems um um I oh oh also
Instagram so I have an Instagram page
for the book and sometimes I'll I'll
tweet um not tweet but post uh Snippets
of book talks and you know other things
from the book on Instagram um but you
know you can also just send me an email
so my email is I a j u
NWA at emory.edu so you can just Google
me at Emory and you can find me and
finally I'm the director of the AI and
future of work program at Emory where I
really am wanting to put into effect
these ideals that I just shared to you
which is the idea that AI can actually
be used in the furtherance and bance of
human welfare in the future of work and
that that entails actually engaging with
businesses and corporations to make that
happen thank you excellent reminder so
this is uh the book The Quantified
worker maybe it's blurring so I'm
reading Sy here is BL so the Quantified
worker if you haven't read yet so go by
it's excellent uh thank you so mucha for
joining me today it's a it was a place
such an enlightening conversation I I
think that the the audience greatly
benefit from it too uh thank you to the
audience for always being so
participative and adding comments and
relevant links so thank you again uh two
reminders so if you want to stay up to
date with AI policy compliance and
regulations so subscribe to my
newsletter also to get informed of the
next live talks uh if you want to join
the AI governance training in January so
save your spot and I see you again in
January so this was our last live it's
such an excellent way to thank you so
much for having me this is such a
pleasure and I've been reading through
your comments you know and I I love how
engage this audience is um it's just
outstanding to to see that people are
really um you know engage with this
issue and really thinking seriously
because this is a future for all of us
workers um so thank you all for coming I
truly appreciate it thank you so much so
thank you everyone bye bye I see you in
January 2025 bye bye and thank you all
right take care bye
