{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87adbb89-d66d-479a-a1c9-935943143b95",
   "metadata": {},
   "source": [
    "### Notebook for processing the transcripts of DataTalksClub podcasts available via GitHub\n",
    "\n",
    "GitHub repo link: https://github.com/DataTalksClub/datatalksclub.github.io/tree/main/_podcast\n",
    "\n",
    "Downloading the transcripts from GitHub is easier than downloading them from YouTube. However, not all podcasts are available on GitHub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db758d6-9503-4c1a-b7cb-c96fee455d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Set, Tuple\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "\n",
    "import requests\n",
    "import frontmatter\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20ed58e-930f-4833-9542-1abfda2c29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# GitHub zip reader (your code, lightly cleaned)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses files from a GitHub repository (main branch) via ZIP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        repo_owner: str,\n",
    "        repo_name: str,\n",
    "        allowed_extensions: Iterable[str] | None = None,\n",
    "        filename_filter: Callable[[str], bool] | None = None,\n",
    "    ):\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "\n",
    "        self.allowed_extensions = {ext.lower() for ext in allowed_extensions} if allowed_extensions else set()\n",
    "        self.filename_filter = filename_filter or (lambda _: True)\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        resp = requests.get(self.url, timeout=60)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        try:\n",
    "            return self._extract_files(zf)\n",
    "        finally:\n",
    "            zf.close()\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        data: List[RawRepositoryFile] = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "\n",
    "                data.append(RawRepositoryFile(filename=filepath, content=content))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        fp = filepath.lower()\n",
    "\n",
    "        if fp.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        filename = fp.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(fp)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(fp):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        return filename.rsplit(\".\", 1)[-1] if \".\" in filename else \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        return parts[1] if len(parts) > 1 else parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7033a35-cfbb-433e-84dd-3c17f3c2ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Transcript + title extraction (your approach)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def _has_transcript(content: str) -> bool:\n",
    "    try:\n",
    "        post = frontmatter.loads(content or \"\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    tr = post.get(\"transcript\")\n",
    "    if not isinstance(tr, list) or not tr:\n",
    "        return False\n",
    "\n",
    "    return any(isinstance(e, dict) and (\"line\" in e or \"header\" in e) for e in tr)\n",
    "\n",
    "\n",
    "def extract_paragraphs_from_raw(raw_file: RawRepositoryFile) -> List[str]:\n",
    "    post = frontmatter.loads(raw_file.content)\n",
    "    paras: List[str] = []\n",
    "\n",
    "    for e in post.get(\"transcript\", []):\n",
    "        if isinstance(e, dict) and \"line\" in e:\n",
    "            paras.append(str(e[\"line\"]).replace(\"\\n\", \" \").strip())\n",
    "        elif isinstance(e, dict) and \"header\" in e:\n",
    "            paras.append(f\"## {str(e['header']).strip()}\")\n",
    "\n",
    "    return [p for p in paras if p]\n",
    "\n",
    "\n",
    "def extract_title_from_raw(raw: RawRepositoryFile) -> Optional[str]:\n",
    "    try:\n",
    "        post = frontmatter.loads(raw.content or \"\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    t = post.get(\"title\")\n",
    "    if isinstance(t, (str, int, float)):\n",
    "        s = str(t).strip()\n",
    "        return s or None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e472d5b1-4181-4c2e-b77b-8e6d610b06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# YouTube URL -> video_id\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "_YT_ID_RE = re.compile(r\"^[A-Za-z0-9_-]{8,20}$\")\n",
    "\n",
    "def youtube_video_id(url_or_id: str) -> Optional[str]:\n",
    "    s = (url_or_id or \"\").strip()\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    # already an ID\n",
    "    if _YT_ID_RE.fullmatch(s):\n",
    "        return s\n",
    "\n",
    "    try:\n",
    "        u = urlparse(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    host = (u.netloc or \"\").lower()\n",
    "    path = (u.path or \"\").strip(\"/\")\n",
    "\n",
    "    if \"youtu.be\" in host:\n",
    "        # youtu.be/<id>\n",
    "        parts = path.split(\"/\")\n",
    "        return parts[0] if parts and parts[0] else None\n",
    "\n",
    "    if \"youtube.com\" in host:\n",
    "        if path == \"watch\":\n",
    "            qs = parse_qs(u.query)\n",
    "            return qs.get(\"v\", [None])[0]\n",
    "        parts = path.split(\"/\")\n",
    "        if parts and parts[0] in {\"live\", \"embed\", \"shorts\"} and len(parts) >= 2:\n",
    "            return parts[1]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e07b26e-4d35-4cad-8bc3-9d1fb5f63f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_title(s: str) -> str:\n",
    "    s = (s or \"\").lower().strip()\n",
    "    s = re.sub(r\"[\\u2010-\\u2015]\", \"-\", s)          # normalize weird dashes\n",
    "    s = re.sub(r\"[^a-z0-9\\s-]\", \" \", s)             # drop punctuation\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()              # collapse spaces\n",
    "    return s\n",
    "\n",
    "def slugify(s: str, max_len: int = 80) -> str:\n",
    "    s = normalize_title(s)\n",
    "    s = s.replace(\" \", \"-\")\n",
    "    s = re.sub(r\"-+\", \"-\", s).strip(\"-\")\n",
    "    return s[:max_len] or \"untitled\"\n",
    "\n",
    "def token_overlap_score(a: str, b: str) -> float:\n",
    "    ta = set(normalize_title(a).split())\n",
    "    tb = set(normalize_title(b).split())\n",
    "    if not ta or not tb:\n",
    "        return 0.0\n",
    "    return len(ta & tb) / max(len(ta), len(tb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45947349-90e4-4252-9b71-96464688de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Map podcast files to youtube IDs by scanning front-matter strings\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def _collect_strings(obj: Any) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively collect all string values from a nested structure.\n",
    "    Useful to search front-matter for YouTube links/ids.\n",
    "    \"\"\"\n",
    "    out: List[str] = []\n",
    "    if isinstance(obj, str):\n",
    "        out.append(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        for v in obj.values():\n",
    "            out.extend(_collect_strings(v))\n",
    "    elif isinstance(obj, list):\n",
    "        for v in obj:\n",
    "            out.extend(_collect_strings(v))\n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_video_id_from_frontmatter(raw: RawRepositoryFile) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Parse YAML front-matter and try to find a YouTube video_id in any string field.\n",
    "    This makes the code robust to different keys (youtube, video, links, etc.).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        post = frontmatter.loads(raw.content or \"\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    strings = _collect_strings(post.metadata)\n",
    "\n",
    "    # Try direct IDs first\n",
    "    for s in strings:\n",
    "        vid = youtube_video_id(s)\n",
    "        if vid:\n",
    "            return vid\n",
    "\n",
    "    # Also scan for embedded youtube URLs inside larger strings\n",
    "    for s in strings:\n",
    "        # find any youtube-looking substring\n",
    "        m = re.search(r\"(https?://(?:www\\.)?(?:youtube\\.com|youtu\\.be)[^\\s)\\\"']+)\", s)\n",
    "        if m:\n",
    "            vid = youtube_video_id(m.group(1))\n",
    "            if vid:\n",
    "                return vid\n",
    "\n",
    "    return None\n",
    "\n",
    "def build_title_to_raw_map(files: List[RawRepositoryFile]) -> Dict[str, RawRepositoryFile]:\n",
    "    mapping: Dict[str, RawRepositoryFile] = {}\n",
    "    for raw in files:\n",
    "        t = extract_title_from_raw(raw)\n",
    "        if not t:\n",
    "            continue\n",
    "        mapping[normalize_title(t)] = raw\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33a7670-8e40-4b46-9e30-b15eebdd69b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Saving: JSON + TXT + manifest\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "def append_manifest(path: Path, record: Dict[str, Any]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def load_processed_video_ids(manifest_path: Path) -> Set[str]:\n",
    "    if not manifest_path.exists():\n",
    "        return set()\n",
    "\n",
    "    processed: Set[str] = set()\n",
    "    for line in manifest_path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            vid = obj.get(\"video_id\")\n",
    "            if isinstance(vid, str) and vid:\n",
    "                processed.add(vid)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return processed\n",
    "\n",
    "\n",
    "def transcript_to_text(paras: List[str]) -> str:\n",
    "    # Keep headers as-is; otherwise join by newlines.\n",
    "    return \"\\n\".join([p.strip() for p in paras if p.strip()]).strip() + \"\\n\"\n",
    "\n",
    "\n",
    "def save_transcript_files(\n",
    "    out_dir: Path,\n",
    "    channel: str,\n",
    "    video_id: str,\n",
    "    video_url: str,\n",
    "    raw_filename: str,\n",
    "    title: Optional[str],\n",
    "    paras: List[str],\n",
    ") -> None:\n",
    "    json_dir = out_dir / \"json\" / channel\n",
    "    txt_dir = out_dir / \"txt\" / channel\n",
    "    json_dir.mkdir(parents=True, exist_ok=True)\n",
    "    txt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    now = utc_now_iso()\n",
    "\n",
    "    # You asked for a format \"very similar\" to your YouTube JSON:\n",
    "    # We'll keep transcript as a list of paragraph strings (because GitHub source is paragraphs),\n",
    "    # and also include a \"segments\" version (simple conversion) in case you want it later.\n",
    "    payload = {\n",
    "        \"source\": \"github\",\n",
    "        \"repo\": \"DataTalksClub/datatalksclub.github.io\",\n",
    "        \"channel\": channel,\n",
    "        \"video_id\": video_id,\n",
    "        \"video_url\": video_url,\n",
    "        \"video_title\": title,\n",
    "        \"github_filename\": raw_filename,\n",
    "        \"fetched_at\": now,\n",
    "        \"transcript_paragraphs\": paras,  # list[str]\n",
    "    }\n",
    "\n",
    "    (json_dir / f\"{video_id}.json\").write_text(\n",
    "        json.dumps(payload, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    (txt_dir / f\"{video_id}.txt\").write_text(\n",
    "        transcript_to_text(paras),\n",
    "        encoding=\"utf-8\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0225e15-cb24-4893-b1f8-461bbe6da13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_title_match(\n",
    "    query_title: str,\n",
    "    title_map: Dict[str, RawRepositoryFile],\n",
    "    *,\n",
    "    min_score: float = 0.5,\n",
    ") -> Tuple[Optional[str], Optional[RawRepositoryFile]]:\n",
    "    qn = normalize_title(query_title)\n",
    "    if not qn:\n",
    "        return None, None\n",
    "\n",
    "    # 1) exact normalized match\n",
    "    if qn in title_map:\n",
    "        return qn, title_map[qn]\n",
    "\n",
    "    # 2) substring match (either direction)\n",
    "    for tn, raw in title_map.items():\n",
    "        if qn in tn or tn in qn:\n",
    "            return tn, raw\n",
    "\n",
    "    # 3) token overlap scoring\n",
    "    best_tn = None\n",
    "    best_raw = None\n",
    "    best_score = 0.0\n",
    "\n",
    "    for tn, raw in title_map.items():\n",
    "        score = token_overlap_score(qn, tn)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_tn = tn\n",
    "            best_raw = raw\n",
    "\n",
    "    if best_score >= min_score:\n",
    "        return best_tn, best_raw\n",
    "\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be0b31b8-4718-4b82-81cf-88230e0d80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Main: given list of YouTube URLs -> process matching podcasts\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def download_github_podcast_transcripts_for_titles(\n",
    "    titles: List[str],\n",
    "    *,\n",
    "    out_dir: Path = Path(\"data/github_podcast\"),\n",
    "    channel: str = \"datatalksclub\",\n",
    "    sleep_s: float = 0.1,\n",
    "    only_with_transcripts: bool = True,\n",
    ") -> None:\n",
    "    manifest_path = out_dir / \"manifest.jsonl\"\n",
    "    processed = load_processed_video_ids(manifest_path)  # consider updating this to track slugs instead\n",
    "\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner=\"DataTalksClub\",\n",
    "        repo_name=\"datatalksclub.github.io\",\n",
    "        allowed_extensions={\"md\", \"mdx\"},\n",
    "        filename_filter=lambda p: p.startswith(\"_podcast/\"),\n",
    "    )\n",
    "    files = reader.read()\n",
    "    if only_with_transcripts:\n",
    "        files = [f for f in files if _has_transcript(f.content)]\n",
    "\n",
    "    title_map = build_title_to_raw_map(files)\n",
    "\n",
    "    for title in titles:\n",
    "        now = utc_now_iso()\n",
    "        slug = slugify(title)\n",
    "\n",
    "        # If you keep your processed logic keyed by video_id, change it to slug:\n",
    "        # (recommended) skip if slug already OK in manifest\n",
    "        # For now, just don't skip:\n",
    "        matched_key, raw = find_best_title_match(title, title_map)\n",
    "\n",
    "        if raw is None:\n",
    "            append_manifest(\n",
    "                manifest_path,\n",
    "                {\n",
    "                    \"title\": title,\n",
    "                    \"slug\": slug,\n",
    "                    \"channel\": channel,\n",
    "                    \"status\": \"not_found\",\n",
    "                    \"error\": \"No matching episode title found in _podcast front-matter\",\n",
    "                    \"fetched_at\": now,\n",
    "                },\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            real_title = extract_title_from_raw(raw)\n",
    "            paras = extract_paragraphs_from_raw(raw)\n",
    "\n",
    "            # Save using slug (no video_id available)\n",
    "            save_transcript_files(\n",
    "                out_dir=out_dir,\n",
    "                channel=channel,\n",
    "                video_id=slug,           # reuse field for filename\n",
    "                video_url=\"\",            # unknown / not provided\n",
    "                raw_filename=raw.filename,\n",
    "                title=real_title,\n",
    "                paras=paras,\n",
    "            )\n",
    "\n",
    "            append_manifest(\n",
    "                manifest_path,\n",
    "                {\n",
    "                    \"title\": title,\n",
    "                    \"matched_title\": real_title,\n",
    "                    \"slug\": slug,\n",
    "                    \"channel\": channel,\n",
    "                    \"status\": \"ok\",\n",
    "                    \"github_filename\": raw.filename,\n",
    "                    \"fetched_at\": now,\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            append_manifest(\n",
    "                manifest_path,\n",
    "                {\n",
    "                    \"title\": title,\n",
    "                    \"slug\": slug,\n",
    "                    \"channel\": channel,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": repr(e),\n",
    "                    \"github_filename\": raw.filename if raw else None,\n",
    "                    \"fetched_at\": now,\n",
    "                },\n",
    "            )\n",
    "        finally:\n",
    "            time.sleep(sleep_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "289789c6-850c-4288-9458-59dbe546208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#episode_titles = [\"Cracking the Code: Machine Learning Made Understandable - Christoph Molnar\", \n",
    "#                  \"Knowledge Graphs and LLMs Across Academia and Industry - Anahita Pakiman\", \n",
    "#                  \"Working as a Core Developer in the Scikit-Learn Universe - Guillaume Lemaître\", \n",
    "#                  \"Trends in AI Infrastructure - Andrey Cheptsov\", \n",
    "#                  \"Trends in Data Engineering - Adrian Brudaru\", \n",
    "#                  \"From Human-in-the-Loop to Agent-in-the-Loop: A Practical Transition Guide - Ertugrul Mutlu\"\n",
    "#                 ]\n",
    "episode_titles = [\"Data Science for Public Policy — Ethical AI, Climate Justice & Impact Projects (Christine Cepelak)\", \n",
    "                  \"Responsible & Explainable AI: Practical Guide to Bias Detection, Fairness & Governance (Supreet Kaur)\", \n",
    "                  \"MLOps in Finance: Regulated Deployment, CI/CD and Model Governance (Nemanja Radojkovic)\", \n",
    "                  \"Master Industrial Data: Synthetic Tabular Data, Small-Data Modeling, Sensors & MLOps (Rosona Eldred)\", \n",
    "                  \"Building and Scaling Data Science Practice in Industrial Enterprises: AI Adoption, MLOps Maturity & Career Growth (Andrey Shtylenko)\", \n",
    "                  \"Optimize Decisions with ML: Prescriptive & Robust Optimization for Supply Chain and Pricing (Dan Becker)\", \n",
    "                  \"Build & Scale Data Products for AI: Roadmaps, MLOps, Customer Research & Metrics (Greg Coquillo)\", \n",
    "                  \"Cracking the Code: Machine Learning Made Understandable (Christoph Molnar)\", # not available\n",
    "                  \"Knowledge Graphs and LLMs Across Academia and Industry (Anahita Pakiman)\", # not available \n",
    "                  \"Trends in AI Infrastructure (Andrey Cheptsov)\", # not available (because new?)\n",
    "                  \"Trends in Data Engineering (Adrian Brudaru)\", # not available (because new?)\n",
    "                  \"From Human-in-the-Loop to Agent-in-the-Loop: A Practical Transition Guide (Ertugrul Mutlu)\", # not available because new\n",
    "                  \"Responsible & Explainable AI: Practical Guide to Bias Detection, Fairness & Governance (Supreet Kaur)\", \n",
    "                  \"Master Industrial Data: Synthetic Tabular Data, Small-Data Modeling, Sensors & MLOps (Rosona Eldred)\",\n",
    "                  \"Building and Scaling Data Science Practice in Industrial Enterprises: AI Adoption, MLOps Maturity & Career Growth (Andrey Shtylenko)\",\n",
    "                  \"Data Governance & Data Access Management: Access Controls, Data Catalogs & Access-as-Code (Bart Vandekerckhove)\",\n",
    "                  \"From Black-Box Systems to Augmented Decision-Making (Anusha Akkina)\", \n",
    "                  \"AI in Industry: Trust, Return on Investment and Future (Maria Sukhareva)\", \n",
    "                  \"MLOps in Finance: Regulated Deployment, CI/CD and Model Governance (Nemanja Radojkovic)\"\n",
    "                 ]\n",
    "\n",
    "download_github_podcast_transcripts_for_titles(episode_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e3edd68-e7f0-4f20-a1af-caafe19f2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls = [\"https://www.youtube.com/watch?v=LBuGzyOkx7c&list=PL3MmuxUbc_hK60wsCyvrEK2RjQsUi4Oa_&index=54\",\n",
    "#        \"https://www.youtube.com/watch?v=YncdlUscUOo&list=PL3MmuxUbc_hK60wsCyvrEK2RjQsUi4Oa_&index=41\", \n",
    "#        \"https://www.youtube.com/watch?v=RR6xaYqKJ3o&list=PL3MmuxUbc_hK60wsCyvrEK2RjQsUi4Oa_&index=36\", \n",
    "#        \"https://www.youtube.com/watch?v=1aMuynlLM3o&list=PL3MmuxUbc_hK60wsCyvrEK2RjQsUi4Oa_&index=25\", \n",
    "#        \"https://www.youtube.com/watch?v=AlCFKbFIEM8&list=PL3MmuxUbc_hK60wsCyvrEK2RjQsUi4Oa_&index=23\", \n",
    "#        \"https://www.youtube.com/watch?v=HwCR59VuYn4&list=PL3MmuxUbc_hK60wsCyvrEK2RjQsUi4Oa_&index=1\"\n",
    "#       ]\n",
    "#download_github_podcast_transcripts_for_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faabcde-92a1-48a4-8b2b-c2ba679b449f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ae066-b14a-4c6e-b769-4f896ac549cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
